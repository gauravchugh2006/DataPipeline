[0m15:43:03.187404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce687f9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce46173d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce4616250>]}


============================== 15:43:03.207060 | f0b2c889-f0bd-4ab1-b06f-b53a65379707 ==============================
[0m15:43:03.207060 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:43:03.211130 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:43:03.215857 [error] [MainThread]: Encountered an error:
Runtime Error
  No dbt_project.yml found at expected path /usr/app/dbt_project.yml
  Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml
  
[0m15:43:03.222594 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 0.21082681, "process_in_blocks": "0", "process_kernel_time": 0.329457, "process_mem_max_rss": "89136", "process_out_blocks": "56", "process_user_time": 3.074937}
[0m15:43:03.228622 [debug] [MainThread]: Command `dbt build` failed at 15:43:03.228222 after 0.22 seconds
[0m15:43:03.232519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce467a0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce4620cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6ce4694f90>]}
[0m15:43:03.235430 [debug] [MainThread]: Flushing usage events
[0m15:43:03.727622 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:48:40.119844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c14d63b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c16f8ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c15255350>]}


============================== 15:48:40.126710 | 97956a3b-3b29-485a-9bc4-1ecbbd8d259d ==============================
[0m15:48:40.126710 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:48:40.129810 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:48:40.133805 [error] [MainThread]: Encountered an error:
Runtime Error
  No dbt_project.yml found at expected path /usr/app/dbt_project.yml
  Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml
  
[0m15:48:40.140034 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 0.08782516, "process_in_blocks": "0", "process_kernel_time": 0.101249, "process_mem_max_rss": "88532", "process_out_blocks": "0", "process_user_time": 1.093493}
[0m15:48:40.143838 [debug] [MainThread]: Command `dbt build` failed at 15:48:40.143651 after 0.09 seconds
[0m15:48:40.147684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c14da9150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c14d40c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c14d94f10>]}
[0m15:48:40.148976 [debug] [MainThread]: Flushing usage events
[0m15:48:40.543797 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:49:57.719812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d694cd510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d694b3390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d69bbb350>]}


============================== 15:49:57.726955 | 5ddabc5b-af7b-4d38-995c-2f379213e4d4 ==============================
[0m15:49:57.726955 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:49:57.730273 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:49:57.742139 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.postgres'
[0m15:49:57.743412 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "data_pipeline", target "dev" invalid: Runtime Error
    Could not find adapter type postgres!
[0m15:49:57.745787 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 0.10207198, "process_in_blocks": "0", "process_kernel_time": 0.099116, "process_mem_max_rss": "88088", "process_out_blocks": "0", "process_user_time": 1.12002}
[0m15:49:57.755149 [debug] [MainThread]: Command `dbt build` failed at 15:49:57.754706 after 0.11 seconds
[0m15:49:57.757908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d694daa50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d6cdd4d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8d6cdd42d0>]}
[0m15:49:57.758850 [debug] [MainThread]: Flushing usage events
[0m15:49:58.131106 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:52:15.898631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec7599c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec7401010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec7401fd0>]}


============================== 15:52:15.904238 | 0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8 ==============================
[0m15:52:15.904238 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:52:15.906953 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt build', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:52:15.980427 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `source-paths` config has been renamed to `model-paths`. Please update your
`dbt_project.yml` configuration to reflect this change.
[0m15:52:15.982443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec5ccfdd0>]}
[0m15:52:16.079658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec74c1550>]}
[0m15:52:16.132207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec9808110>]}
[0m15:52:16.135224 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:52:16.215814 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:52:16.222138 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:52:16.223122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec5afa590>]}
[0m15:52:16.915012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec5b23910>]}
[0m15:52:16.970003 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m15:52:16.976821 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m15:52:17.017532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0bb2e2e1-338e-4e07-bf89-4a7aa954cdf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec583b610>]}
[0m15:52:17.018669 [info ] [MainThread]: Found 433 macros
[0m15:52:17.023529 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:52:17.027508 [debug] [MainThread]: Command end result
[0m15:52:17.067548 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m15:52:17.072175 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m15:52:17.078068 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/target/run_results.json
[0m15:52:17.080278 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 1.2436041, "process_in_blocks": "0", "process_kernel_time": 0.165775, "process_mem_max_rss": "102264", "process_out_blocks": "120", "process_user_time": 2.258693}
[0m15:52:17.081187 [debug] [MainThread]: Command `dbt build` succeeded at 15:52:17.081074 after 1.24 seconds
[0m15:52:17.082094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcec740bad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcecaef0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcecaef0c90>]}
[0m15:52:17.082958 [debug] [MainThread]: Flushing usage events
[0m15:52:17.530092 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:53:29.519937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c387af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c3b73210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c376b590>]}


============================== 15:53:29.526649 | 2594c392-d711-4664-b4f3-c68fde9f2368 ==============================
[0m15:53:29.526649 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:53:29.528409 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:53:29.672574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2594c392-d711-4664-b4f3-c68fde9f2368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c35fd310>]}
[0m15:53:29.719370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2594c392-d711-4664-b4f3-c68fde9f2368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c59f83d0>]}
[0m15:53:29.721229 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:53:29.772668 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:53:29.823454 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:53:29.825119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2594c392-d711-4664-b4f3-c68fde9f2368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c457c890>]}
[0m15:53:30.495381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2594c392-d711-4664-b4f3-c68fde9f2368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c1d1b450>]}
[0m15:53:30.558194 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m15:53:30.566348 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m15:53:30.594869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2594c392-d711-4664-b4f3-c68fde9f2368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c1a85d10>]}
[0m15:53:30.596645 [info ] [MainThread]: Found 433 macros
[0m15:53:30.602276 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:53:30.606316 [debug] [MainThread]: Command end result
[0m15:53:30.645856 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m15:53:30.652081 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m15:53:30.658680 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/target/run_results.json
[0m15:53:30.660191 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 1.2152436, "process_in_blocks": "0", "process_kernel_time": 0.111144, "process_mem_max_rss": "102336", "process_out_blocks": "0", "process_user_time": 2.101643}
[0m15:53:30.661034 [debug] [MainThread]: Command `dbt build` succeeded at 15:53:30.660928 after 1.22 seconds
[0m15:53:30.661773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c37db690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c379fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7c70ec290>]}
[0m15:53:30.663539 [debug] [MainThread]: Flushing usage events
[0m15:53:31.079143 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m06:34:09.862941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc6294910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc67ad6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc6296190>]}


============================== 06:34:09.882528 | 62a9f0e3-b9bc-4eb9-9d85-15c6cf3fc9da ==============================
[0m06:34:09.882528 [info ] [MainThread]: Running with dbt=1.9.3
[0m06:34:09.888617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt build', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m06:34:10.164593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '62a9f0e3-b9bc-4eb9-9d85-15c6cf3fc9da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc6f83910>]}
[0m06:34:10.255655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '62a9f0e3-b9bc-4eb9-9d85-15c6cf3fc9da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc85044d0>]}
[0m06:34:10.259175 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m06:34:10.378517 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m06:34:10.525874 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:34:10.528181 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:34:10.541071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '62a9f0e3-b9bc-4eb9-9d85-15c6cf3fc9da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc6105910>]}
[0m06:34:10.660627 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m06:34:10.675906 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m06:34:10.800505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '62a9f0e3-b9bc-4eb9-9d85-15c6cf3fc9da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc43c6d90>]}
[0m06:34:10.803122 [info ] [MainThread]: Found 433 macros
[0m06:34:10.807694 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m06:34:10.812799 [debug] [MainThread]: Command end result
[0m06:34:10.872343 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/app/target/manifest.json
[0m06:34:10.882526 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/app/target/semantic_manifest.json
[0m06:34:10.895624 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/app/target/run_results.json
[0m06:34:10.899415 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 1.1831601, "process_in_blocks": "0", "process_kernel_time": 0.424192, "process_mem_max_rss": "102352", "process_out_blocks": "0", "process_user_time": 2.63605}
[0m06:34:10.901550 [debug] [MainThread]: Command `dbt build` succeeded at 06:34:10.901298 after 1.19 seconds
[0m06:34:10.903259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc60fe150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc9d61750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efcc47e9510>]}
[0m06:34:10.904721 [debug] [MainThread]: Flushing usage events
[0m06:34:11.608594 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:47:35.746238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232ADA17050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232ADA17020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD9C4DD0>]}


============================== 11:47:35.760719 | 75bcf745-6a8e-4256-813e-24b2afbcad44 ==============================
[0m11:47:35.760719 [info ] [MainThread]: Running with dbt=1.9.4
[0m11:47:35.762480 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:47:36.070029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75bcf745-6a8e-4256-813e-24b2afbcad44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232ADA6D9D0>]}
[0m11:47:36.148814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75bcf745-6a8e-4256-813e-24b2afbcad44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AE9EDB50>]}
[0m11:47:36.152658 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m11:47:36.498188 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m11:47:36.548444 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m11:47:36.548444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '75bcf745-6a8e-4256-813e-24b2afbcad44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AE042C90>]}
[0m11:47:37.488500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75bcf745-6a8e-4256-813e-24b2afbcad44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD07FB00>]}
[0m11:47:37.550414 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m11:47:37.553539 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m11:47:37.608891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75bcf745-6a8e-4256-813e-24b2afbcad44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AFD4D6A0>]}
[0m11:47:37.608891 [info ] [MainThread]: Found 433 macros
[0m11:47:37.611240 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:47:37.611240 [debug] [MainThread]: Command end result
[0m11:47:37.638132 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m11:47:37.642242 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m11:47:37.643990 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m11:47:37.643990 [debug] [MainThread]: Command `dbt build` succeeded at 11:47:37.643990 after 2.26 seconds
[0m11:47:37.643990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AFBCCA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD989D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AFD3C2C0>]}
[0m11:47:37.643990 [debug] [MainThread]: Flushing usage events
[0m11:47:38.339901 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:12:54.148378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269774C3C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269774C3CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269774C3BC0>]}


============================== 12:12:54.156819 | 40a90560-5014-4d42-94e0-b1e26b57da07 ==============================
[0m12:12:54.156819 [info ] [MainThread]: Running with dbt=1.9.4
[0m12:12:54.156819 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m12:12:54.511965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40a90560-5014-4d42-94e0-b1e26b57da07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269775311F0>]}
[0m12:12:54.599412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40a90560-5014-4d42-94e0-b1e26b57da07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026977AA0440>]}
[0m12:12:54.599412 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:12:54.952706 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m12:12:55.082244 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:12:55.082244 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:12:55.091187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40a90560-5014-4d42-94e0-b1e26b57da07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026978B0C410>]}
[0m12:12:55.142606 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:12:55.151450 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:12:55.214977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40a90560-5014-4d42-94e0-b1e26b57da07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026978B35490>]}
[0m12:12:55.214977 [info ] [MainThread]: Found 433 macros
[0m12:12:55.214977 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m12:12:55.214977 [debug] [MainThread]: Command end result
[0m12:12:55.242896 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:12:55.249444 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:12:55.249444 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:12:55.256266 [debug] [MainThread]: Command `dbt build` succeeded at 12:12:55.256266 after 1.48 seconds
[0m12:12:55.258431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026977A0CFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026978B01E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026978B03500>]}
[0m12:12:55.258431 [debug] [MainThread]: Flushing usage events
[0m12:12:55.624546 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:40:48.162916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C9086F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C9086E40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C9B2FC20>]}


============================== 12:40:48.172520 | b4abb57e-dc75-432a-9de5-c66d682406aa ==============================
[0m12:40:48.172520 [info ] [MainThread]: Running with dbt=1.9.4
[0m12:40:48.173580 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:40:48.612421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4abb57e-dc75-432a-9de5-c66d682406aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C9C9DC70>]}
[0m12:40:48.716510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4abb57e-dc75-432a-9de5-c66d682406aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C8A68620>]}
[0m12:40:48.725882 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:40:49.165970 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m12:40:49.275232 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:40:49.276286 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:40:49.283426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4abb57e-dc75-432a-9de5-c66d682406aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228CA1C5550>]}
[0m12:40:49.320584 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:40:49.333395 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:40:49.608564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4abb57e-dc75-432a-9de5-c66d682406aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228CB771EB0>]}
[0m12:40:49.609187 [info ] [MainThread]: Found 433 macros
[0m12:40:49.611746 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m12:40:49.614187 [debug] [MainThread]: Command end result
[0m12:40:49.635251 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:40:49.638539 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:40:49.644851 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:40:49.646638 [debug] [MainThread]: Command `dbt build` succeeded at 12:40:49.646254 after 1.85 seconds
[0m12:40:49.646982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C93DEEA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228C9368560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000228CB37F950>]}
[0m12:40:49.646982 [debug] [MainThread]: Flushing usage events
[0m12:40:50.012377 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:40:32.256868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCE3A6E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCE2B3B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCB71FAD0>]}


============================== 04:40:32.261622 | defed430-f847-482a-8614-e44e89ea1320 ==============================
[0m04:40:32.261622 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:40:32.261622 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'send_anonymous_usage_stats': 'True'}
[0m04:40:32.461867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'defed430-f847-482a-8614-e44e89ea1320', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCB954410>]}
[0m04:40:32.525350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'defed430-f847-482a-8614-e44e89ea1320', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCE820C50>]}
[0m04:40:32.525350 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:40:32.694696 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:40:32.725307 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m04:40:32.725307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'defed430-f847-482a-8614-e44e89ea1320', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCE3A6720>]}
[0m04:40:33.323070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'defed430-f847-482a-8614-e44e89ea1320', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FD00675F0>]}
[0m04:40:33.353329 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:40:33.353329 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:40:33.447998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'defed430-f847-482a-8614-e44e89ea1320', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FD0066D20>]}
[0m04:40:33.447998 [info ] [MainThread]: Found 433 macros
[0m04:40:33.447998 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m04:40:33.447998 [debug] [MainThread]: Command end result
[0m04:40:33.463740 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:40:33.463740 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:40:33.463740 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m04:40:33.463740 [debug] [MainThread]: Command `dbt build` succeeded at 04:40:33.463740 after 1.41 seconds
[0m04:40:33.463740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCE318140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCDCB2120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FCDCB1400>]}
[0m04:40:33.463740 [debug] [MainThread]: Flushing usage events
[0m04:40:33.887203 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:45:22.326974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002982DD963F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298314EACF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298314E92E0>]}


============================== 04:45:22.331343 | 8da6c4b5-9029-4879-a4a3-8266baa7e971 ==============================
[0m04:45:22.331343 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:45:22.331343 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:45:22.512532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8da6c4b5-9029-4879-a4a3-8266baa7e971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029831812330>]}
[0m04:45:22.586901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8da6c4b5-9029-4879-a4a3-8266baa7e971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029832C16A80>]}
[0m04:45:22.586901 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:45:22.799113 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:45:22.824405 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m04:45:22.824405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8da6c4b5-9029-4879-a4a3-8266baa7e971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002983330F2F0>]}
[0m04:45:23.307970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8da6c4b5-9029-4879-a4a3-8266baa7e971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002983339AD20>]}
[0m04:45:23.330846 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:45:23.332852 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:45:23.360318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8da6c4b5-9029-4879-a4a3-8266baa7e971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298333A6930>]}
[0m04:45:23.360318 [info ] [MainThread]: Found 433 macros
[0m04:45:23.362335 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m04:45:23.362335 [debug] [MainThread]: Command end result
[0m04:45:23.372982 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:45:23.376313 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:45:23.380493 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m04:45:23.380493 [debug] [MainThread]: Command `dbt build` succeeded at 04:45:23.380493 after 1.24 seconds
[0m04:45:23.381626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002982DD963F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002982B719A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298318E7620>]}
[0m04:45:23.383923 [debug] [MainThread]: Flushing usage events
[0m04:45:23.797373 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:48:45.494263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E9356D20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8C62270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E93569F0>]}


============================== 04:48:45.497919 | 77daa29f-0b47-4261-8a70-65b091830c85 ==============================
[0m04:48:45.497919 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:48:45.498451 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:48:45.626297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '77daa29f-0b47-4261-8a70-65b091830c85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8EA8E3CE0>]}
[0m04:48:45.700531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '77daa29f-0b47-4261-8a70-65b091830c85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E9639D00>]}
[0m04:48:45.700531 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:48:45.900727 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:48:45.934062 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m04:48:45.936960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '77daa29f-0b47-4261-8a70-65b091830c85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E93565D0>]}
[0m04:48:46.819901 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline.raw_to_normalized' (../../models\raw_to_normalized.sql) depends on a source named 'raw.raw_data' which was not found
[0m04:48:46.819901 [debug] [MainThread]: Command `dbt build` failed at 04:48:46.819901 after 1.50 seconds
[0m04:48:46.819901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E97AB5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8EB1D02C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8EB256240>]}
[0m04:48:46.819901 [debug] [MainThread]: Flushing usage events
[0m04:48:47.238159 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:00:01.823459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002090F0BDC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911EB73B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911EB4B00>]}


============================== 05:00:01.826353 | 6f92e768-44e0-4c69-9869-94e4953ff19c ==============================
[0m05:00:01.826353 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:00:01.826353 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:00:01.969232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f92e768-44e0-4c69-9869-94e4953ff19c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209136A9580>]}
[0m05:00:02.018064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f92e768-44e0-4c69-9869-94e4953ff19c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020912217770>]}
[0m05:00:02.018064 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:00:02.208140 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:00:02.238574 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m05:00:02.239764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6f92e768-44e0-4c69-9869-94e4953ff19c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002091396EF60>]}
[0m05:00:03.233558 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline.raw_to_normalized' (../../models\raw_to_normalized.sql) depends on a source named 'raw.raw_data' which was not found
[0m05:00:03.233558 [debug] [MainThread]: Command `dbt build` failed at 05:00:03.233558 after 1.60 seconds
[0m05:00:03.233558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911F8DBE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020911301550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209139E8E30>]}
[0m05:00:03.233558 [debug] [MainThread]: Flushing usage events
[0m05:00:03.656274 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:02:04.613108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F78174C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7BCA6BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7BCA4CE0>]}


============================== 05:02:04.617183 | a2fc53bb-7365-49f2-b3a0-4f80bc671ae2 ==============================
[0m05:02:04.617183 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:02:04.617183 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt build', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m05:02:04.762270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2fc53bb-7365-49f2-b3a0-4f80bc671ae2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F79254320>]}
[0m05:02:04.804081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2fc53bb-7365-49f2-b3a0-4f80bc671ae2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7947FAD0>]}
[0m05:02:04.804081 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:02:04.966689 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:02:04.992974 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m05:02:04.992974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a2fc53bb-7365-49f2-b3a0-4f80bc671ae2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7D927F20>]}
[0m05:02:06.047869 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline.raw_to_normalized' (../../models\raw_to_normalized.sql) depends on a source named 'raw.raw_data' which was not found
[0m05:02:06.047869 [debug] [MainThread]: Command `dbt build` failed at 05:02:06.047869 after 1.77 seconds
[0m05:02:06.047869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7901FC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7B63DCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F7DAC8980>]}
[0m05:02:06.047869 [debug] [MainThread]: Flushing usage events
[0m05:02:06.474083 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:03:27.699819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002026FF366C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002026D9C74D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202707EC440>]}


============================== 05:03:27.701827 | a3d9a0d6-1da8-4857-abb2-44b92716f22f ==============================
[0m05:03:27.701827 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:03:27.701827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt list --resource-type source', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:03:27.836619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a3d9a0d6-1da8-4857-abb2-44b92716f22f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202709C34A0>]}
[0m05:03:27.886431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a3d9a0d6-1da8-4857-abb2-44b92716f22f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020271B54860>]}
[0m05:03:27.888438 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:03:28.112181 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:03:28.148208 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m05:03:28.150217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a3d9a0d6-1da8-4857-abb2-44b92716f22f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202721FB7D0>]}
[0m05:03:29.206145 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline.raw_to_normalized' (../../models\raw_to_normalized.sql) depends on a source named 'raw.raw_data' which was not found
[0m05:03:29.217687 [debug] [MainThread]: Command `dbt list` failed at 05:03:29.217687 after 1.73 seconds
[0m05:03:29.219702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202706E9820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202723768A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002026F9A6120>]}
[0m05:03:29.219702 [debug] [MainThread]: Flushing usage events
[0m05:03:29.630835 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:04:33.377385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002405FBC7080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024062722600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240627215B0>]}


============================== 05:04:33.381251 | 5220d0e3-241a-47a7-a30c-9fab1382aa5b ==============================
[0m05:04:33.381251 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:04:33.383255 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:04:33.537828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5220d0e3-241a-47a7-a30c-9fab1382aa5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002405FBC7080>]}
[0m05:04:33.608848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5220d0e3-241a-47a7-a30c-9fab1382aa5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240628A1C70>]}
[0m05:04:33.609850 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:04:33.872634 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:04:33.910043 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m05:04:33.911062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5220d0e3-241a-47a7-a30c-9fab1382aa5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240627466C0>]}
[0m05:04:34.947580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5220d0e3-241a-47a7-a30c-9fab1382aa5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240643E5760>]}
[0m05:04:35.004983 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:04:35.004983 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:04:35.055385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5220d0e3-241a-47a7-a30c-9fab1382aa5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024063DE7740>]}
[0m05:04:35.055924 [info ] [MainThread]: Found 1 model, 21 data tests, 3 sources, 433 macros
[0m05:04:35.061956 [info ] [MainThread]: 
[0m05:04:35.062461 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:04:35.065806 [info ] [MainThread]: 
[0m05:04:35.065806 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:04:35.071818 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m05:04:35.163037 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m05:04:35.163621 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m05:04:35.163621 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:04:37.871039 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
could not translate host name "postgres_dw" to address: No such host is known. 

[0m05:04:40.550111 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m05:04:40.552116 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m05:04:40.552116 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m05:04:40.554122 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m05:04:40.554122 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m05:04:40.556128 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:04:40.556128 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m05:04:40.556128 [info ] [MainThread]: 
[0m05:04:40.558134 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 5.49 seconds (5.49s).
[0m05:04:40.558134 [error] [MainThread]: Encountered an error:
Database Error
  could not translate host name "postgres_dw" to address: No such host is known. 
  
[0m05:04:40.562145 [debug] [MainThread]: Command `dbt build` failed at 05:04:40.562145 after 7.36 seconds
[0m05:04:40.562145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024061BA5CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024063DE5F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024062780BF0>]}
[0m05:04:40.562145 [debug] [MainThread]: Flushing usage events
[0m05:04:40.945480 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:13:15.683405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F2376C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F24D1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F1D623F0>]}


============================== 05:13:15.684222 | 9b95d384-d9cd-4efe-8429-4566bca48e79 ==============================
[0m05:13:15.684222 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:13:15.687772 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m05:13:15.702634 [info ] [MainThread]: dbt version: 1.9.6
[0m05:13:15.702634 [info ] [MainThread]: python version: 3.12.6
[0m05:13:15.702634 [info ] [MainThread]: python path: C:\Project\DataPipeline\venv\Scripts\python.exe
[0m05:13:15.703708 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m05:13:15.742996 [info ] [MainThread]: Using profiles dir at C:\Users\Gaurav Chugh\.dbt
[0m05:13:15.746837 [info ] [MainThread]: Using profiles.yml file at C:\Users\Gaurav Chugh\.dbt\profiles.yml
[0m05:13:15.746837 [info ] [MainThread]: Using dbt_project.yml file at C:\Project\DataPipeline\dags\dbt_project\dbt_project.yml
[0m05:13:15.747853 [info ] [MainThread]: adapter type: postgres
[0m05:13:15.748849 [info ] [MainThread]: adapter version: 1.9.0
[0m05:13:15.830953 [info ] [MainThread]: Configuration:
[0m05:13:15.835971 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m05:13:15.835971 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m05:13:15.835971 [info ] [MainThread]: Required dependencies:
[0m05:13:15.835971 [debug] [MainThread]: Executing "git --help"
[0m05:13:15.875048 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m05:13:15.876049 [debug] [MainThread]: STDERR: "b''"
[0m05:13:15.876049 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m05:13:15.876049 [info ] [MainThread]: Connection:
[0m05:13:15.877051 [info ] [MainThread]:   host: localhost
[0m05:13:15.877051 [info ] [MainThread]:   port: 5432
[0m05:13:15.877051 [info ] [MainThread]:   user: dwh_user
[0m05:13:15.878049 [info ] [MainThread]:   database: datamart
[0m05:13:15.878049 [info ] [MainThread]:   schema: public
[0m05:13:15.879049 [info ] [MainThread]:   connect_timeout: 10
[0m05:13:15.879049 [info ] [MainThread]:   role: None
[0m05:13:15.879049 [info ] [MainThread]:   search_path: None
[0m05:13:15.879049 [info ] [MainThread]:   keepalives_idle: 0
[0m05:13:15.880048 [info ] [MainThread]:   sslmode: None
[0m05:13:15.880048 [info ] [MainThread]:   sslcert: None
[0m05:13:15.881048 [info ] [MainThread]:   sslkey: None
[0m05:13:15.881048 [info ] [MainThread]:   sslrootcert: None
[0m05:13:15.881048 [info ] [MainThread]:   application_name: dbt
[0m05:13:15.882048 [info ] [MainThread]:   retries: 1
[0m05:13:15.882048 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:13:16.021640 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m05:13:16.089851 [debug] [MainThread]: Using postgres connection "debug"
[0m05:13:16.094228 [debug] [MainThread]: On debug: select 1 as id
[0m05:13:16.094228 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:13:16.193928 [debug] [MainThread]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m05:13:16.242148 [debug] [MainThread]: Postgres adapter: Error running SQL: select 1 as id
[0m05:13:16.242148 [debug] [MainThread]: Postgres adapter: Rolling back transaction.
[0m05:13:16.243694 [debug] [MainThread]: On debug: No close available on handle
[0m05:13:16.243694 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m05:13:16.243694 [info ] [MainThread]: [31m1 check failed:[0m
[0m05:13:16.243694 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m05:13:16.243694 [debug] [MainThread]: Command `dbt debug` failed at 05:13:16.243694 after 0.73 seconds
[0m05:13:16.243694 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m05:13:16.243694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F23B2630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F25E0CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7F25E1A00>]}
[0m05:13:16.243694 [debug] [MainThread]: Flushing usage events
[0m05:13:16.644318 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:15:04.130765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001653FA51A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001653CFFCAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016540511400>]}


============================== 05:15:04.139506 | 3c44796f-a673-46ce-b2fc-d6ceb2f41d5f ==============================
[0m05:15:04.139506 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:15:04.140013 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:15:04.157986 [info ] [MainThread]: dbt version: 1.9.6
[0m05:15:04.159003 [info ] [MainThread]: python version: 3.12.6
[0m05:15:04.159999 [info ] [MainThread]: python path: C:\Project\DataPipeline\venv\Scripts\python.exe
[0m05:15:04.161332 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m05:15:04.216188 [info ] [MainThread]: Using profiles dir at C:\Users\Gaurav Chugh\.dbt
[0m05:15:04.216188 [info ] [MainThread]: Using profiles.yml file at C:\Users\Gaurav Chugh\.dbt\profiles.yml
[0m05:15:04.216188 [info ] [MainThread]: Using dbt_project.yml file at C:\Project\DataPipeline\dags\dbt_project\dbt_project.yml
[0m05:15:04.216188 [info ] [MainThread]: adapter type: postgres
[0m05:15:04.216188 [info ] [MainThread]: adapter version: 1.9.0
[0m05:15:04.353101 [info ] [MainThread]: Configuration:
[0m05:15:04.356308 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m05:15:04.356308 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m05:15:04.356967 [info ] [MainThread]: Required dependencies:
[0m05:15:04.357677 [debug] [MainThread]: Executing "git --help"
[0m05:15:04.388965 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m05:15:04.389492 [debug] [MainThread]: STDERR: "b''"
[0m05:15:04.390015 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m05:15:04.390354 [info ] [MainThread]: Connection:
[0m05:15:04.391357 [info ] [MainThread]:   host: localhost
[0m05:15:04.391357 [info ] [MainThread]:   port: 5432
[0m05:15:04.391867 [info ] [MainThread]:   user: dwh_user
[0m05:15:04.391867 [info ] [MainThread]:   database: datamart
[0m05:15:04.392872 [info ] [MainThread]:   schema: public
[0m05:15:04.393375 [info ] [MainThread]:   connect_timeout: 10
[0m05:15:04.393375 [info ] [MainThread]:   role: None
[0m05:15:04.393375 [info ] [MainThread]:   search_path: None
[0m05:15:04.393375 [info ] [MainThread]:   keepalives_idle: 0
[0m05:15:04.394379 [info ] [MainThread]:   sslmode: None
[0m05:15:04.394379 [info ] [MainThread]:   sslcert: None
[0m05:15:04.394379 [info ] [MainThread]:   sslkey: None
[0m05:15:04.394379 [info ] [MainThread]:   sslrootcert: None
[0m05:15:04.395380 [info ] [MainThread]:   application_name: dbt
[0m05:15:04.395380 [info ] [MainThread]:   retries: 1
[0m05:15:04.396381 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:15:04.550624 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m05:15:04.598601 [debug] [MainThread]: Using postgres connection "debug"
[0m05:15:04.598601 [debug] [MainThread]: On debug: select 1 as id
[0m05:15:04.598601 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:15:04.661520 [debug] [MainThread]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m05:15:04.711324 [debug] [MainThread]: Postgres adapter: Error running SQL: select 1 as id
[0m05:15:04.711324 [debug] [MainThread]: Postgres adapter: Rolling back transaction.
[0m05:15:04.713330 [debug] [MainThread]: On debug: No close available on handle
[0m05:15:04.713330 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m05:15:04.714506 [info ] [MainThread]: [31m1 check failed:[0m
[0m05:15:04.715502 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m05:15:04.716303 [debug] [MainThread]: Command `dbt debug` failed at 05:15:04.716303 after 0.80 seconds
[0m05:15:04.716303 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m05:15:04.716303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001653CFFCAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016540762A80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016540216870>]}
[0m05:15:04.716303 [debug] [MainThread]: Flushing usage events
[0m05:15:05.078607 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:16:20.273321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8BB7FA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8BB7C380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F91A95F10>]}


============================== 05:16:20.274334 | 538f267f-e6e9-469e-a327-116c19b7653c ==============================
[0m05:16:20.274334 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:16:20.274334 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m05:16:20.293732 [info ] [MainThread]: dbt version: 1.9.6
[0m05:16:20.293732 [info ] [MainThread]: python version: 3.12.6
[0m05:16:20.295740 [info ] [MainThread]: python path: C:\Project\DataPipeline\venv\Scripts\python.exe
[0m05:16:20.296254 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m05:16:20.343041 [info ] [MainThread]: Using profiles dir at C:\Users\Gaurav Chugh\.dbt
[0m05:16:20.344053 [info ] [MainThread]: Using profiles.yml file at C:\Users\Gaurav Chugh\.dbt\profiles.yml
[0m05:16:20.345053 [info ] [MainThread]: Using dbt_project.yml file at C:\Project\DataPipeline\dags\dbt_project\dbt_project.yml
[0m05:16:20.346053 [info ] [MainThread]: adapter type: postgres
[0m05:16:20.347053 [info ] [MainThread]: adapter version: 1.9.0
[0m05:16:20.418699 [info ] [MainThread]: Configuration:
[0m05:16:20.418699 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m05:16:20.418699 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m05:16:20.418699 [info ] [MainThread]: Required dependencies:
[0m05:16:20.418699 [debug] [MainThread]: Executing "git --help"
[0m05:16:20.449237 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m05:16:20.450720 [debug] [MainThread]: STDERR: "b''"
[0m05:16:20.451726 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m05:16:20.451726 [info ] [MainThread]: Connection:
[0m05:16:20.452725 [info ] [MainThread]:   host: localhost
[0m05:16:20.453422 [info ] [MainThread]:   port: 5432
[0m05:16:20.453422 [info ] [MainThread]:   user: dwh_user
[0m05:16:20.453422 [info ] [MainThread]:   database: datamart
[0m05:16:20.453422 [info ] [MainThread]:   schema: public
[0m05:16:20.453422 [info ] [MainThread]:   connect_timeout: 10
[0m05:16:20.453422 [info ] [MainThread]:   role: None
[0m05:16:20.453422 [info ] [MainThread]:   search_path: None
[0m05:16:20.453422 [info ] [MainThread]:   keepalives_idle: 0
[0m05:16:20.453422 [info ] [MainThread]:   sslmode: None
[0m05:16:20.456812 [info ] [MainThread]:   sslcert: None
[0m05:16:20.457315 [info ] [MainThread]:   sslkey: None
[0m05:16:20.457650 [info ] [MainThread]:   sslrootcert: None
[0m05:16:20.457650 [info ] [MainThread]:   application_name: dbt
[0m05:16:20.460105 [info ] [MainThread]:   retries: 1
[0m05:16:20.460105 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:16:20.627554 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m05:16:20.676235 [debug] [MainThread]: Using postgres connection "debug"
[0m05:16:20.676235 [debug] [MainThread]: On debug: select 1 as id
[0m05:16:20.676235 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:16:20.857120 [debug] [MainThread]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m05:16:20.996980 [debug] [MainThread]: Postgres adapter: Error running SQL: select 1 as id
[0m05:16:21.006714 [debug] [MainThread]: Postgres adapter: Rolling back transaction.
[0m05:16:21.006714 [debug] [MainThread]: On debug: No close available on handle
[0m05:16:21.006714 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m05:16:21.006714 [info ] [MainThread]: [31m1 check failed:[0m
[0m05:16:21.009641 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m05:16:21.009641 [debug] [MainThread]: Command `dbt debug` failed at 05:16:21.009641 after 0.96 seconds
[0m05:16:21.009641 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m05:16:21.009641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F917FA720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F930177A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F91011BE0>]}
[0m05:16:21.009641 [debug] [MainThread]: Flushing usage events
[0m05:16:21.416626 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:52.926906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D44806F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D47427EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D472F8200>]}


============================== 14:24:52.969721 | cb1a702a-78ef-46fc-acd2-e07d452915ca ==============================
[0m14:24:52.969721 [info ] [MainThread]: Running with dbt=1.9.6
[0m14:24:52.971597 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:24:53.495968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D46C924B0>]}
[0m14:24:53.626422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D47425730>]}
[0m14:24:53.633257 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m14:24:54.065897 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m14:24:54.231745 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:24:54.234021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D4906BD10>]}
[0m14:24:55.819216 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'customers' in the 'models' section of file '../../models\schema.yml'
[0m14:24:55.824792 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'orders' in the 'models' section of file '../../models\schema.yml'
[0m14:24:55.831039 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'products' in the 'models' section of file '../../models\schema.yml'
[0m14:24:55.838083 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'reviews' in the 'models' section of file '../../models\schema.yml'
[0m14:24:56.585150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D492D8D70>]}
[0m14:24:56.801624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D492CCEC0>]}
[0m14:24:56.802139 [info ] [MainThread]: Found 1 model, 18 data tests, 3 sources, 433 macros
[0m14:24:56.803444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb1a702a-78ef-46fc-acd2-e07d452915ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D49216FC0>]}
[0m14:24:56.806345 [info ] [MainThread]: 
[0m14:24:56.806955 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:56.808404 [info ] [MainThread]: 
[0m14:24:56.810051 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:24:56.811783 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m14:24:56.952788 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m14:24:56.953881 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m14:24:56.954423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:57.039181 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m14:24:57.136247 [debug] [ThreadPool]: Postgres adapter: Error running SQL: BEGIN
[0m14:24:57.137262 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m14:24:57.138260 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_relations_without_caching
[0m14:24:57.139273 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m14:24:57.139273 [debug] [ThreadPool]: On list_datamart_public: No close available on handle
[0m14:24:57.141793 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:57.142818 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m14:24:57.143807 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m14:24:57.145919 [debug] [MainThread]: Command `dbt docs generate` failed at 14:24:57.145919 after 4.81 seconds
[0m14:24:57.147449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D4766CDA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D492BB080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023D4925A750>]}
[0m14:24:57.147449 [debug] [MainThread]: Flushing usage events
[0m14:24:57.577918 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:35:52.375311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DF80A6F00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DF8A551C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DF8A54EC0>]}


============================== 23:35:52.397321 | d1557795-d657-4908-bcd3-d754ef2fb93c ==============================
[0m23:35:52.397321 [info ] [MainThread]: Running with dbt=1.9.6
[0m23:35:52.399830 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m23:35:52.895484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd1557795-d657-4908-bcd3-d754ef2fb93c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFC1671A0>]}
[0m23:35:53.009732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd1557795-d657-4908-bcd3-d754ef2fb93c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFC235EB0>]}
[0m23:35:53.017769 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m23:35:53.570644 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m23:35:53.891823 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:35:53.893431 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\customers.yml
[0m23:35:54.021143 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'orders' in the 'models' section of file '../../models\customers.yml'
[0m23:35:54.028435 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'products' in the 'models' section of file '../../models\customers.yml'
[0m23:35:54.033436 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'reviews' in the 'models' section of file '../../models\customers.yml'
[0m23:35:54.038499 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'customers' in the 'models' section of file '../../models\customers.yml'
[0m23:35:54.197358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd1557795-d657-4908-bcd3-d754ef2fb93c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFC2C6FC0>]}
[0m23:35:54.337139 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m23:35:54.353408 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m23:35:54.656322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd1557795-d657-4908-bcd3-d754ef2fb93c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFC2F23C0>]}
[0m23:35:54.657839 [info ] [MainThread]: Found 1 model, 18 data tests, 3 sources, 433 macros
[0m23:35:54.663458 [info ] [MainThread]: 
[0m23:35:54.664459 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:35:54.665463 [info ] [MainThread]: 
[0m23:35:54.666975 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m23:35:54.669499 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m23:35:54.921821 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m23:35:54.924072 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m23:35:54.925098 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:35:55.025551 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.100 seconds
[0m23:35:55.027302 [debug] [ThreadPool]: On list_datamart: Close
[0m23:35:55.030644 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m23:35:55.038173 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m23:35:55.038704 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m23:35:55.038704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:35:55.061555 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m23:35:55.062112 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m23:35:55.062661 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m23:35:55.094146 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.031 seconds
[0m23:35:55.095799 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m23:35:55.097994 [debug] [ThreadPool]: On list_datamart_public: Close
[0m23:35:55.106664 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:55.107766 [debug] [MainThread]: On master: BEGIN
[0m23:35:55.108327 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:35:55.155977 [debug] [MainThread]: SQL status: BEGIN in 0.048 seconds
[0m23:35:55.157115 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:55.157819 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m23:35:55.178034 [debug] [MainThread]: SQL status: SELECT 0 in 0.020 seconds
[0m23:35:55.181230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1557795-d657-4908-bcd3-d754ef2fb93c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFC879D90>]}
[0m23:35:55.182934 [debug] [MainThread]: On master: ROLLBACK
[0m23:35:55.186318 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:55.186871 [debug] [MainThread]: On master: BEGIN
[0m23:35:55.191180 [debug] [MainThread]: SQL status: BEGIN in 0.003 seconds
[0m23:35:55.191735 [debug] [MainThread]: On master: COMMIT
[0m23:35:55.192272 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:55.192801 [debug] [MainThread]: On master: COMMIT
[0m23:35:55.196005 [debug] [MainThread]: SQL status: COMMIT in 0.002 seconds
[0m23:35:55.196592 [debug] [MainThread]: On master: Close
[0m23:35:55.228578 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m23:35:55.230416 [info ] [Thread-1 (]: 1 of 19 START test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [RUN]
[0m23:35:55.232588 [debug] [Thread-1 (]: Acquiring new postgres connection 'test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6'
[0m23:35:55.233269 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m23:35:55.266428 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m23:35:55.269970 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m23:35:55.298656 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m23:35:55.299663 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m23:35:55.301174 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: BEGIN
[0m23:35:55.303042 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:35:55.329048 [debug] [Thread-1 (]: SQL status: BEGIN in 0.026 seconds
[0m23:35:55.329582 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m23:35:55.330700 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_method

)

select *
from all_values
where value_field not in (
    'Credit Card','PayPal','Bank Transfer'
)



  
  
      
    ) dbt_internal_test
[0m23:35:55.333902 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.payments" does not exist
LINE 20:     from "datamart"."raw"."payments"
                  ^

[0m23:35:55.334992 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: ROLLBACK
[0m23:35:55.337122 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: Close
[0m23:35:55.404047 [debug] [Thread-1 (]: Database Error in test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_81e9ba7a9392ec12ec328aa660bf3829.sql
[0m23:35:55.405566 [error] [Thread-1 (]: 1 of 19 ERROR source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [[31mERROR[0m in 0.17s]
[0m23:35:55.407077 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m23:35:55.408091 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m23:35:55.409094 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_81e9ba7a9392ec12ec328aa660bf3829.sql.
[0m23:35:55.410553 [info ] [Thread-1 (]: 2 of 19 START test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending  [RUN]
[0m23:35:55.416082 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6, now test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc)
[0m23:35:55.417078 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m23:35:55.435322 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m23:35:55.439905 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m23:35:55.447554 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m23:35:55.450554 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m23:35:55.451569 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: BEGIN
[0m23:35:55.452558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.492409 [debug] [Thread-1 (]: SQL status: BEGIN in 0.040 seconds
[0m23:35:55.493345 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m23:35:55.494119 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_status

)

select *
from all_values
where value_field not in (
    'Completed','Failed','Pending'
)



  
  
      
    ) dbt_internal_test
[0m23:35:55.497515 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.payments" does not exist
LINE 20:     from "datamart"."raw"."payments"
                  ^

[0m23:35:55.498074 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: ROLLBACK
[0m23:35:55.500449 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: Close
[0m23:35:55.503432 [debug] [Thread-1 (]: Database Error in test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_9e076b3898151400e6c50cdfdb2e3b16.sql
[0m23:35:55.504932 [error] [Thread-1 (]: 2 of 19 ERROR source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending  [[31mERROR[0m in 0.09s]
[0m23:35:55.508865 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m23:35:55.509431 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m23:35:55.513250 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_9e076b3898151400e6c50cdfdb2e3b16.sql.
[0m23:35:55.510961 [info ] [Thread-1 (]: 3 of 19 START test source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded  [RUN]
[0m23:35:55.514526 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc, now test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21)
[0m23:35:55.516036 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m23:35:55.526076 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m23:35:55.528591 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m23:35:55.531101 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m23:35:55.533115 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m23:35:55.533115 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: BEGIN
[0m23:35:55.534117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.577859 [debug] [Thread-1 (]: SQL status: BEGIN in 0.043 seconds
[0m23:35:55.578458 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m23:35:55.579863 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."raw_data"
    group by payment_status

)

select *
from all_values
where value_field not in (
    'Paid','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m23:35:55.582842 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 20:     from "datamart"."raw"."raw_data"
                  ^

[0m23:35:55.583365 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: ROLLBACK
[0m23:35:55.587330 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: Close
[0m23:35:55.592297 [debug] [Thread-1 (]: Database Error in test source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 20:     from "datamart"."raw"."raw_data"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_76585dff0454bdaee97924d29f62e64d.sql
[0m23:35:55.593308 [error] [Thread-1 (]: 3 of 19 ERROR source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded  [[31mERROR[0m in 0.08s]
[0m23:35:55.594686 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m23:35:55.595973 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m23:35:55.596986 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 20:     from "datamart"."raw"."raw_data"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_76585dff0454bdaee97924d29f62e64d.sql.
[0m23:35:55.598216 [info ] [Thread-1 (]: 4 of 19 START test source_not_null_raw_payments_order_id ....................... [RUN]
[0m23:35:55.599523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21, now test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9)
[0m23:35:55.600180 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m23:35:55.615328 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m23:35:55.624468 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m23:35:55.630008 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m23:35:55.634533 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m23:35:55.634533 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: BEGIN
[0m23:35:55.635535 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.667927 [debug] [Thread-1 (]: SQL status: BEGIN in 0.032 seconds
[0m23:35:55.668578 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m23:35:55.669835 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."payments"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m23:35:55.673431 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.payments" does not exist
LINE 17: from "datamart"."raw"."payments"
              ^

[0m23:35:55.676027 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: ROLLBACK
[0m23:35:55.680676 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: Close
[0m23:35:55.685348 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_payments_order_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_order_id.sql
[0m23:35:55.686362 [error] [Thread-1 (]: 4 of 19 ERROR source_not_null_raw_payments_order_id ............................ [[31mERROR[0m in 0.09s]
[0m23:35:55.687368 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m23:35:55.688380 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m23:35:55.689383 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_payments_order_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_order_id.sql.
[0m23:35:55.690381 [info ] [Thread-1 (]: 5 of 19 START test source_not_null_raw_payments_payment_id ..................... [RUN]
[0m23:35:55.691629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9, now test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5)
[0m23:35:55.692640 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m23:35:55.699460 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m23:35:55.701462 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m23:35:55.707975 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m23:35:55.709989 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m23:35:55.711506 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: BEGIN
[0m23:35:55.712529 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.743362 [debug] [Thread-1 (]: SQL status: BEGIN in 0.031 seconds
[0m23:35:55.744149 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m23:35:55.744954 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select payment_id
from "datamart"."raw"."payments"
where payment_id is null



  
  
      
    ) dbt_internal_test
[0m23:35:55.748382 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.payments" does not exist
LINE 17: from "datamart"."raw"."payments"
              ^

[0m23:35:55.748382 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: ROLLBACK
[0m23:35:55.751593 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: Close
[0m23:35:55.756034 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_payment_id.sql
[0m23:35:55.757213 [error] [Thread-1 (]: 5 of 19 ERROR source_not_null_raw_payments_payment_id .......................... [[31mERROR[0m in 0.07s]
[0m23:35:55.758424 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m23:35:55.759445 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m23:35:55.760464 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_payment_id.sql.
[0m23:35:55.761456 [info ] [Thread-1 (]: 6 of 19 START test source_not_null_raw_products_category ....................... [RUN]
[0m23:35:55.762516 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5, now test.data_pipeline.source_not_null_raw_products_category.9265557239)
[0m23:35:55.763486 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m23:35:55.771339 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m23:35:55.773433 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m23:35:55.777458 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m23:35:55.779932 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m23:35:55.779932 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: BEGIN
[0m23:35:55.781448 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.810028 [debug] [Thread-1 (]: SQL status: BEGIN in 0.028 seconds
[0m23:35:55.810028 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m23:35:55.811600 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_category.9265557239"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select category
from "datamart"."raw"."products"
where category is null



  
  
      
    ) dbt_internal_test
[0m23:35:55.815150 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.products" does not exist
LINE 17: from "datamart"."raw"."products"
              ^

[0m23:35:55.816291 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: ROLLBACK
[0m23:35:55.819455 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: Close
[0m23:35:55.823691 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_products_category (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_category.sql
[0m23:35:55.824910 [error] [Thread-1 (]: 6 of 19 ERROR source_not_null_raw_products_category ............................ [[31mERROR[0m in 0.06s]
[0m23:35:55.826745 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m23:35:55.827286 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m23:35:55.828873 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_products_category.9265557239' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_products_category (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_category.sql.
[0m23:35:55.828208 [info ] [Thread-1 (]: 7 of 19 START test source_not_null_raw_products_price .......................... [RUN]
[0m23:35:55.830899 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_category.9265557239, now test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b)
[0m23:35:55.831922 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m23:35:55.839085 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m23:35:55.840083 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m23:35:55.845095 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m23:35:55.846119 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m23:35:55.847145 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: BEGIN
[0m23:35:55.847145 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.880470 [debug] [Thread-1 (]: SQL status: BEGIN in 0.032 seconds
[0m23:35:55.881144 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m23:35:55.882411 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select price
from "datamart"."raw"."products"
where price is null



  
  
      
    ) dbt_internal_test
[0m23:35:55.885003 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.products" does not exist
LINE 17: from "datamart"."raw"."products"
              ^

[0m23:35:55.886219 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: ROLLBACK
[0m23:35:55.888234 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: Close
[0m23:35:55.892899 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_products_price (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_price.sql
[0m23:35:55.893512 [error] [Thread-1 (]: 7 of 19 ERROR source_not_null_raw_products_price ............................... [[31mERROR[0m in 0.06s]
[0m23:35:55.895508 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m23:35:55.896340 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m23:35:55.896340 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_products_price (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_price.sql.
[0m23:35:55.898893 [info ] [Thread-1 (]: 8 of 19 START test source_not_null_raw_products_product_id ..................... [RUN]
[0m23:35:55.902515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b, now test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae)
[0m23:35:55.904088 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m23:35:55.913135 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m23:35:55.916318 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m23:35:55.922398 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m23:35:55.922958 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m23:35:55.925485 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: BEGIN
[0m23:35:55.927263 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:55.958682 [debug] [Thread-1 (]: SQL status: BEGIN in 0.032 seconds
[0m23:35:55.959273 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m23:35:55.959978 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."products"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m23:35:55.964343 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.products" does not exist
LINE 17: from "datamart"."raw"."products"
              ^

[0m23:35:55.966010 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: ROLLBACK
[0m23:35:55.970937 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: Close
[0m23:35:55.973973 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_id.sql
[0m23:35:55.975936 [error] [Thread-1 (]: 8 of 19 ERROR source_not_null_raw_products_product_id .......................... [[31mERROR[0m in 0.07s]
[0m23:35:55.977003 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m23:35:55.978207 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m23:35:55.980646 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_id.sql.
[0m23:35:55.978834 [info ] [Thread-1 (]: 9 of 19 START test source_not_null_raw_products_product_name ................... [RUN]
[0m23:35:55.982943 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae, now test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51)
[0m23:35:55.983625 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m23:35:55.994220 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m23:35:55.996734 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m23:35:56.003229 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m23:35:56.006320 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m23:35:56.007323 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: BEGIN
[0m23:35:56.008862 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.044851 [debug] [Thread-1 (]: SQL status: BEGIN in 0.036 seconds
[0m23:35:56.047839 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m23:35:56.048984 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_name
from "datamart"."raw"."products"
where product_name is null



  
  
      
    ) dbt_internal_test
[0m23:35:56.051217 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.products" does not exist
LINE 17: from "datamart"."raw"."products"
              ^

[0m23:35:56.051781 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: ROLLBACK
[0m23:35:56.053415 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: Close
[0m23:35:56.056660 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_products_product_name (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_name.sql
[0m23:35:56.057759 [error] [Thread-1 (]: 9 of 19 ERROR source_not_null_raw_products_product_name ........................ [[31mERROR[0m in 0.07s]
[0m23:35:56.058562 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m23:35:56.058562 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m23:35:56.059577 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_products_product_name (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_name.sql.
[0m23:35:56.060577 [info ] [Thread-1 (]: 10 of 19 START test source_not_null_raw_raw_data_customer_id ................... [RUN]
[0m23:35:56.061596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51, now test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3)
[0m23:35:56.061596 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m23:35:56.066119 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m23:35:56.067625 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m23:35:56.069634 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m23:35:56.071142 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m23:35:56.072157 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: BEGIN
[0m23:35:56.073161 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.108371 [debug] [Thread-1 (]: SQL status: BEGIN in 0.035 seconds
[0m23:35:56.108933 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m23:35:56.109555 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."raw_data"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m23:35:56.111831 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 17: from "datamart"."raw"."raw_data"
              ^

[0m23:35:56.112919 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: ROLLBACK
[0m23:35:56.115230 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: Close
[0m23:35:56.118986 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_customer_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_customer_id.sql
[0m23:35:56.120101 [error] [Thread-1 (]: 10 of 19 ERROR source_not_null_raw_raw_data_customer_id ........................ [[31mERROR[0m in 0.06s]
[0m23:35:56.120617 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m23:35:56.121241 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m23:35:56.121893 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_customer_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_customer_id.sql.
[0m23:35:56.123055 [info ] [Thread-1 (]: 11 of 19 START test source_not_null_raw_raw_data_order_date .................... [RUN]
[0m23:35:56.123770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3, now test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc)
[0m23:35:56.124423 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m23:35:56.129888 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m23:35:56.131397 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m23:35:56.135415 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m23:35:56.136415 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m23:35:56.137414 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: BEGIN
[0m23:35:56.137414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.170640 [debug] [Thread-1 (]: SQL status: BEGIN in 0.032 seconds
[0m23:35:56.171306 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m23:35:56.172487 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_date
from "datamart"."raw"."raw_data"
where order_date is null



  
  
      
    ) dbt_internal_test
[0m23:35:56.175531 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 17: from "datamart"."raw"."raw_data"
              ^

[0m23:35:56.176240 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: ROLLBACK
[0m23:35:56.178879 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: Close
[0m23:35:56.182864 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_order_date (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_date.sql
[0m23:35:56.184015 [error] [Thread-1 (]: 11 of 19 ERROR source_not_null_raw_raw_data_order_date ......................... [[31mERROR[0m in 0.06s]
[0m23:35:56.185416 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m23:35:56.186039 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m23:35:56.187221 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_order_date (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_date.sql.
[0m23:35:56.187810 [info ] [Thread-1 (]: 12 of 19 START test source_not_null_raw_raw_data_order_id ...................... [RUN]
[0m23:35:56.189497 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc, now test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6)
[0m23:35:56.190148 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m23:35:56.196209 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m23:35:56.198745 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m23:35:56.201260 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m23:35:56.203193 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m23:35:56.203760 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: BEGIN
[0m23:35:56.204309 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.226799 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m23:35:56.227462 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m23:35:56.228365 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."raw_data"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m23:35:56.230549 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 17: from "datamart"."raw"."raw_data"
              ^

[0m23:35:56.231106 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: ROLLBACK
[0m23:35:56.232836 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: Close
[0m23:35:56.236290 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_id.sql
[0m23:35:56.236832 [error] [Thread-1 (]: 12 of 19 ERROR source_not_null_raw_raw_data_order_id ........................... [[31mERROR[0m in 0.05s]
[0m23:35:56.237916 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m23:35:56.238485 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m23:35:56.239042 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_id.sql.
[0m23:35:56.239607 [info ] [Thread-1 (]: 13 of 19 START test source_not_null_raw_raw_data_total_amount .................. [RUN]
[0m23:35:56.240722 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6, now test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e)
[0m23:35:56.241284 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m23:35:56.246369 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m23:35:56.247472 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m23:35:56.250235 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m23:35:56.251250 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m23:35:56.251250 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: BEGIN
[0m23:35:56.252250 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.278680 [debug] [Thread-1 (]: SQL status: BEGIN in 0.026 seconds
[0m23:35:56.279445 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m23:35:56.280122 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_amount
from "datamart"."raw"."raw_data"
where total_amount is null



  
  
      
    ) dbt_internal_test
[0m23:35:56.283317 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 17: from "datamart"."raw"."raw_data"
              ^

[0m23:35:56.283869 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: ROLLBACK
[0m23:35:56.286762 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: Close
[0m23:35:56.290376 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_total_amount (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_total_amount.sql
[0m23:35:56.290923 [error] [Thread-1 (]: 13 of 19 ERROR source_not_null_raw_raw_data_total_amount ....................... [[31mERROR[0m in 0.05s]
[0m23:35:56.292209 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m23:35:56.293307 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m23:35:56.294464 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_total_amount (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_total_amount.sql.
[0m23:35:56.294464 [info ] [Thread-1 (]: 14 of 19 START test source_positive_values_raw_products_price .................. [RUN]
[0m23:35:56.296015 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e, now test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9)
[0m23:35:56.296878 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m23:35:56.327746 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m23:35:56.328932 [error] [Thread-1 (]: 14 of 19 ERROR source_positive_values_raw_products_price ....................... [[31mERROR[0m in 0.03s]
[0m23:35:56.330102 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m23:35:56.330755 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m23:35:56.331314 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m23:35:56.331879 [info ] [Thread-1 (]: 15 of 19 START test source_positive_values_raw_raw_data_total_amount ........... [RUN]
[0m23:35:56.332443 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9, now test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3)
[0m23:35:56.333019 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m23:35:56.340489 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m23:35:56.342708 [error] [Thread-1 (]: 15 of 19 ERROR source_positive_values_raw_raw_data_total_amount ................ [[31mERROR[0m in 0.01s]
[0m23:35:56.344693 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m23:35:56.345877 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m23:35:56.346443 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m23:35:56.346443 [info ] [Thread-1 (]: 16 of 19 START test source_unique_raw_payments_payment_id ...................... [RUN]
[0m23:35:56.348482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3, now test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533)
[0m23:35:56.349087 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m23:35:56.358822 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m23:35:56.360538 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m23:35:56.363359 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m23:35:56.365486 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m23:35:56.365998 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: BEGIN
[0m23:35:56.367325 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.390872 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m23:35:56.392013 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m23:35:56.392013 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."payments"
where payment_id is not null
group by payment_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:35:56.394260 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.payments" does not exist
LINE 18: from "datamart"."raw"."payments"
              ^

[0m23:35:56.394805 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: ROLLBACK
[0m23:35:56.396460 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: Close
[0m23:35:56.400311 [debug] [Thread-1 (]: Database Error in test source_unique_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 18: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_payments_payment_id.sql
[0m23:35:56.401548 [error] [Thread-1 (]: 16 of 19 ERROR source_unique_raw_payments_payment_id ........................... [[31mERROR[0m in 0.05s]
[0m23:35:56.402696 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m23:35:56.403293 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m23:35:56.403969 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 18: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_payments_payment_id.sql.
[0m23:35:56.404561 [info ] [Thread-1 (]: 17 of 19 START test source_unique_raw_products_product_id ...................... [RUN]
[0m23:35:56.405709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533, now test.data_pipeline.source_unique_raw_products_product_id.518dac90ba)
[0m23:35:56.406262 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m23:35:56.411173 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m23:35:56.412309 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m23:35:56.416150 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m23:35:56.418448 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m23:35:56.419032 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: BEGIN
[0m23:35:56.419596 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.461925 [debug] [Thread-1 (]: SQL status: BEGIN in 0.042 seconds
[0m23:35:56.463620 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m23:35:56.465421 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."products"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:35:56.468255 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.products" does not exist
LINE 18: from "datamart"."raw"."products"
              ^

[0m23:35:56.469636 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: ROLLBACK
[0m23:35:56.474074 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: Close
[0m23:35:56.481308 [debug] [Thread-1 (]: Database Error in test source_unique_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 18: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_products_product_id.sql
[0m23:35:56.482451 [error] [Thread-1 (]: 17 of 19 ERROR source_unique_raw_products_product_id ........................... [[31mERROR[0m in 0.08s]
[0m23:35:56.484172 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m23:35:56.485881 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m23:35:56.486509 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_unique_raw_products_product_id.518dac90ba' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 18: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_products_product_id.sql.
[0m23:35:56.487219 [info ] [Thread-1 (]: 18 of 19 START test source_unique_raw_raw_data_order_id ........................ [RUN]
[0m23:35:56.488361 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_products_product_id.518dac90ba, now test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd)
[0m23:35:56.488942 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m23:35:56.500333 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m23:35:56.502184 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m23:35:56.592852 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m23:35:56.595118 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m23:35:56.596220 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: BEGIN
[0m23:35:56.597236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:35:56.621502 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m23:35:56.622044 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m23:35:56.623143 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    order_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."raw_data"
where order_id is not null
group by order_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m23:35:56.626415 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "raw.raw_data" does not exist
LINE 18: from "datamart"."raw"."raw_data"
              ^

[0m23:35:56.627432 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: ROLLBACK
[0m23:35:56.630363 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: Close
[0m23:35:56.635602 [debug] [Thread-1 (]: Database Error in test source_unique_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 18: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_raw_data_order_id.sql
[0m23:35:56.636147 [error] [Thread-1 (]: 18 of 19 ERROR source_unique_raw_raw_data_order_id ............................. [[31mERROR[0m in 0.15s]
[0m23:35:56.637248 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m23:35:56.637809 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd' to be skipped because of status 'error'.  Reason: Database Error in test source_unique_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 18: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_raw_data_order_id.sql.
[0m23:35:56.638914 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m23:35:56.639477 [info ] [Thread-1 (]: 19 of 19 SKIP relation public.raw_to_normalized ................................ [[33mSKIP[0m]
[0m23:35:56.640040 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m23:35:56.640760 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.raw_to_normalized' to be skipped because of status 'skipped'. 
[0m23:35:56.642822 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:56.643396 [debug] [MainThread]: On master: BEGIN
[0m23:35:56.643971 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:35:56.674872 [debug] [MainThread]: SQL status: BEGIN in 0.031 seconds
[0m23:35:56.676019 [debug] [MainThread]: On master: COMMIT
[0m23:35:56.676680 [debug] [MainThread]: Using postgres connection "master"
[0m23:35:56.676680 [debug] [MainThread]: On master: COMMIT
[0m23:35:56.681764 [debug] [MainThread]: SQL status: COMMIT in 0.004 seconds
[0m23:35:56.682917 [debug] [MainThread]: On master: Close
[0m23:35:56.684036 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:35:56.685173 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m23:35:56.685743 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m23:35:56.687346 [debug] [MainThread]: Connection 'test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd' was properly closed.
[0m23:35:56.688415 [info ] [MainThread]: 
[0m23:35:56.690207 [info ] [MainThread]: Finished running 18 data tests, 1 view model in 0 hours 0 minutes and 2.02 seconds (2.02s).
[0m23:35:56.695588 [debug] [MainThread]: Command end result
[0m23:35:56.722416 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m23:35:56.726405 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m23:35:56.737311 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m23:35:56.738445 [info ] [MainThread]: 
[0m23:35:56.739442 [info ] [MainThread]: [31mCompleted with 18 errors, 0 partial successes, and 0 warnings:[0m
[0m23:35:56.740443 [info ] [MainThread]: 
[0m23:35:56.741793 [error] [MainThread]:   Database Error in test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_81e9ba7a9392ec12ec328aa660bf3829.sql
[0m23:35:56.742785 [info ] [MainThread]: 
[0m23:35:56.742785 [error] [MainThread]:   Database Error in test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 20:     from "datamart"."raw"."payments"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_9e076b3898151400e6c50cdfdb2e3b16.sql
[0m23:35:56.743791 [info ] [MainThread]: 
[0m23:35:56.745406 [error] [MainThread]:   Database Error in test source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 20:     from "datamart"."raw"."raw_data"
                    ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_76585dff0454bdaee97924d29f62e64d.sql
[0m23:35:56.746428 [info ] [MainThread]: 
[0m23:35:56.747427 [error] [MainThread]:   Database Error in test source_not_null_raw_payments_order_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_order_id.sql
[0m23:35:56.747943 [info ] [MainThread]: 
[0m23:35:56.748755 [error] [MainThread]:   Database Error in test source_not_null_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 17: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_payments_payment_id.sql
[0m23:35:56.749779 [info ] [MainThread]: 
[0m23:35:56.749779 [error] [MainThread]:   Database Error in test source_not_null_raw_products_category (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_category.sql
[0m23:35:56.751291 [info ] [MainThread]: 
[0m23:35:56.752308 [error] [MainThread]:   Database Error in test source_not_null_raw_products_price (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_price.sql
[0m23:35:56.752308 [info ] [MainThread]: 
[0m23:35:56.753306 [error] [MainThread]:   Database Error in test source_not_null_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_id.sql
[0m23:35:56.754306 [info ] [MainThread]: 
[0m23:35:56.754306 [error] [MainThread]:   Database Error in test source_not_null_raw_products_product_name (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 17: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_products_product_name.sql
[0m23:35:56.756077 [info ] [MainThread]: 
[0m23:35:56.757715 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_customer_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_customer_id.sql
[0m23:35:56.758745 [info ] [MainThread]: 
[0m23:35:56.759738 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_order_date (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_date.sql
[0m23:35:56.759738 [info ] [MainThread]: 
[0m23:35:56.761256 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_order_id.sql
[0m23:35:56.762767 [info ] [MainThread]: 
[0m23:35:56.764156 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_total_amount (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 17: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_total_amount.sql
[0m23:35:56.765731 [info ] [MainThread]: 
[0m23:35:56.767257 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m23:35:56.768279 [info ] [MainThread]: 
[0m23:35:56.769333 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m23:35:56.770352 [info ] [MainThread]: 
[0m23:35:56.771355 [error] [MainThread]:   Database Error in test source_unique_raw_payments_payment_id (../../models\sources.yml)
  relation "raw.payments" does not exist
  LINE 18: from "datamart"."raw"."payments"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_payments_payment_id.sql
[0m23:35:56.773351 [info ] [MainThread]: 
[0m23:35:56.775353 [error] [MainThread]:   Database Error in test source_unique_raw_products_product_id (../../models\sources.yml)
  relation "raw.products" does not exist
  LINE 18: from "datamart"."raw"."products"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_products_product_id.sql
[0m23:35:56.776660 [info ] [MainThread]: 
[0m23:35:56.778764 [error] [MainThread]:   Database Error in test source_unique_raw_raw_data_order_id (../../models\sources.yml)
  relation "raw.raw_data" does not exist
  LINE 18: from "datamart"."raw"."raw_data"
                ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_unique_raw_raw_data_order_id.sql
[0m23:35:56.779778 [info ] [MainThread]: 
[0m23:35:56.781334 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=18 SKIP=1 TOTAL=19
[0m23:35:56.786018 [debug] [MainThread]: Command `dbt build` failed at 23:35:56.785503 after 4.79 seconds
[0m23:35:56.787962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFAE519D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFAD810A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DFA2D7410>]}
[0m23:35:56.789151 [debug] [MainThread]: Flushing usage events
[0m23:35:57.238268 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:59:17.581830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EB2FBE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9E87E2240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EAC42DE0>]}


============================== 00:59:17.608513 | ec6ec2b9-f149-44cb-9a87-4328915e3e9e ==============================
[0m00:59:17.608513 [info ] [MainThread]: Running with dbt=1.9.6
[0m00:59:17.610168 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:59:17.937340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec6ec2b9-f149-44cb-9a87-4328915e3e9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9ECBB7D10>]}
[0m00:59:18.033578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec6ec2b9-f149-44cb-9a87-4328915e3e9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EA702240>]}
[0m00:59:18.040706 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m00:59:18.437982 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 5 files added, 2 files changed.
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_customers.sql
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_raw_data.sql
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_orders.sql
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_products.sql
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_payments.sql
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\sources.yml
[0m00:59:18.712398 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\raw_to_normalized.sql
[0m00:59:19.513807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec6ec2b9-f149-44cb-9a87-4328915e3e9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9ED36D550>]}
[0m00:59:19.619094 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m00:59:19.698235 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m00:59:19.859176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec6ec2b9-f149-44cb-9a87-4328915e3e9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EB31DB50>]}
[0m00:59:19.860250 [info ] [MainThread]: Found 6 models, 22 data tests, 4 sources, 433 macros
[0m00:59:19.863005 [info ] [MainThread]: 
[0m00:59:19.863005 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:59:19.863005 [info ] [MainThread]: 
[0m00:59:19.863005 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m00:59:19.873355 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m00:59:20.081216 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m00:59:20.081216 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m00:59:20.081216 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:59:20.144615 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m00:59:20.223314 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m00:59:20.223314 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m00:59:20.223314 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m00:59:20.223314 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m00:59:20.223314 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m00:59:20.223314 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:59:20.223314 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m00:59:20.223314 [info ] [MainThread]: 
[0m00:59:20.223314 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m00:59:20.239069 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m00:59:20.239069 [debug] [MainThread]: Command `dbt build` failed at 00:59:20.239069 after 3.01 seconds
[0m00:59:20.239069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9ED50DC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EDA2D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B9EDA2DE50>]}
[0m00:59:20.239069 [debug] [MainThread]: Flushing usage events
[0m00:59:20.661567 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:04:04.891947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CB492480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CDE7E330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CDE7C170>]}


============================== 02:04:04.897298 | 10e65cd5-9dea-4cac-a084-b09b246e9948 ==============================
[0m02:04:04.897298 [info ] [MainThread]: Running with dbt=1.9.6
[0m02:04:04.897827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:04:05.246898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10e65cd5-9dea-4cac-a084-b09b246e9948', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CE221CA0>]}
[0m02:04:05.317770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10e65cd5-9dea-4cac-a084-b09b246e9948', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CF6052B0>]}
[0m02:04:05.318830 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m02:04:05.630102 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m02:04:05.888680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:04:05.889254 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:04:05.941077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10e65cd5-9dea-4cac-a084-b09b246e9948', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CFA987A0>]}
[0m02:04:06.039497 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m02:04:06.089363 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m02:04:06.158407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10e65cd5-9dea-4cac-a084-b09b246e9948', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CFFF2450>]}
[0m02:04:06.159527 [info ] [MainThread]: Found 6 models, 22 data tests, 4 sources, 433 macros
[0m02:04:06.164066 [info ] [MainThread]: 
[0m02:04:06.165183 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:04:06.165653 [info ] [MainThread]: 
[0m02:04:06.166855 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m02:04:06.173006 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m02:04:06.342695 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m02:04:06.344311 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m02:04:06.344847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:04:06.426975 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m02:04:06.486014 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m02:04:06.486621 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m02:04:06.487586 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m02:04:06.488456 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m02:04:06.488964 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m02:04:06.489724 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:04:06.490230 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m02:04:06.490783 [info ] [MainThread]: 
[0m02:04:06.493249 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m02:04:06.495408 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m02:04:06.498850 [debug] [MainThread]: Command `dbt build` failed at 02:04:06.498103 after 1.80 seconds
[0m02:04:06.498850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284CDA33680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284C8092A20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284D0018B90>]}
[0m02:04:06.499619 [debug] [MainThread]: Flushing usage events
[0m02:04:06.888341 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:20:33.331708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948914A960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019489149A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948914BCB0>]}


============================== 04:20:33.336176 | 03ced606-1ea1-4598-8547-9672372ff804 ==============================
[0m04:20:33.336176 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:20:33.336728 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m04:20:33.497503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948A690500>]}
[0m04:20:33.549895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194866672C0>]}
[0m04:20:33.551298 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:20:33.809426 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:20:33.944983 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:20:33.945571 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:20:33.984997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948A6912E0>]}
[0m04:20:34.061588 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:20:34.093957 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:20:34.126771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B14A660>]}
[0m04:20:34.127306 [info ] [MainThread]: Found 6 models, 22 data tests, 4 sources, 433 macros
[0m04:20:34.127818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B12EA50>]}
[0m04:20:34.129534 [info ] [MainThread]: 
[0m04:20:34.130103 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:20:34.130699 [info ] [MainThread]: 
[0m04:20:34.131242 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m04:20:34.134057 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m04:20:34.212847 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m04:20:34.213437 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m04:20:34.213437 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:20:34.248685 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.035 seconds
[0m04:20:34.249788 [debug] [ThreadPool]: On list_datamart: Close
[0m04:20:34.257854 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m04:20:34.262391 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:20:34.262984 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m04:20:34.262984 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:20:34.287246 [debug] [ThreadPool]: SQL status: BEGIN in 0.024 seconds
[0m04:20:34.287926 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:20:34.288480 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m04:20:34.296672 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.008 seconds
[0m04:20:34.297744 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m04:20:34.299254 [debug] [ThreadPool]: On list_datamart_public: Close
[0m04:20:34.304039 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.304039 [debug] [MainThread]: On master: BEGIN
[0m04:20:34.304039 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:20:34.322403 [debug] [MainThread]: SQL status: BEGIN in 0.018 seconds
[0m04:20:34.322957 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.322957 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m04:20:34.328462 [debug] [MainThread]: SQL status: SELECT 0 in 0.005 seconds
[0m04:20:34.329581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948A692A80>]}
[0m04:20:34.330225 [debug] [MainThread]: On master: ROLLBACK
[0m04:20:34.330794 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.331349 [debug] [MainThread]: On master: BEGIN
[0m04:20:34.332441 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m04:20:34.332441 [debug] [MainThread]: On master: COMMIT
[0m04:20:34.332999 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.332999 [debug] [MainThread]: On master: COMMIT
[0m04:20:34.335765 [debug] [MainThread]: SQL status: COMMIT in 0.002 seconds
[0m04:20:34.336362 [debug] [MainThread]: On master: Close
[0m04:20:34.344966 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m04:20:34.345613 [info ] [Thread-1 (]: 1 of 6 START sql view model public.stg_payments ................................ [RUN]
[0m04:20:34.346734 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.stg_payments'
[0m04:20:34.347289 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m04:20:34.354807 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m04:20:34.356427 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m04:20:34.404236 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m04:20:34.405346 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:20:34.405885 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m04:20:34.406517 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:20:34.418848 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m04:20:34.419395 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:20:34.419395 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql

-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        payment_status -- This is the 'payment_status' for the transaction itself
    FROM
        "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(payment_status AS VARCHAR) AS status -- Renaming to 'status' for consistency if needed later
FROM
    source_payments
WHERE
    payment_id IS NOT NULL
  );
[0m04:20:34.424009 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m04:20:34.428547 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:20:34.429194 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m04:20:34.430895 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:20:34.440769 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m04:20:34.441322 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:20:34.441322 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m04:20:34.446837 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m04:20:34.458473 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m04:20:34.462583 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:20:34.462583 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m04:20:34.464486 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m04:20:34.466198 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m04:20:34.470179 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948A73BB30>]}
[0m04:20:34.470729 [info ] [Thread-1 (]: 1 of 6 OK created sql view model public.stg_payments ........................... [[32mCREATE VIEW[0m in 0.12s]
[0m04:20:34.471331 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m04:20:34.471858 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m04:20:34.472411 [info ] [Thread-1 (]: 2 of 6 START sql view model public.stg_products ................................ [RUN]
[0m04:20:34.472941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m04:20:34.472941 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m04:20:34.475736 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m04:20:34.476272 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m04:20:34.479064 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m04:20:34.479599 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:20:34.480216 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m04:20:34.480216 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:20:34.502046 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m04:20:34.502610 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:20:34.503194 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name AS name, -- Renaming to match schema.yml 'name'
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(name AS VARCHAR) AS name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m04:20:34.506837 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m04:20:34.509918 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:20:34.509918 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m04:20:34.511734 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:20:34.512836 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m04:20:34.512836 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:20:34.513422 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m04:20:34.515072 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m04:20:34.517201 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m04:20:34.517763 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:20:34.518328 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m04:20:34.519531 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m04:20:34.521214 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m04:20:34.521781 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B5C0620>]}
[0m04:20:34.522910 [info ] [Thread-1 (]: 2 of 6 OK created sql view model public.stg_products ........................... [[32mCREATE VIEW[0m in 0.05s]
[0m04:20:34.523491 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m04:20:34.524151 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m04:20:34.524666 [info ] [Thread-1 (]: 3 of 6 START sql view model public.stg_raw_data ................................ [RUN]
[0m04:20:34.525680 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m04:20:34.525680 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m04:20:34.528745 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m04:20:34.529849 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m04:20:34.532068 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m04:20:34.533179 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:20:34.533179 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m04:20:34.533751 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:20:34.562568 [debug] [Thread-1 (]: SQL status: BEGIN in 0.029 seconds
[0m04:20:34.563711 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:20:34.564320 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        payment_status AS order_payment_status -- Renaming to differentiate from payments.payment_status
    FROM
        "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount, -- Assuming 2 decimal places for currency
    CAST(order_payment_status AS VARCHAR) AS order_payment_status
FROM
    source_data
  );
[0m04:20:34.566600 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m04:20:34.569388 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:20:34.569388 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m04:20:34.570509 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:20:34.572145 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m04:20:34.572684 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:20:34.573328 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m04:20:34.574996 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m04:20:34.576860 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m04:20:34.577413 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:20:34.577967 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m04:20:34.578521 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m04:20:34.580258 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m04:20:34.581359 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B5C3CE0>]}
[0m04:20:34.581930 [info ] [Thread-1 (]: 3 of 6 OK created sql view model public.stg_raw_data ........................... [[32mCREATE VIEW[0m in 0.06s]
[0m04:20:34.583074 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m04:20:34.584226 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers
[0m04:20:34.584804 [info ] [Thread-1 (]: 4 of 6 START sql view model public.stg_customers ............................... [RUN]
[0m04:20:34.585410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.stg_customers)
[0m04:20:34.585974 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers
[0m04:20:34.588832 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers"
[0m04:20:34.589928 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers
[0m04:20:34.593877 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers"
[0m04:20:34.595753 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m04:20:34.596098 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: BEGIN
[0m04:20:34.596606 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:20:34.621468 [debug] [Thread-1 (]: SQL status: BEGIN in 0.025 seconds
[0m04:20:34.621468 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m04:20:34.622066 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */

  create view "datamart"."public"."stg_customers__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m04:20:34.626636 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m04:20:34.629653 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m04:20:34.629971 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
alter table "datamart"."public"."stg_customers__dbt_tmp" rename to "stg_customers"
[0m04:20:34.631022 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:20:34.632165 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m04:20:34.632706 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m04:20:34.632706 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m04:20:34.635358 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m04:20:34.636529 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers__dbt_backup"
[0m04:20:34.637692 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m04:20:34.637692 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
drop view if exists "datamart"."public"."stg_customers__dbt_backup" cascade
[0m04:20:34.638821 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m04:20:34.639376 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: Close
[0m04:20:34.640533 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B612300>]}
[0m04:20:34.641260 [info ] [Thread-1 (]: 4 of 6 OK created sql view model public.stg_customers .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m04:20:34.641924 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers
[0m04:20:34.642289 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m04:20:34.642952 [info ] [Thread-1 (]: 5 of 6 START sql view model public.stg_orders .................................. [RUN]
[0m04:20:34.643455 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers, now model.data_pipeline.stg_orders)
[0m04:20:34.643994 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m04:20:34.646281 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m04:20:34.646854 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m04:20:34.649434 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m04:20:34.651103 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m04:20:34.651103 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m04:20:34.652115 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:20:34.673128 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m04:20:34.673684 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m04:20:34.674346 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m04:20:34.678979 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m04:20:34.682377 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m04:20:34.682928 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m04:20:34.683854 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:20:34.686189 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m04:20:34.687406 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m04:20:34.687942 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m04:20:34.690800 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m04:20:34.693610 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m04:20:34.694160 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m04:20:34.694732 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m04:20:34.695457 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m04:20:34.696688 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m04:20:34.697360 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B63AFC0>]}
[0m04:20:34.698145 [info ] [Thread-1 (]: 5 of 6 OK created sql view model public.stg_orders ............................. [[32mCREATE VIEW[0m in 0.05s]
[0m04:20:34.699248 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m04:20:34.699976 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m04:20:34.700542 [info ] [Thread-1 (]: 6 of 6 START sql view model public.raw_to_normalized ........................... [RUN]
[0m04:20:34.701150 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.raw_to_normalized)
[0m04:20:34.701760 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.raw_to_normalized
[0m04:20:34.704050 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.raw_to_normalized"
[0m04:20:34.705726 [debug] [Thread-1 (]: Began executing node model.data_pipeline.raw_to_normalized
[0m04:20:34.709151 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.raw_to_normalized"
[0m04:20:34.710836 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m04:20:34.711447 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: BEGIN
[0m04:20:34.712070 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:20:34.729195 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m04:20:34.729783 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m04:20:34.730339 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */

  create view "datamart"."public"."raw_to_normalized__dbt_tmp"
    
    
  as (
    -- models/raw_to_normalized.sql

-- This model combines data from the staged customer, order, product, and payment tables
-- to create a comprehensive, normalized view of the e-commerce data.
-- It serves as the primary data source for analytics and reporting.

WITH customers AS (
    SELECT * FROM "datamart"."public"."stg_customers"
),

orders AS (
    SELECT * FROM "datamart"."public"."stg_orders"
),

products AS (
    SELECT * FROM "datamart"."public"."stg_products"
),

payments AS (
    SELECT * FROM "datamart"."public"."stg_payments"
)

SELECT
    o.order_id,
    o.order_date,
    o.total_amount,
    o.status AS order_status,
    o.payment_method,
    o.shipping_address,

    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    c.phone,
    c.address AS customer_address,
    c.created_at AS customer_created_at,

    -- Note: This assumes a simple link between orders and products based on the combined raw data.
    -- In a more complex scenario with an 'order_items' table, you would join to that
    -- to link specific products purchased in an order. For this simplified setup,
    -- we'll just pull in product details from the 'products' dimension.
    -- If your raw_data had product_id per order_line, we'd join on that for detailed line items.
    -- Since the sample CSV provided implicitly groups products with orders by having product details
    -- on the same row as order details, we'll aim for a denormalized "order-with-product-summary"
    -- if the original raw_data supports it for this final view, or keep products separate.
    -- Given the provided schema.yml has a 'products' table, we will assume a separate dimension.
    -- For a joined view like this, we'd need to link product_id from the original raw_data.
    -- Reconsidering: The `sample_data.csv` has `product_id` on each row.
    -- Let's include `product_id` from the raw data and then join to the `products` dimension.
    srd.product_id,
    p.name AS product_name,
    p.category AS product_category,
    p.price AS product_price,

    pay.payment_id,
    pay.status AS payment_transaction_status -- Renamed to avoid conflict with order_status

FROM
    orders o
JOIN
    customers c ON o.customer_id = c.customer_id
LEFT JOIN
    "datamart"."public"."stg_raw_data" srd ON o.order_id = srd.order_id -- Join back to original raw_data for product_id on order_line
LEFT JOIN
    products p ON srd.product_id = p.product_id
LEFT JOIN
    payments pay ON o.order_id = pay.order_id -- Assuming one payment per order for simplicity
  );
[0m04:20:34.733007 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column srd.product_id does not exist
LINE 72:     products p ON srd.product_id = p.product_id
                           ^
HINT:  Perhaps you meant to reference the column "p.product_id".

[0m04:20:34.733587 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: ROLLBACK
[0m04:20:34.734709 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: Close
[0m04:20:34.757103 [debug] [Thread-1 (]: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column srd.product_id does not exist
  LINE 72:     products p ON srd.product_id = p.product_id
                             ^
  HINT:  Perhaps you meant to reference the column "p.product_id".
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m04:20:34.757103 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03ced606-1ea1-4598-8547-9672372ff804', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001948B118560>]}
[0m04:20:34.758206 [error] [Thread-1 (]: 6 of 6 ERROR creating sql view model public.raw_to_normalized .................. [[31mERROR[0m in 0.06s]
[0m04:20:34.759108 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m04:20:34.759666 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.raw_to_normalized' to be skipped because of status 'error'.  Reason: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column srd.product_id does not exist
  LINE 72:     products p ON srd.product_id = p.product_id
                             ^
  HINT:  Perhaps you meant to reference the column "p.product_id".
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql.
[0m04:20:34.761027 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.761604 [debug] [MainThread]: On master: BEGIN
[0m04:20:34.761604 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:20:34.781684 [debug] [MainThread]: SQL status: BEGIN in 0.020 seconds
[0m04:20:34.782225 [debug] [MainThread]: On master: COMMIT
[0m04:20:34.782758 [debug] [MainThread]: Using postgres connection "master"
[0m04:20:34.782758 [debug] [MainThread]: On master: COMMIT
[0m04:20:34.783327 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m04:20:34.783850 [debug] [MainThread]: On master: Close
[0m04:20:34.784406 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:20:34.784997 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m04:20:34.784997 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m04:20:34.784997 [debug] [MainThread]: Connection 'model.data_pipeline.raw_to_normalized' was properly closed.
[0m04:20:34.786315 [info ] [MainThread]: 
[0m04:20:34.786821 [info ] [MainThread]: Finished running 6 view models in 0 hours 0 minutes and 0.66 seconds (0.66s).
[0m04:20:34.788777 [debug] [MainThread]: Command end result
[0m04:20:34.805782 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:20:34.808418 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:20:34.813747 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m04:20:34.813747 [info ] [MainThread]: 
[0m04:20:34.814328 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m04:20:34.814900 [info ] [MainThread]: 
[0m04:20:34.814900 [error] [MainThread]:   Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column srd.product_id does not exist
  LINE 72:     products p ON srd.product_id = p.product_id
                             ^
  HINT:  Perhaps you meant to reference the column "p.product_id".
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m04:20:34.815480 [info ] [MainThread]: 
[0m04:20:34.815480 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m04:20:34.816642 [debug] [MainThread]: Command `dbt run` failed at 04:20:34.816642 after 1.64 seconds
[0m04:20:34.817184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194892C0B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019489331520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194893B9430>]}
[0m04:20:34.817184 [debug] [MainThread]: Flushing usage events
[0m04:20:35.287481 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:30:25.860357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BFFC6BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BFAA6420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BFD2CB60>]}


============================== 04:30:25.866440 | af690ce9-5ddf-4a48-b891-ced0e996ac60 ==============================
[0m04:30:25.866440 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:30:25.867590 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m04:30:26.121509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BD21F290>]}
[0m04:30:26.207378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C1607BF0>]}
[0m04:30:26.208757 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:30:26.552812 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:30:26.730731 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:30:26.731886 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_raw_data.sql
[0m04:30:27.082337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C1D91430>]}
[0m04:30:27.183281 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:30:27.236160 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:30:27.276698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C1D80D40>]}
[0m04:30:27.277268 [info ] [MainThread]: Found 6 models, 22 data tests, 4 sources, 433 macros
[0m04:30:27.278406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C1CC9DC0>]}
[0m04:30:27.281390 [info ] [MainThread]: 
[0m04:30:27.281981 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:30:27.282540 [info ] [MainThread]: 
[0m04:30:27.283713 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m04:30:27.289927 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m04:30:27.405064 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m04:30:27.406252 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m04:30:27.406914 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:30:27.437772 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.031 seconds
[0m04:30:27.439919 [debug] [ThreadPool]: On list_datamart: Close
[0m04:30:27.443837 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m04:30:27.452217 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:30:27.452771 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m04:30:27.453319 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:30:27.476358 [debug] [ThreadPool]: SQL status: BEGIN in 0.022 seconds
[0m04:30:27.477007 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:30:27.477007 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m04:30:27.482307 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.005 seconds
[0m04:30:27.484453 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m04:30:27.485566 [debug] [ThreadPool]: On list_datamart_public: Close
[0m04:30:27.493504 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.494211 [debug] [MainThread]: On master: BEGIN
[0m04:30:27.494211 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:30:27.528551 [debug] [MainThread]: SQL status: BEGIN in 0.034 seconds
[0m04:30:27.529746 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.529746 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m04:30:27.536880 [debug] [MainThread]: SQL status: SELECT 7 in 0.006 seconds
[0m04:30:27.538756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C16F90A0>]}
[0m04:30:27.539427 [debug] [MainThread]: On master: ROLLBACK
[0m04:30:27.540825 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.541429 [debug] [MainThread]: On master: BEGIN
[0m04:30:27.544002 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m04:30:27.544630 [debug] [MainThread]: On master: COMMIT
[0m04:30:27.544630 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.545326 [debug] [MainThread]: On master: COMMIT
[0m04:30:27.546632 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m04:30:27.547744 [debug] [MainThread]: On master: Close
[0m04:30:27.555411 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m04:30:27.555975 [info ] [Thread-1 (]: 1 of 6 START sql view model public.stg_payments ................................ [RUN]
[0m04:30:27.557125 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.stg_payments'
[0m04:30:27.557554 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m04:30:27.566256 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m04:30:27.567613 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m04:30:27.613241 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m04:30:27.615373 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.616015 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m04:30:27.616015 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:30:27.638226 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m04:30:27.639434 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.640108 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql

-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        payment_status -- This is the 'payment_status' for the transaction itself
    FROM
        "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(payment_status AS VARCHAR) AS status -- Renaming to 'status' for consistency if needed later
FROM
    source_payments
WHERE
    payment_id IS NOT NULL
  );
[0m04:30:27.643816 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m04:30:27.652946 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.653324 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m04:30:27.655125 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:30:27.659141 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.659699 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m04:30:27.661498 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:30:27.679432 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m04:30:27.680003 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.680569 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m04:30:27.691704 [debug] [Thread-1 (]: SQL status: COMMIT in 0.011 seconds
[0m04:30:27.700744 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m04:30:27.707138 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m04:30:27.707863 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m04:30:27.716406 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.008 seconds
[0m04:30:27.719871 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m04:30:27.723740 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BD21E2D0>]}
[0m04:30:27.724499 [info ] [Thread-1 (]: 1 of 6 OK created sql view model public.stg_payments ........................... [[32mCREATE VIEW[0m in 0.16s]
[0m04:30:27.725861 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m04:30:27.727097 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m04:30:27.727730 [info ] [Thread-1 (]: 2 of 6 START sql view model public.stg_products ................................ [RUN]
[0m04:30:27.729582 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m04:30:27.729910 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m04:30:27.734151 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m04:30:27.736438 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m04:30:27.740768 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m04:30:27.741875 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.742532 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m04:30:27.743172 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:30:27.789246 [debug] [Thread-1 (]: SQL status: BEGIN in 0.046 seconds
[0m04:30:27.789828 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.790397 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name AS name, -- Renaming to match schema.yml 'name'
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(name AS VARCHAR) AS name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m04:30:27.794124 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m04:30:27.799573 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.800114 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products" rename to "stg_products__dbt_backup"
[0m04:30:27.802020 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:30:27.805845 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.806585 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m04:30:27.808036 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m04:30:27.810001 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m04:30:27.810675 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.811256 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m04:30:27.814375 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m04:30:27.816922 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m04:30:27.818356 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m04:30:27.818356 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m04:30:27.821739 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m04:30:27.823881 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m04:30:27.824603 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C21E9B50>]}
[0m04:30:27.824603 [info ] [Thread-1 (]: 2 of 6 OK created sql view model public.stg_products ........................... [[32mCREATE VIEW[0m in 0.09s]
[0m04:30:27.827088 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m04:30:27.827647 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m04:30:27.828272 [info ] [Thread-1 (]: 3 of 6 START sql view model public.stg_raw_data ................................ [RUN]
[0m04:30:27.829383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m04:30:27.829981 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m04:30:27.833991 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m04:30:27.835362 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m04:30:27.838736 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m04:30:27.840145 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:30:27.840744 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m04:30:27.840744 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:30:27.864668 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m04:30:27.865176 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m04:30:27.865890 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.
WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        payment_status AS order_payment_status,
        product_id  -- Added to project product_id from the raw source
    FROM
        "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,  -- Assuming 2 decimal places for currency
    CAST(order_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id  -- Ensure product_id is included and cast appropriately
FROM
    source_data
  );
[0m04:30:27.868351 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "product_id" does not exist
LINE 19:         product_id  -- Added to project product_id from the ...
                 ^

[0m04:30:27.868351 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: ROLLBACK
[0m04:30:27.870290 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m04:30:27.881623 [debug] [Thread-1 (]: Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "product_id" does not exist
  LINE 19:         product_id  -- Added to project product_id from the ...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql
[0m04:30:27.883279 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af690ce9-5ddf-4a48-b891-ced0e996ac60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C1CCB290>]}
[0m04:30:27.885804 [error] [Thread-1 (]: 3 of 6 ERROR creating sql view model public.stg_raw_data ....................... [[31mERROR[0m in 0.05s]
[0m04:30:27.888623 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m04:30:27.890109 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.stg_raw_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "product_id" does not exist
  LINE 19:         product_id  -- Added to project product_id from the ...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql.
[0m04:30:27.894980 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers
[0m04:30:27.896174 [info ] [Thread-1 (]: 4 of 6 SKIP relation public.stg_customers ...................................... [[33mSKIP[0m]
[0m04:30:27.898079 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers
[0m04:30:27.899278 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m04:30:27.900497 [info ] [Thread-1 (]: 5 of 6 SKIP relation public.stg_orders ......................................... [[33mSKIP[0m]
[0m04:30:27.901732 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m04:30:27.902412 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m04:30:27.903452 [info ] [Thread-1 (]: 6 of 6 SKIP relation public.raw_to_normalized .................................. [[33mSKIP[0m]
[0m04:30:27.905699 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m04:30:27.909464 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.910108 [debug] [MainThread]: On master: BEGIN
[0m04:30:27.911321 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:30:27.942847 [debug] [MainThread]: SQL status: BEGIN in 0.031 seconds
[0m04:30:27.943476 [debug] [MainThread]: On master: COMMIT
[0m04:30:27.944210 [debug] [MainThread]: Using postgres connection "master"
[0m04:30:27.944210 [debug] [MainThread]: On master: COMMIT
[0m04:30:27.945925 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m04:30:27.945925 [debug] [MainThread]: On master: Close
[0m04:30:27.947171 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:30:27.947841 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m04:30:27.948852 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m04:30:27.949481 [debug] [MainThread]: Connection 'model.data_pipeline.stg_raw_data' was properly closed.
[0m04:30:27.951046 [info ] [MainThread]: 
[0m04:30:27.952462 [info ] [MainThread]: Finished running 6 view models in 0 hours 0 minutes and 0.67 seconds (0.67s).
[0m04:30:27.954608 [debug] [MainThread]: Command end result
[0m04:30:27.988674 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:30:27.992939 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:30:28.011085 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m04:30:28.012339 [info ] [MainThread]: 
[0m04:30:28.013429 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m04:30:28.015418 [info ] [MainThread]: 
[0m04:30:28.017008 [error] [MainThread]:   Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "product_id" does not exist
  LINE 19:         product_id  -- Added to project product_id from the ...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql
[0m04:30:28.018120 [info ] [MainThread]: 
[0m04:30:28.019866 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=3 TOTAL=6
[0m04:30:28.022316 [debug] [MainThread]: Command `dbt run` failed at 04:30:28.021518 after 2.54 seconds
[0m04:30:28.023199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BDA7A5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275BDA7A930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000275C209FB60>]}
[0m04:30:28.023731 [debug] [MainThread]: Flushing usage events
[0m04:30:28.440716 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:39:22.167102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D8223B2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D824AB1220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D822D8BFE0>]}


============================== 04:39:22.173636 | f42c2cc4-c82f-4882-8d56-7f124f358809 ==============================
[0m04:39:22.173636 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:39:22.174853 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:39:22.211283 [info ] [MainThread]: dbt version: 1.9.6
[0m04:39:22.212508 [info ] [MainThread]: python version: 3.12.6
[0m04:39:22.213705 [info ] [MainThread]: python path: C:\Project\DataPipeline\venv\Scripts\python.exe
[0m04:39:22.214373 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m04:39:22.293144 [info ] [MainThread]: Using profiles dir at C:\Users\Gaurav Chugh\.dbt
[0m04:39:22.294205 [info ] [MainThread]: Using profiles.yml file at C:\Users\Gaurav Chugh\.dbt\profiles.yml
[0m04:39:22.294744 [info ] [MainThread]: Using dbt_project.yml file at C:\Project\DataPipeline\dags\dbt_project\dbt_project.yml
[0m04:39:22.295808 [info ] [MainThread]: adapter type: postgres
[0m04:39:22.296398 [info ] [MainThread]: adapter version: 1.9.0
[0m04:39:22.464714 [info ] [MainThread]: Configuration:
[0m04:39:22.465895 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m04:39:22.466407 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m04:39:22.467011 [info ] [MainThread]: Required dependencies:
[0m04:39:22.467600 [debug] [MainThread]: Executing "git --help"
[0m04:39:22.520592 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m04:39:22.521179 [debug] [MainThread]: STDERR: "b''"
[0m04:39:22.521736 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m04:39:22.522362 [info ] [MainThread]: Connection:
[0m04:39:22.522963 [info ] [MainThread]:   host: localhost
[0m04:39:22.523480 [info ] [MainThread]:   port: 5432
[0m04:39:22.524234 [info ] [MainThread]:   user: dwh_user
[0m04:39:22.525078 [info ] [MainThread]:   database: datamart
[0m04:39:22.525773 [info ] [MainThread]:   schema: public
[0m04:39:22.526930 [info ] [MainThread]:   connect_timeout: 10
[0m04:39:22.528119 [info ] [MainThread]:   role: None
[0m04:39:22.529377 [info ] [MainThread]:   search_path: None
[0m04:39:22.530005 [info ] [MainThread]:   keepalives_idle: 0
[0m04:39:22.531086 [info ] [MainThread]:   sslmode: None
[0m04:39:22.531633 [info ] [MainThread]:   sslcert: None
[0m04:39:22.532173 [info ] [MainThread]:   sslkey: None
[0m04:39:22.532173 [info ] [MainThread]:   sslrootcert: None
[0m04:39:22.533508 [info ] [MainThread]:   application_name: dbt
[0m04:39:22.534028 [info ] [MainThread]:   retries: 1
[0m04:39:22.535223 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:39:22.868293 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m04:39:22.986227 [debug] [MainThread]: Using postgres connection "debug"
[0m04:39:22.986794 [debug] [MainThread]: On debug: select 1 as id
[0m04:39:22.987350 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:39:23.016174 [debug] [MainThread]: SQL status: SELECT 1 in 0.029 seconds
[0m04:39:23.018065 [debug] [MainThread]: On debug: Close
[0m04:39:23.019151 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m04:39:23.020311 [info ] [MainThread]: [32mAll checks passed![0m
[0m04:39:23.023833 [debug] [MainThread]: Command `dbt debug` succeeded at 04:39:23.023325 after 1.16 seconds
[0m04:39:23.023833 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m04:39:23.023833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D826B90260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D822297950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D8248A2F90>]}
[0m04:39:23.023833 [debug] [MainThread]: Flushing usage events
[0m04:39:23.432269 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:39:35.266369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272942DB920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272942DA960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272942DA990>]}


============================== 04:39:35.272585 | fec220f7-f9af-4849-81b5-033d1574887c ==============================
[0m04:39:35.272585 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:39:35.273774 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m04:39:35.523244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027294781070>]}
[0m04:39:35.597449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002729370FB90>]}
[0m04:39:35.598649 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m04:39:35.874748 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m04:39:36.056450 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:39:36.057933 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\sources.yml
[0m04:39:36.737169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002729625D3A0>]}
[0m04:39:36.829065 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:39:36.875151 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:39:36.933057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002729662BA40>]}
[0m04:39:36.933815 [info ] [MainThread]: Found 6 models, 23 data tests, 4 sources, 433 macros
[0m04:39:36.935395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002729659B9E0>]}
[0m04:39:36.939894 [info ] [MainThread]: 
[0m04:39:36.941065 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:39:36.942727 [info ] [MainThread]: 
[0m04:39:36.944651 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m04:39:36.950583 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m04:39:37.127836 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:39:37.128583 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m04:39:37.129249 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:39:37.176377 [debug] [ThreadPool]: SQL status: BEGIN in 0.047 seconds
[0m04:39:37.177140 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m04:39:37.177448 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m04:39:37.183227 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.006 seconds
[0m04:39:37.185373 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m04:39:37.186808 [debug] [ThreadPool]: On list_datamart_public: Close
[0m04:39:37.194343 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:37.194945 [debug] [MainThread]: On master: BEGIN
[0m04:39:37.195534 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:39:37.213038 [debug] [MainThread]: SQL status: BEGIN in 0.018 seconds
[0m04:39:37.213793 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:37.214513 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m04:39:37.220591 [debug] [MainThread]: SQL status: SELECT 5 in 0.006 seconds
[0m04:39:37.223135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fec220f7-f9af-4849-81b5-033d1574887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272966C83E0>]}
[0m04:39:37.223768 [debug] [MainThread]: On master: ROLLBACK
[0m04:39:37.225040 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:37.225040 [debug] [MainThread]: On master: BEGIN
[0m04:39:37.226944 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m04:39:37.226944 [debug] [MainThread]: On master: COMMIT
[0m04:39:37.227616 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:37.228418 [debug] [MainThread]: On master: COMMIT
[0m04:39:37.228978 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m04:39:37.229654 [debug] [MainThread]: On master: Close
[0m04:39:37.237437 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m04:39:37.238136 [info ] [Thread-1 (]: 1 of 23 START test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [RUN]
[0m04:39:37.239458 [debug] [Thread-1 (]: Acquiring new postgres connection 'test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6'
[0m04:39:37.239458 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m04:39:37.250107 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m04:39:37.251839 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m04:39:37.274895 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m04:39:37.276866 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m04:39:37.277431 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: BEGIN
[0m04:39:37.277984 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m04:39:37.312348 [debug] [Thread-1 (]: SQL status: BEGIN in 0.034 seconds
[0m04:39:37.312934 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m04:39:37.314299 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_method

)

select *
from all_values
where value_field not in (
    'Credit Card','PayPal','Bank Transfer'
)



  
  
      
    ) dbt_internal_test
[0m04:39:37.317870 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m04:39:37.323830 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: ROLLBACK
[0m04:39:37.325891 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: Close
[0m04:39:37.327071 [info ] [Thread-1 (]: 1 of 23 PASS source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [[32mPASS[0m in 0.09s]
[0m04:39:37.328420 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m04:39:37.329092 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m04:39:37.329873 [info ] [Thread-1 (]: 2 of 23 START test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending  [RUN]
[0m04:39:37.330610 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6, now test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc)
[0m04:39:37.331391 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m04:39:37.337213 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m04:39:37.339223 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m04:39:37.342550 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m04:39:37.343268 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m04:39:37.343938 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: BEGIN
[0m04:39:37.344614 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.374641 [debug] [Thread-1 (]: SQL status: BEGIN in 0.030 seconds
[0m04:39:37.375838 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"
[0m04:39:37.376735 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_status

)

select *
from all_values
where value_field not in (
    'Completed','Failed','Pending'
)



  
  
      
    ) dbt_internal_test
[0m04:39:37.380882 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.004 seconds
[0m04:39:37.383469 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: ROLLBACK
[0m04:39:37.385232 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc: Close
[0m04:39:37.387020 [error] [Thread-1 (]: 2 of 23 FAIL 1 source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending  [[31mFAIL 1[0m in 0.06s]
[0m04:39:37.388401 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc
[0m04:39:37.388977 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m04:39:37.389587 [info ] [Thread-1 (]: 3 of 23 START test source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded  [RUN]
[0m04:39:37.391327 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending.251d1a1dcc, now test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21)
[0m04:39:37.391327 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m04:39:37.397265 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m04:39:37.399226 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m04:39:37.402472 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m04:39:37.403756 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m04:39:37.404380 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: BEGIN
[0m04:39:37.404380 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.427063 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m04:39:37.428225 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"
[0m04:39:37.428225 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."raw_data"
    group by payment_status

)

select *
from all_values
where value_field not in (
    'Paid','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m04:39:37.431178 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.432841 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: ROLLBACK
[0m04:39:37.434240 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21: Close
[0m04:39:37.435421 [info ] [Thread-1 (]: 3 of 23 PASS source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded  [[32mPASS[0m in 0.04s]
[0m04:39:37.436664 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21
[0m04:39:37.437361 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m04:39:37.437980 [info ] [Thread-1 (]: 4 of 23 START test source_not_null_raw_customers_source_customer_id ............ [RUN]
[0m04:39:37.438651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_raw_data_payment_status__Paid__Pending__Refunded.596345ab21, now test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556)
[0m04:39:37.439262 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m04:39:37.449039 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m04:39:37.450918 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m04:39:37.453815 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m04:39:37.454964 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m04:39:37.454964 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: BEGIN
[0m04:39:37.455547 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.476758 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m04:39:37.476758 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m04:39:37.477472 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."customers_source"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.479861 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.481747 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: ROLLBACK
[0m04:39:37.483567 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: Close
[0m04:39:37.485417 [info ] [Thread-1 (]: 4 of 23 PASS source_not_null_raw_customers_source_customer_id .................. [[32mPASS[0m in 0.05s]
[0m04:39:37.486559 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m04:39:37.486559 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m04:39:37.487919 [info ] [Thread-1 (]: 5 of 23 START test source_not_null_raw_customers_source_signup_date ............ [RUN]
[0m04:39:37.488515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556, now test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f)
[0m04:39:37.489110 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m04:39:37.494375 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m04:39:37.495555 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m04:39:37.497963 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m04:39:37.500104 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m04:39:37.500700 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: BEGIN
[0m04:39:37.500700 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.520672 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:37.521953 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m04:39:37.521953 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from "datamart"."raw"."customers_source"
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.524375 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.526572 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: ROLLBACK
[0m04:39:37.528342 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: Close
[0m04:39:37.529529 [info ] [Thread-1 (]: 5 of 23 PASS source_not_null_raw_customers_source_signup_date .................. [[32mPASS[0m in 0.04s]
[0m04:39:37.531174 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m04:39:37.531691 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m04:39:37.532135 [info ] [Thread-1 (]: 6 of 23 START test source_not_null_raw_payments_order_id ....................... [RUN]
[0m04:39:37.533141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f, now test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9)
[0m04:39:37.533848 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m04:39:37.538824 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m04:39:37.540453 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m04:39:37.543115 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m04:39:37.544297 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m04:39:37.544902 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: BEGIN
[0m04:39:37.545601 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.580582 [debug] [Thread-1 (]: SQL status: BEGIN in 0.035 seconds
[0m04:39:37.581171 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m04:39:37.581774 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."payments"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.584319 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.586126 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: ROLLBACK
[0m04:39:37.587285 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: Close
[0m04:39:37.588461 [info ] [Thread-1 (]: 6 of 23 PASS source_not_null_raw_payments_order_id ............................. [[32mPASS[0m in 0.05s]
[0m04:39:37.589820 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m04:39:37.590935 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m04:39:37.591599 [info ] [Thread-1 (]: 7 of 23 START test source_not_null_raw_payments_payment_id ..................... [RUN]
[0m04:39:37.592216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9, now test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5)
[0m04:39:37.592973 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m04:39:37.598670 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m04:39:37.601056 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m04:39:37.604175 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m04:39:37.605338 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m04:39:37.605942 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: BEGIN
[0m04:39:37.606544 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.626613 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:37.627936 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m04:39:37.627936 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select payment_id
from "datamart"."raw"."payments"
where payment_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.630369 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m04:39:37.632113 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: ROLLBACK
[0m04:39:37.633933 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: Close
[0m04:39:37.635171 [info ] [Thread-1 (]: 7 of 23 PASS source_not_null_raw_payments_payment_id ........................... [[32mPASS[0m in 0.04s]
[0m04:39:37.635748 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m04:39:37.636372 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m04:39:37.636970 [info ] [Thread-1 (]: 8 of 23 START test source_not_null_raw_products_category ....................... [RUN]
[0m04:39:37.638300 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5, now test.data_pipeline.source_not_null_raw_products_category.9265557239)
[0m04:39:37.638901 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m04:39:37.644085 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m04:39:37.645872 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m04:39:37.648269 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m04:39:37.649785 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m04:39:37.650359 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: BEGIN
[0m04:39:37.650950 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.671330 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:37.671931 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m04:39:37.672529 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_category.9265557239"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select category
from "datamart"."raw"."products"
where category is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.674460 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.677365 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: ROLLBACK
[0m04:39:37.679421 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: Close
[0m04:39:37.680829 [info ] [Thread-1 (]: 8 of 23 PASS source_not_null_raw_products_category ............................. [[32mPASS[0m in 0.04s]
[0m04:39:37.682677 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m04:39:37.683008 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m04:39:37.684161 [info ] [Thread-1 (]: 9 of 23 START test source_not_null_raw_products_price .......................... [RUN]
[0m04:39:37.685268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_category.9265557239, now test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b)
[0m04:39:37.685863 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m04:39:37.690451 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m04:39:37.691679 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m04:39:37.694574 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m04:39:37.695176 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m04:39:37.695771 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: BEGIN
[0m04:39:37.696406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.737214 [debug] [Thread-1 (]: SQL status: BEGIN in 0.041 seconds
[0m04:39:37.737826 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m04:39:37.738441 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select price
from "datamart"."raw"."products"
where price is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.740208 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.742191 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: ROLLBACK
[0m04:39:37.743390 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: Close
[0m04:39:37.744575 [info ] [Thread-1 (]: 9 of 23 PASS source_not_null_raw_products_price ................................ [[32mPASS[0m in 0.06s]
[0m04:39:37.746516 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m04:39:37.746516 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m04:39:37.747707 [info ] [Thread-1 (]: 10 of 23 START test source_not_null_raw_products_product_id .................... [RUN]
[0m04:39:37.748345 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b, now test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae)
[0m04:39:37.749004 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m04:39:37.754970 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m04:39:37.756294 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m04:39:37.759940 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m04:39:37.761282 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m04:39:37.761866 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: BEGIN
[0m04:39:37.762438 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.782204 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:37.783331 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m04:39:37.783919 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."products"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.785730 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m04:39:37.787486 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: ROLLBACK
[0m04:39:37.789350 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: Close
[0m04:39:37.790630 [info ] [Thread-1 (]: 10 of 23 PASS source_not_null_raw_products_product_id .......................... [[32mPASS[0m in 0.04s]
[0m04:39:37.791958 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m04:39:37.792569 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m04:39:37.793299 [info ] [Thread-1 (]: 11 of 23 START test source_not_null_raw_products_product_name .................. [RUN]
[0m04:39:37.793900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae, now test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51)
[0m04:39:37.794698 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m04:39:37.799447 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m04:39:37.801257 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m04:39:37.804175 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m04:39:37.805686 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m04:39:37.806201 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: BEGIN
[0m04:39:37.806577 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.836920 [debug] [Thread-1 (]: SQL status: BEGIN in 0.030 seconds
[0m04:39:37.837613 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m04:39:37.838813 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_name
from "datamart"."raw"."products"
where product_name is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.840828 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.842727 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: ROLLBACK
[0m04:39:37.843906 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: Close
[0m04:39:37.845086 [info ] [Thread-1 (]: 11 of 23 PASS source_not_null_raw_products_product_name ........................ [[32mPASS[0m in 0.05s]
[0m04:39:37.845993 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m04:39:37.846683 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m04:39:37.847287 [info ] [Thread-1 (]: 12 of 23 START test source_not_null_raw_raw_data_customer_id ................... [RUN]
[0m04:39:37.848048 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51, now test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3)
[0m04:39:37.848661 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m04:39:37.853921 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m04:39:37.855108 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m04:39:37.857362 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m04:39:37.859192 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m04:39:37.859504 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: BEGIN
[0m04:39:37.859997 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.879154 [debug] [Thread-1 (]: SQL status: BEGIN in 0.019 seconds
[0m04:39:37.879758 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m04:39:37.880365 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."raw_data"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.882116 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m04:39:37.884559 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: ROLLBACK
[0m04:39:37.885788 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: Close
[0m04:39:37.886973 [info ] [Thread-1 (]: 12 of 23 PASS source_not_null_raw_raw_data_customer_id ......................... [[32mPASS[0m in 0.04s]
[0m04:39:37.887582 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m04:39:37.888227 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m04:39:37.889378 [info ] [Thread-1 (]: 13 of 23 START test source_not_null_raw_raw_data_order_date .................... [RUN]
[0m04:39:37.890024 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3, now test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc)
[0m04:39:37.890434 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m04:39:37.895477 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m04:39:37.896623 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m04:39:37.899435 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m04:39:37.900656 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m04:39:37.901259 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: BEGIN
[0m04:39:37.901851 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.921421 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:37.921993 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m04:39:37.922583 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_date
from "datamart"."raw"."raw_data"
where order_date is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.926154 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.927942 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: ROLLBACK
[0m04:39:37.929145 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: Close
[0m04:39:37.931016 [info ] [Thread-1 (]: 13 of 23 PASS source_not_null_raw_raw_data_order_date .......................... [[32mPASS[0m in 0.04s]
[0m04:39:37.932089 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m04:39:37.932835 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m04:39:37.933522 [info ] [Thread-1 (]: 14 of 23 START test source_not_null_raw_raw_data_order_id ...................... [RUN]
[0m04:39:37.934115 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc, now test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6)
[0m04:39:37.934800 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m04:39:37.939514 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m04:39:37.940606 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m04:39:37.943095 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m04:39:37.944963 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m04:39:37.945552 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: BEGIN
[0m04:39:37.946124 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:37.968245 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m04:39:37.968836 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m04:39:37.969502 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."raw_data"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:37.971618 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:37.973329 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: ROLLBACK
[0m04:39:37.974378 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: Close
[0m04:39:37.975877 [info ] [Thread-1 (]: 14 of 23 PASS source_not_null_raw_raw_data_order_id ............................ [[32mPASS[0m in 0.04s]
[0m04:39:37.977345 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m04:39:37.977956 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m04:39:37.978553 [info ] [Thread-1 (]: 15 of 23 START test source_not_null_raw_raw_data_product_id .................... [RUN]
[0m04:39:37.979144 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6, now test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e)
[0m04:39:37.979747 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m04:39:37.984789 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m04:39:37.985959 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m04:39:37.988317 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m04:39:37.990055 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m04:39:37.990463 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: BEGIN
[0m04:39:37.990969 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.009805 [debug] [Thread-1 (]: SQL status: BEGIN in 0.019 seconds
[0m04:39:38.010420 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m04:39:38.011070 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."raw_data"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m04:39:38.012848 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "product_id" does not exist
LINE 16: select product_id
                ^

[0m04:39:38.013423 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: ROLLBACK
[0m04:39:38.014690 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: Close
[0m04:39:38.024654 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql
[0m04:39:38.025792 [error] [Thread-1 (]: 15 of 23 ERROR source_not_null_raw_raw_data_product_id ......................... [[31mERROR[0m in 0.05s]
[0m04:39:38.026385 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m04:39:38.026975 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m04:39:38.027588 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql.
[0m04:39:38.028180 [info ] [Thread-1 (]: 16 of 23 START test source_not_null_raw_raw_data_total_amount .................. [RUN]
[0m04:39:38.029999 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e, now test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e)
[0m04:39:38.030597 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m04:39:38.037177 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m04:39:38.038937 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m04:39:38.041289 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m04:39:38.042451 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m04:39:38.043114 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: BEGIN
[0m04:39:38.043114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.067774 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m04:39:38.068389 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m04:39:38.068970 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_amount
from "datamart"."raw"."raw_data"
where total_amount is null



  
  
      
    ) dbt_internal_test
[0m04:39:38.071327 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:38.073754 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: ROLLBACK
[0m04:39:38.075195 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: Close
[0m04:39:38.076478 [info ] [Thread-1 (]: 16 of 23 PASS source_not_null_raw_raw_data_total_amount ........................ [[32mPASS[0m in 0.05s]
[0m04:39:38.078474 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m04:39:38.079058 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m04:39:38.079623 [info ] [Thread-1 (]: 17 of 23 START test source_positive_values_raw_products_price .................. [RUN]
[0m04:39:38.080181 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e, now test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9)
[0m04:39:38.080758 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m04:39:38.102836 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m04:39:38.103411 [error] [Thread-1 (]: 17 of 23 ERROR source_positive_values_raw_products_price ....................... [[31mERROR[0m in 0.02s]
[0m04:39:38.104553 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m04:39:38.104553 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m04:39:38.105303 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m04:39:38.105894 [info ] [Thread-1 (]: 18 of 23 START test source_positive_values_raw_raw_data_total_amount ........... [RUN]
[0m04:39:38.107101 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9, now test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3)
[0m04:39:38.107742 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m04:39:38.113765 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m04:39:38.114378 [error] [Thread-1 (]: 18 of 23 ERROR source_positive_values_raw_raw_data_total_amount ................ [[31mERROR[0m in 0.01s]
[0m04:39:38.115407 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m04:39:38.115989 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m04:39:38.116662 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m04:39:38.117098 [info ] [Thread-1 (]: 19 of 23 START test source_unique_raw_customers_source_customer_id ............. [RUN]
[0m04:39:38.118371 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3, now test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c)
[0m04:39:38.118957 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m04:39:38.125777 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m04:39:38.127845 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m04:39:38.130903 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m04:39:38.132072 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m04:39:38.132657 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: BEGIN
[0m04:39:38.133512 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.154467 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m04:39:38.155114 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m04:39:38.155729 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m04:39:38.158244 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:38.160030 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: ROLLBACK
[0m04:39:38.161938 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: Close
[0m04:39:38.163137 [info ] [Thread-1 (]: 19 of 23 PASS source_unique_raw_customers_source_customer_id ................... [[32mPASS[0m in 0.04s]
[0m04:39:38.164261 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m04:39:38.164867 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m04:39:38.165769 [info ] [Thread-1 (]: 20 of 23 START test source_unique_raw_customers_source_email ................... [RUN]
[0m04:39:38.166362 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c, now test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b)
[0m04:39:38.167018 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m04:39:38.173871 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m04:39:38.177185 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m04:39:38.180706 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m04:39:38.181840 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m04:39:38.182552 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: BEGIN
[0m04:39:38.183547 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.205772 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m04:39:38.206381 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m04:39:38.207006 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    email as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where email is not null
group by email
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m04:39:38.210137 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m04:39:38.212554 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: ROLLBACK
[0m04:39:38.213748 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: Close
[0m04:39:38.214919 [info ] [Thread-1 (]: 20 of 23 PASS source_unique_raw_customers_source_email ......................... [[32mPASS[0m in 0.05s]
[0m04:39:38.215981 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m04:39:38.216573 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m04:39:38.217381 [info ] [Thread-1 (]: 21 of 23 START test source_unique_raw_payments_payment_id ...................... [RUN]
[0m04:39:38.217891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b, now test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533)
[0m04:39:38.217891 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m04:39:38.223182 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m04:39:38.224342 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m04:39:38.227599 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m04:39:38.229382 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m04:39:38.229382 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: BEGIN
[0m04:39:38.230123 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.269495 [debug] [Thread-1 (]: SQL status: BEGIN in 0.039 seconds
[0m04:39:38.270102 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m04:39:38.270102 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."payments"
where payment_id is not null
group by payment_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m04:39:38.273053 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:38.275598 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: ROLLBACK
[0m04:39:38.276799 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: Close
[0m04:39:38.277996 [info ] [Thread-1 (]: 21 of 23 PASS source_unique_raw_payments_payment_id ............................ [[32mPASS[0m in 0.06s]
[0m04:39:38.279201 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m04:39:38.279803 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m04:39:38.280391 [info ] [Thread-1 (]: 22 of 23 START test source_unique_raw_products_product_id ...................... [RUN]
[0m04:39:38.281583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533, now test.data_pipeline.source_unique_raw_products_product_id.518dac90ba)
[0m04:39:38.282222 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m04:39:38.287112 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m04:39:38.288285 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m04:39:38.292352 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m04:39:38.294116 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m04:39:38.294804 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: BEGIN
[0m04:39:38.295428 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.315704 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:38.316229 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m04:39:38.316714 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."products"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m04:39:38.319321 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m04:39:38.321131 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: ROLLBACK
[0m04:39:38.322333 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: Close
[0m04:39:38.323686 [info ] [Thread-1 (]: 22 of 23 PASS source_unique_raw_products_product_id ............................ [[32mPASS[0m in 0.04s]
[0m04:39:38.325655 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m04:39:38.326162 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m04:39:38.326162 [info ] [Thread-1 (]: 23 of 23 START test source_unique_raw_raw_data_order_id ........................ [RUN]
[0m04:39:38.327405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_products_product_id.518dac90ba, now test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd)
[0m04:39:38.327405 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m04:39:38.332685 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m04:39:38.333922 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m04:39:38.336721 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m04:39:38.337831 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m04:39:38.338388 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: BEGIN
[0m04:39:38.338388 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:39:38.358518 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m04:39:38.359077 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"
[0m04:39:38.359647 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    order_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."raw_data"
where order_id is not null
group by order_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m04:39:38.362947 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m04:39:38.364566 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: ROLLBACK
[0m04:39:38.366229 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd: Close
[0m04:39:38.367497 [info ] [Thread-1 (]: 23 of 23 PASS source_unique_raw_raw_data_order_id .............................. [[32mPASS[0m in 0.04s]
[0m04:39:38.369479 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd
[0m04:39:38.371789 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:38.372428 [debug] [MainThread]: On master: BEGIN
[0m04:39:38.373013 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m04:39:38.394475 [debug] [MainThread]: SQL status: BEGIN in 0.021 seconds
[0m04:39:38.395082 [debug] [MainThread]: On master: COMMIT
[0m04:39:38.395636 [debug] [MainThread]: Using postgres connection "master"
[0m04:39:38.396182 [debug] [MainThread]: On master: COMMIT
[0m04:39:38.397297 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m04:39:38.397865 [debug] [MainThread]: On master: Close
[0m04:39:38.398992 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:39:38.400611 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m04:39:38.401280 [debug] [MainThread]: Connection 'test.data_pipeline.source_unique_raw_raw_data_order_id.667867dacd' was properly closed.
[0m04:39:38.402451 [info ] [MainThread]: 
[0m04:39:38.403028 [info ] [MainThread]: Finished running 23 data tests in 0 hours 0 minutes and 1.46 seconds (1.46s).
[0m04:39:38.409123 [debug] [MainThread]: Command end result
[0m04:39:38.446824 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m04:39:38.451047 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m04:39:38.464967 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m04:39:38.465565 [info ] [MainThread]: 
[0m04:39:38.465565 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m04:39:38.467596 [info ] [MainThread]: 
[0m04:39:38.468404 [error] [MainThread]: [31mFailure in test source_accepted_values_raw_payments_payment_status__Completed__Failed__Pending (../../models\sources.yml)[0m
[0m04:39:38.469036 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m04:39:38.469662 [info ] [MainThread]: 
[0m04:39:38.470879 [info ] [MainThread]:   compiled code at target\compiled\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_9e076b3898151400e6c50cdfdb2e3b16.sql
[0m04:39:38.471484 [info ] [MainThread]: 
[0m04:39:38.473076 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql
[0m04:39:38.473695 [info ] [MainThread]: 
[0m04:39:38.475396 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m04:39:38.476408 [info ] [MainThread]: 
[0m04:39:38.477674 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m04:39:38.479444 [info ] [MainThread]: 
[0m04:39:38.481485 [info ] [MainThread]: Done. PASS=19 WARN=0 ERROR=4 SKIP=0 TOTAL=23
[0m04:39:38.483660 [debug] [MainThread]: Command `dbt test` failed at 04:39:38.482995 after 3.45 seconds
[0m04:39:38.484250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027293E55A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027294597E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027291B289B0>]}
[0m04:39:38.484933 [debug] [MainThread]: Flushing usage events
[0m04:39:38.888031 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:59:16.488878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021064E42480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210672B6F00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210672B7980>]}


============================== 04:59:16.495762 | 1e5ef742-69ed-4173-a47e-1d5d3191f6be ==============================
[0m04:59:16.495762 [info ] [MainThread]: Running with dbt=1.9.6
[0m04:59:16.499227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m04:59:16.704034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e5ef742-69ed-4173-a47e-1d5d3191f6be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021067EDE0C0>]}
[0m04:59:16.766823 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-z4o59lhl'
[0m04:59:16.768002 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m04:59:16.876859 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m04:59:16.878213 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m04:59:16.922699 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m04:59:16.931321 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m04:59:16.944804 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m04:59:16.947789 [debug] [MainThread]: Command `dbt deps` failed at 04:59:16.947429 after 0.85 seconds
[0m04:59:16.948861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021067BD10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210677ACE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021067E488F0>]}
[0m04:59:16.949581 [debug] [MainThread]: Flushing usage events
[0m04:59:17.324028 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:03:36.173453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E50DFB380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E50DF8140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E514FA630>]}


============================== 05:03:36.180614 | 483a4161-12e8-439a-b67b-ce069b46fc55 ==============================
[0m05:03:36.180614 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:03:36.182265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m05:03:36.426257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '483a4161-12e8-439a-b67b-ce069b46fc55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E516714C0>]}
[0m05:03:36.477045 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-hpl63y7p'
[0m05:03:36.478888 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:03:36.579571 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:03:36.580858 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:03:36.609484 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:03:36.619167 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_expectations was not found in the package index
[0m05:03:36.623058 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_expectations was not found in the package index

[0m05:03:36.626374 [debug] [MainThread]: Command `dbt deps` failed at 05:03:36.625931 after 0.91 seconds
[0m05:03:36.626936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E51643A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E50A4EAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E528F1580>]}
[0m05:03:36.627518 [debug] [MainThread]: Flushing usage events
[0m05:03:37.006723 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:05:29.825462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F55A623C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F58319370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F57E9F380>]}


============================== 05:05:29.830850 | 37736cbe-4e9a-4ea0-9ca5-215af3761dd2 ==============================
[0m05:05:29.830850 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:05:29.832527 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m05:05:30.029903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37736cbe-4e9a-4ea0-9ca5-215af3761dd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F58A8A6C0>]}
[0m05:05:30.058407 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-0tzy642b'
[0m05:05:30.059091 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:05:30.126202 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:05:30.127456 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:05:30.155396 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:05:30.164783 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m05:05:30.211245 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m05:05:30.212992 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_date` package is deprecated in favor of
`godatadriven/dbt_date`. Please update your `packages.yml` configuration to use
`godatadriven/dbt_date` instead.
[0m05:05:30.213660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '37736cbe-4e9a-4ea0-9ca5-215af3761dd2', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F59F45970>]}
[0m05:05:30.216973 [error] [MainThread]: Encountered an error:
Could not find a matching compatible version for package calogica/dbt_date
  Requested range: =0.4.0, =0.4.0
  Compatible versions: ['0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.5.0', '0.5.1', '0.5.2', '0.5.3', '0.5.4', '0.5.5', '0.5.6', '0.5.7', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.7.0', '0.7.1', '0.7.2', '0.8.0', '0.8.1', '0.9.0', '0.9.1', '0.9.2', '0.10.0', '0.10.1']

  Not shown: package versions incompatible with installed version of dbt-core
  To include them, run 'dbt --no-version-check deps'
[0m05:05:30.220812 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 124, in resolved
    raise PackageVersionNotFoundError(
dbt.exceptions.PackageVersionNotFoundError: Could not find a matching compatible version for package calogica/dbt_date
  Requested range: =0.4.0, =0.4.0
  Compatible versions: ['0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.5.0', '0.5.1', '0.5.2', '0.5.3', '0.5.4', '0.5.5', '0.5.6', '0.5.7', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.7.0', '0.7.1', '0.7.2', '0.8.0', '0.8.1', '0.9.0', '0.9.1', '0.9.2', '0.10.0', '0.10.1']

  Not shown: package versions incompatible with installed version of dbt-core
  To include them, run 'dbt --no-version-check deps'

[0m05:05:30.222130 [debug] [MainThread]: Command `dbt deps` failed at 05:05:30.222130 after 0.82 seconds
[0m05:05:30.223139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F58A35A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F58A34530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F588788F0>]}
[0m05:05:30.223940 [debug] [MainThread]: Flushing usage events
[0m05:05:30.601489 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:05:57.566349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48A2C2360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48CDDFDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48CDDF290>]}


============================== 05:05:57.572338 | ece8fae7-e286-4724-b7d6-1ef1b817104d ==============================
[0m05:05:57.572338 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:05:57.573768 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt --no-version-check deps', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:05:57.768719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ece8fae7-e286-4724-b7d6-1ef1b817104d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48C322240>]}
[0m05:05:57.792775 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-6c7lnn96'
[0m05:05:57.794039 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:05:57.853316 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:05:57.854724 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:05:57.880378 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:05:57.888455 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m05:05:57.936649 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m05:05:57.938396 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_date` package is deprecated in favor of
`godatadriven/dbt_date`. Please update your `packages.yml` configuration to use
`godatadriven/dbt_date` instead.
[0m05:05:57.938982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ece8fae7-e286-4724-b7d6-1ef1b817104d', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48E3D5610>]}
[0m05:05:57.942099 [error] [MainThread]: Encountered an error:
Package fivetran/dbt_audit_helper was not found in the package index
[0m05:05:57.945312 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package fivetran/dbt_audit_helper was not found in the package index

[0m05:05:57.947605 [debug] [MainThread]: Command `dbt deps` failed at 05:05:57.947605 after 0.65 seconds
[0m05:05:57.949015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48CDDFDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48D244470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A48D2C71A0>]}
[0m05:05:57.950033 [debug] [MainThread]: Flushing usage events
[0m05:05:58.393940 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:06:27.100878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E026F2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E0510B1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E0529BB00>]}


============================== 05:06:27.108548 | 4dcf1d59-888c-4e51-8ae7-d2d572e15548 ==============================
[0m05:06:27.108548 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:06:27.109693 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt --no-version-check deps', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:06:27.288218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4dcf1d59-888c-4e51-8ae7-d2d572e15548', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E057442C0>]}
[0m05:06:27.315927 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-bqgh92_7'
[0m05:06:27.317268 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:06:27.356388 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:06:27.357604 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:06:27.386982 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:06:27.394214 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m05:06:27.423245 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m05:06:27.424601 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `calogica/dbt_date` package is deprecated in favor of
`godatadriven/dbt_date`. Please update your `packages.yml` configuration to use
`godatadriven/dbt_date` instead.
[0m05:06:27.425950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '4dcf1d59-888c-4e51-8ae7-d2d572e15548', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E06893FB0>]}
[0m05:06:27.428239 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_incremental_utils was not found in the package index
[0m05:06:27.430923 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_incremental_utils was not found in the package index

[0m05:06:27.433134 [debug] [MainThread]: Command `dbt deps` failed at 05:06:27.432583 after 0.59 seconds
[0m05:06:27.433481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E04FC9100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E068568A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E068843B0>]}
[0m05:06:27.434564 [debug] [MainThread]: Flushing usage events
[0m05:06:27.815046 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:08:16.294633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162441EF1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016241712390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001624418D2B0>]}


============================== 05:08:16.299825 | bba88a07-90da-45ac-981c-5e63d61bd78b ==============================
[0m05:08:16.299825 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:08:16.301091 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'False', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt --no-version-check deps', 'send_anonymous_usage_stats': 'True'}
[0m05:08:16.543313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bba88a07-90da-45ac-981c-5e63d61bd78b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016243616E40>]}
[0m05:08:16.588816 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-ibc5nfml'
[0m05:08:16.589907 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:08:16.665801 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:08:16.666869 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:08:16.695721 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:08:16.703252 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:08:16.746125 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:08:16.748290 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_spark_utils was not found in the package index
[0m05:08:16.751527 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_spark_utils was not found in the package index

[0m05:08:16.752554 [debug] [MainThread]: Command `dbt deps` failed at 05:08:16.752554 after 0.87 seconds
[0m05:08:16.753229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016244527B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162446959D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162446E79B0>]}
[0m05:08:16.753861 [debug] [MainThread]: Flushing usage events
[0m05:08:17.143277 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:09:32.911935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C48D0DE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C48F31220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C48BBCBF0>]}


============================== 05:09:32.917830 | bc2784bc-9334-48b9-9c9f-5b6de4f9125a ==============================
[0m05:09:32.917830 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:09:32.919727 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'version_check': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt --no-version-check deps', 'send_anonymous_usage_stats': 'True'}
[0m05:09:33.113308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc2784bc-9334-48b9-9c9f-5b6de4f9125a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C492386E0>]}
[0m05:09:33.151648 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-zo5m3qdx'
[0m05:09:33.152450 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:09:33.246686 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:09:33.247922 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:09:33.274828 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:09:33.283289 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:09:33.323789 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:09:33.327070 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_postgres_utils was not found in the package index
[0m05:09:33.331602 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_postgres_utils was not found in the package index

[0m05:09:33.333653 [debug] [MainThread]: Command `dbt deps` failed at 05:09:33.332954 after 0.82 seconds
[0m05:09:33.334277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C491A9D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C49006210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C486C6720>]}
[0m05:09:33.334882 [debug] [MainThread]: Flushing usage events
[0m05:09:33.739087 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:09:55.346401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B4B63050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B4B601A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B4B63BC0>]}


============================== 05:09:55.353127 | a794fd86-2893-43f2-a639-a3989972ad02 ==============================
[0m05:09:55.353127 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:09:55.355160 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'False', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt --no-version-check deps', 'send_anonymous_usage_stats': 'True'}
[0m05:09:55.576182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a794fd86-2893-43f2-a639-a3989972ad02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B4D594F0>]}
[0m05:09:55.605289 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-gan54mn3'
[0m05:09:55.606519 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:09:55.667374 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:09:55.668793 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:09:55.708539 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:09:55.717819 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:09:55.758274 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:09:55.762566 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_snowflake_utils was not found in the package index
[0m05:09:55.765651 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_snowflake_utils was not found in the package index

[0m05:09:55.767469 [debug] [MainThread]: Command `dbt deps` failed at 05:09:55.767469 after 0.74 seconds
[0m05:09:55.768117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B4E57B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B44BB9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4B64F82C0>]}
[0m05:09:55.768809 [debug] [MainThread]: Flushing usage events
[0m05:09:56.178177 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:12:51.759902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD91DE5C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD91DE6900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD91DE6A20>]}


============================== 05:12:51.765087 | 0e57c326-70d8-496b-9bf2-8997cbcf7e82 ==============================
[0m05:12:51.765087 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:12:51.766187 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt deps', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m05:12:51.956199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e57c326-70d8-496b-9bf2-8997cbcf7e82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD917B3A40>]}
[0m05:12:51.987698 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-ueeizrfd'
[0m05:12:51.988447 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:12:52.077092 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:12:52.078613 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:12:52.107174 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:12:52.116555 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:12:52.153116 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:12:52.157584 [error] [MainThread]: Encountered an error:
Could not find a matching compatible version for package godatadriven/dbt_date
  Requested range: =0.4.0, =0.4.0
  Compatible versions: ['0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.5.0', '0.5.1', '0.5.2', '0.5.3', '0.5.4', '0.5.5', '0.5.6', '0.5.7', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.7.0', '0.7.1', '0.7.2', '0.8.0', '0.8.1', '0.9.0', '0.9.1', '0.9.2', '0.10.0', '0.10.1', '0.11.0', '0.12.0', '0.13.0', '0.14.0', '0.14.1']

  Not shown: package versions incompatible with installed version of dbt-core
  To include them, run 'dbt --no-version-check deps'
[0m05:12:52.161482 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 124, in resolved
    raise PackageVersionNotFoundError(
dbt.exceptions.PackageVersionNotFoundError: Could not find a matching compatible version for package godatadriven/dbt_date
  Requested range: =0.4.0, =0.4.0
  Compatible versions: ['0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.5.0', '0.5.1', '0.5.2', '0.5.3', '0.5.4', '0.5.5', '0.5.6', '0.5.7', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.7.0', '0.7.1', '0.7.2', '0.8.0', '0.8.1', '0.9.0', '0.9.1', '0.9.2', '0.10.0', '0.10.1', '0.11.0', '0.12.0', '0.13.0', '0.14.0', '0.14.1']

  Not shown: package versions incompatible with installed version of dbt-core
  To include them, run 'dbt --no-version-check deps'

[0m05:12:52.163383 [debug] [MainThread]: Command `dbt deps` failed at 05:12:52.163383 after 0.86 seconds
[0m05:12:52.164147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD92215A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD92267E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD922676B0>]}
[0m05:12:52.164918 [debug] [MainThread]: Flushing usage events
[0m05:12:52.597297 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:13:48.156690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836A9B8DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836B2D36E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836ABF11F0>]}


============================== 05:13:48.163154 | db6db790-cd97-4e35-80a4-3be4aec672a2 ==============================
[0m05:13:48.163154 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:13:48.164906 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt --no-version-check deps', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:13:48.346999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db6db790-cd97-4e35-80a4-3be4aec672a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028369F4B320>]}
[0m05:13:48.374541 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-glzrb_r4'
[0m05:13:48.376125 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:13:48.424647 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:13:48.425874 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:13:48.454650 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:13:48.463437 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:13:48.490479 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:13:48.492975 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_bigquery_utils was not found in the package index
[0m05:13:48.496118 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_bigquery_utils was not found in the package index

[0m05:13:48.498307 [debug] [MainThread]: Command `dbt deps` failed at 05:13:48.497555 after 0.63 seconds
[0m05:13:48.499004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836A9B8DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836A5EC470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002836B0BAA50>]}
[0m05:13:48.499608 [debug] [MainThread]: Flushing usage events
[0m05:13:48.912777 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:14:07.049450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B43F32300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B46C5A120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B46C591F0>]}


============================== 05:14:07.056176 | 9b0c9dce-a6a2-4e53-9691-2709fd4e267e ==============================
[0m05:14:07.056176 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:14:07.057959 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'version_check': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt --no-version-check deps', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:14:07.245952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b0c9dce-a6a2-4e53-9691-2709fd4e267e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B46F87DD0>]}
[0m05:14:07.271832 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-nic03c4a'
[0m05:14:07.273266 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:14:07.348857 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:14:07.350178 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:14:07.374396 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:14:07.382885 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:14:07.431263 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:14:07.434568 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_redshift_utils was not found in the package index
[0m05:14:07.438556 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_redshift_utils was not found in the package index

[0m05:14:07.440953 [debug] [MainThread]: Command `dbt deps` failed at 05:14:07.440321 after 0.73 seconds
[0m05:14:07.441517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B46BD6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B483EAAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026B483EAAE0>]}
[0m05:14:07.442073 [debug] [MainThread]: Flushing usage events
[0m05:14:07.841205 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:14:20.175149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B0125C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B01268A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B0126A80>]}


============================== 05:14:20.183494 | 972e7f23-5cff-46f2-99fa-c6424565bbbc ==============================
[0m05:14:20.183494 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:14:20.184639 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt --no-version-check deps', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m05:14:20.409485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '972e7f23-5cff-46f2-99fa-c6424565bbbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B1524980>]}
[0m05:14:20.443510 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-ei5rl0e_'
[0m05:14:20.444827 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:14:20.486926 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:14:20.488600 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:14:20.530650 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:14:20.539146 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:14:20.566183 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:14:20.568988 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_oracle_utils was not found in the package index
[0m05:14:20.572614 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_oracle_utils was not found in the package index

[0m05:14:20.574427 [debug] [MainThread]: Command `dbt deps` failed at 05:14:20.574427 after 0.73 seconds
[0m05:14:20.575008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9AA0E14F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B15A4230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9B1708B60>]}
[0m05:14:20.575520 [debug] [MainThread]: Flushing usage events
[0m05:14:20.937716 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:14:39.329216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFCAD8590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFC4A5190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFC4A5070>]}


============================== 05:14:39.335667 | 68f12949-980c-49a1-8c79-c05d7f2cc544 ==============================
[0m05:14:39.335667 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:14:39.336926 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt --no-version-check deps', 'send_anonymous_usage_stats': 'True'}
[0m05:14:39.552767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '68f12949-980c-49a1-8c79-c05d7f2cc544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFCF5B590>]}
[0m05:14:39.581173 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-o6rqixy4'
[0m05:14:39.582344 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:14:39.619283 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:14:39.620480 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:14:39.651589 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:14:39.661376 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:14:39.706586 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:14:39.709937 [error] [MainThread]: Encountered an error:
Package dbt-labs/dbt_sqlserver_utils was not found in the package index
[0m05:14:39.713751 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package dbt-labs/dbt_sqlserver_utils was not found in the package index

[0m05:14:39.715910 [debug] [MainThread]: Command `dbt deps` failed at 05:14:39.715910 after 0.66 seconds
[0m05:14:39.717078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFCA06510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FFE4701A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FF9FF7440>]}
[0m05:14:39.717823 [debug] [MainThread]: Flushing usage events
[0m05:14:40.113414 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:14:59.414596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D5F242450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D61DEBF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D61DEA180>]}


============================== 05:14:59.420620 | b4847c82-37e7-430c-920b-fed498d0579e ==============================
[0m05:14:59.420620 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:14:59.422803 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt --no-version-check deps', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:14:59.610163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4847c82-37e7-430c-920b-fed498d0579e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D61DEB830>]}
[0m05:14:59.637756 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-ueitdams'
[0m05:14:59.639190 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:14:59.706218 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:14:59.707383 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:14:59.760539 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:14:59.768939 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:14:59.816366 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:14:59.819511 [error] [MainThread]: Encountered an error:
Package fivetran/dbt_modeling was not found in the package index
[0m05:14:59.823463 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 218, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\cli\main.py", line 455, in deps
    results = task.run()
              ^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 211, in run
    self.lock()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\task\deps.py", line 187, in lock
    resolved_deps = resolve_packages(packages, self.project, self.cli_vars)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\resolver.py", line 131, in resolve_packages
    target = final[package].resolved().fetch_metadata(project, renderer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 98, in resolved
    self._check_in_index()
  File "C:\Project\DataPipeline\venv\Lib\site-packages\dbt\deps\registry.py", line 77, in _check_in_index
    raise PackageNotFoundError(self.package)
dbt.exceptions.PackageNotFoundError: Package fivetran/dbt_modeling was not found in the package index

[0m05:14:59.825479 [debug] [MainThread]: Command `dbt deps` failed at 05:14:59.824758 after 0.72 seconds
[0m05:14:59.826192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D62003800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D61E0CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D61E6C470>]}
[0m05:14:59.826890 [debug] [MainThread]: Flushing usage events
[0m05:15:00.263683 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:15:37.478211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2A342180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C8B10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C8B1220>]}


============================== 05:15:37.484418 | 987d4c28-1e22-4e24-9a00-0ccf609a8a38 ==============================
[0m05:15:37.484418 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:15:37.485769 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt --no-version-check deps', 'send_anonymous_usage_stats': 'True'}
[0m05:15:37.680091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '987d4c28-1e22-4e24-9a00-0ccf609a8a38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C7B79B0>]}
[0m05:15:37.714421 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-vie4itm0'
[0m05:15:37.715872 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m05:15:37.824609 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m05:15:37.826157 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m05:15:37.862580 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m05:15:37.871713 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json
[0m05:15:37.914755 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/godatadriven/dbt_date.json 200
[0m05:15:37.932062 [info ] [MainThread]: Updating lock file in file path: C:\Project\DataPipeline\dags\dbt_project/package-lock.yml
[0m05:15:37.938046 [debug] [MainThread]: Set downloads directory='C:\Users\GAURAV~1\AppData\Local\Temp\dbt-downloads-alpcl5ps'
[0m05:15:37.942194 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m05:15:38.476509 [info ] [MainThread]: Installed from version 0.8.6
[0m05:15:38.477261 [info ] [MainThread]: Updated version available: 1.3.0
[0m05:15:38.479648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '987d4c28-1e22-4e24-9a00-0ccf609a8a38', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2E4AC5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2E4AF860>]}
[0m05:15:38.480276 [info ] [MainThread]: Installing godatadriven/dbt_date
[0m05:15:38.800359 [info ] [MainThread]: Installed from version 0.5.0
[0m05:15:38.801046 [info ] [MainThread]: Updated version available: 0.14.1
[0m05:15:38.802487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '987d4c28-1e22-4e24-9a00-0ccf609a8a38', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C31A1B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2D1A03B0>]}
[0m05:15:38.803089 [info ] [MainThread]: 
[0m05:15:38.804221 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils', 'godatadriven/dbt_date']                 
Update your versions in packages.yml, then run dbt deps
[0m05:15:38.807675 [debug] [MainThread]: Command `dbt deps` succeeded at 05:15:38.807675 after 1.62 seconds
[0m05:15:38.808347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C7815B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2CD37260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FC2C7B79B0>]}
[0m05:15:38.808938 [debug] [MainThread]: Flushing usage events
[0m05:15:39.197012 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:16:24.720120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E710854CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E712B411F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7103479B0>]}


============================== 05:16:24.725527 | 1cfc3aec-2bdd-4277-8a93-29c2816bc729 ==============================
[0m05:16:24.725527 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:16:24.726731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:16:24.982622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E713037CE0>]}
[0m05:16:25.065784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7145AAA50>]}
[0m05:16:25.067506 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:16:25.421453 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:16:25.546641 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m05:16:25.547461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E714CA9280>]}
[0m05:16:27.439663 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'customers' in the 'models' section of file '../../models\schema.yml'
[0m05:16:27.446536 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'orders' in the 'models' section of file '../../models\schema.yml'
[0m05:16:27.453815 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'products' in the 'models' section of file '../../models\schema.yml'
[0m05:16:27.458496 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'reviews' in the 'models' section of file '../../models\schema.yml'
[0m05:16:27.971359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7152B4470>]}
[0m05:16:28.091746 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:16:28.143963 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:16:28.195773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E714C149B0>]}
[0m05:16:28.196291 [info ] [MainThread]: Found 6 models, 22 data tests, 4 sources, 766 macros
[0m05:16:28.196889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E714CAA150>]}
[0m05:16:28.201342 [info ] [MainThread]: 
[0m05:16:28.202053 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:16:28.202671 [info ] [MainThread]: 
[0m05:16:28.203357 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:16:28.209633 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m05:16:28.343909 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:16:28.344503 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m05:16:28.345203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:16:28.378438 [debug] [ThreadPool]: SQL status: BEGIN in 0.033 seconds
[0m05:16:28.379128 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:16:28.379662 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:16:28.389330 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.009 seconds
[0m05:16:28.391407 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m05:16:28.393220 [debug] [ThreadPool]: On list_datamart_public: Close
[0m05:16:28.401915 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:28.401915 [debug] [MainThread]: On master: BEGIN
[0m05:16:28.402618 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:16:28.423894 [debug] [MainThread]: SQL status: BEGIN in 0.021 seconds
[0m05:16:28.424578 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:28.425360 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m05:16:28.433116 [debug] [MainThread]: SQL status: SELECT 5 in 0.008 seconds
[0m05:16:28.435777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cfc3aec-2bdd-4277-8a93-29c2816bc729', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E715454E30>]}
[0m05:16:28.436469 [debug] [MainThread]: On master: ROLLBACK
[0m05:16:28.437625 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:28.438338 [debug] [MainThread]: On master: BEGIN
[0m05:16:28.441047 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m05:16:28.441047 [debug] [MainThread]: On master: COMMIT
[0m05:16:28.441748 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:28.442435 [debug] [MainThread]: On master: COMMIT
[0m05:16:28.443964 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:16:28.444623 [debug] [MainThread]: On master: Close
[0m05:16:28.454180 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m05:16:28.454775 [info ] [Thread-1 (]: 1 of 22 START test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [RUN]
[0m05:16:28.456068 [debug] [Thread-1 (]: Acquiring new postgres connection 'test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6'
[0m05:16:28.456670 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m05:16:28.548261 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m05:16:28.550345 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m05:16:28.577364 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m05:16:28.579083 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m05:16:28.579611 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: BEGIN
[0m05:16:28.580168 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:16:28.602928 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:16:28.603505 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m05:16:28.604099 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_method

)

select *
from all_values
where value_field not in (
    'Credit Card','PayPal','Bank Transfer'
)



  
  
      
    ) dbt_internal_test
[0m05:16:28.608660 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.004 seconds
[0m05:16:28.614641 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: ROLLBACK
[0m05:16:28.616027 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: Close
[0m05:16:28.617835 [info ] [Thread-1 (]: 1 of 22 PASS source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [[32mPASS[0m in 0.16s]
[0m05:16:28.618944 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m05:16:28.619561 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m05:16:28.619869 [info ] [Thread-1 (]: 2 of 22 START test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending  [RUN]
[0m05:16:28.620827 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6, now test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700)
[0m05:16:28.621935 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m05:16:28.629630 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m05:16:28.631083 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m05:16:28.635671 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m05:16:28.637338 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m05:16:28.637646 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: BEGIN
[0m05:16:28.639114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.666021 [debug] [Thread-1 (]: SQL status: BEGIN in 0.027 seconds
[0m05:16:28.666713 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m05:16:28.667638 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        transaction_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by transaction_payment_status

)

select *
from all_values
where value_field not in (
    'Completed','Failed','Pending'
)



  
  
      
    ) dbt_internal_test
[0m05:16:28.669965 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "transaction_payment_status" does not exist
LINE 17:         transaction_payment_status as value_field,
                 ^

[0m05:16:28.670546 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: ROLLBACK
[0m05:16:28.672617 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: Close
[0m05:16:28.680408 [debug] [Thread-1 (]: Database Error in test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  column "transaction_payment_status" does not exist
  LINE 17:         transaction_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_b9c989267c20c7ce6e61eefb643b9c68.sql
[0m05:16:28.681983 [error] [Thread-1 (]: 2 of 22 ERROR source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending  [[31mERROR[0m in 0.06s]
[0m05:16:28.683026 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m05:16:28.684364 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m05:16:28.685083 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  column "transaction_payment_status" does not exist
  LINE 17:         transaction_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_b9c989267c20c7ce6e61eefb643b9c68.sql.
[0m05:16:28.685885 [info ] [Thread-1 (]: 3 of 22 START test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [RUN]
[0m05:16:28.687716 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700, now test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e)
[0m05:16:28.688368 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m05:16:28.695955 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m05:16:28.707207 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m05:16:28.712285 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m05:16:28.714775 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m05:16:28.715456 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: BEGIN
[0m05:16:28.716103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.745339 [debug] [Thread-1 (]: SQL status: BEGIN in 0.029 seconds
[0m05:16:28.746665 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m05:16:28.747248 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        order_level_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."raw_data"
    group by order_level_payment_status

)

select *
from all_values
where value_field not in (
    'Paid','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m05:16:28.749726 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "order_level_payment_status" does not exist
LINE 17:         order_level_payment_status as value_field,
                 ^

[0m05:16:28.750352 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: ROLLBACK
[0m05:16:28.751978 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: Close
[0m05:16:28.756779 [debug] [Thread-1 (]: Database Error in test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  column "order_level_payment_status" does not exist
  LINE 17:         order_level_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_2c7045386a3842912406eccccc35de24.sql
[0m05:16:28.757561 [error] [Thread-1 (]: 3 of 22 ERROR source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [[31mERROR[0m in 0.07s]
[0m05:16:28.758075 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m05:16:28.758816 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m05:16:28.759399 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e' to be skipped because of status 'error'.  Reason: Database Error in test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  column "order_level_payment_status" does not exist
  LINE 17:         order_level_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_2c7045386a3842912406eccccc35de24.sql.
[0m05:16:28.759981 [info ] [Thread-1 (]: 4 of 22 START test source_not_null_raw_customers_source_customer_id ............ [RUN]
[0m05:16:28.761306 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e, now test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556)
[0m05:16:28.761675 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m05:16:28.771678 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m05:16:28.773110 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m05:16:28.776249 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m05:16:28.778038 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m05:16:28.778038 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: BEGIN
[0m05:16:28.779466 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.799866 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m05:16:28.800479 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m05:16:28.801090 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."customers_source"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:28.803433 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:28.805947 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: ROLLBACK
[0m05:16:28.807138 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: Close
[0m05:16:28.808372 [info ] [Thread-1 (]: 4 of 22 PASS source_not_null_raw_customers_source_customer_id .................. [[32mPASS[0m in 0.05s]
[0m05:16:28.809369 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m05:16:28.809879 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m05:16:28.810486 [info ] [Thread-1 (]: 5 of 22 START test source_not_null_raw_customers_source_signup_date ............ [RUN]
[0m05:16:28.811069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556, now test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f)
[0m05:16:28.811707 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m05:16:28.819774 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m05:16:28.821114 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m05:16:28.824377 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m05:16:28.826159 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m05:16:28.826159 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: BEGIN
[0m05:16:28.826800 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.846835 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m05:16:28.847437 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m05:16:28.848067 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from "datamart"."raw"."customers_source"
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m05:16:28.849897 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:28.852517 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: ROLLBACK
[0m05:16:28.853561 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: Close
[0m05:16:28.856562 [info ] [Thread-1 (]: 5 of 22 PASS source_not_null_raw_customers_source_signup_date .................. [[32mPASS[0m in 0.04s]
[0m05:16:28.857752 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m05:16:28.858360 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m05:16:28.858900 [info ] [Thread-1 (]: 6 of 22 START test source_not_null_raw_payments_order_id ....................... [RUN]
[0m05:16:28.858900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f, now test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9)
[0m05:16:28.860287 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m05:16:28.866602 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m05:16:28.867788 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m05:16:28.871320 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m05:16:28.871919 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m05:16:28.872510 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: BEGIN
[0m05:16:28.873111 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.910655 [debug] [Thread-1 (]: SQL status: BEGIN in 0.037 seconds
[0m05:16:28.911286 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m05:16:28.911981 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."payments"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:28.914399 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:28.916227 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: ROLLBACK
[0m05:16:28.917423 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: Close
[0m05:16:28.918023 [info ] [Thread-1 (]: 6 of 22 PASS source_not_null_raw_payments_order_id ............................. [[32mPASS[0m in 0.06s]
[0m05:16:28.919770 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m05:16:28.920131 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m05:16:28.920975 [info ] [Thread-1 (]: 7 of 22 START test source_not_null_raw_payments_payment_id ..................... [RUN]
[0m05:16:28.921486 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9, now test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5)
[0m05:16:28.922123 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m05:16:28.927824 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m05:16:28.929343 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m05:16:28.932765 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m05:16:28.933942 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m05:16:28.934585 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: BEGIN
[0m05:16:28.934585 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:28.957119 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:16:28.958300 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m05:16:28.958910 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select payment_id
from "datamart"."raw"."payments"
where payment_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:28.961420 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:28.963475 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: ROLLBACK
[0m05:16:28.964684 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: Close
[0m05:16:28.965916 [info ] [Thread-1 (]: 7 of 22 PASS source_not_null_raw_payments_payment_id ........................... [[32mPASS[0m in 0.04s]
[0m05:16:28.967904 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m05:16:28.968524 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m05:16:28.969750 [info ] [Thread-1 (]: 8 of 22 START test source_not_null_raw_products_category ....................... [RUN]
[0m05:16:28.970566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5, now test.data_pipeline.source_not_null_raw_products_category.9265557239)
[0m05:16:28.970566 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m05:16:28.976237 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m05:16:28.978253 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m05:16:28.982457 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m05:16:28.983648 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m05:16:28.984389 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: BEGIN
[0m05:16:28.984389 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.008678 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m05:16:29.009275 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m05:16:29.009861 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_category.9265557239"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select category
from "datamart"."raw"."products"
where category is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.012655 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.014559 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: ROLLBACK
[0m05:16:29.015824 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: Close
[0m05:16:29.016977 [info ] [Thread-1 (]: 8 of 22 PASS source_not_null_raw_products_category ............................. [[32mPASS[0m in 0.05s]
[0m05:16:29.017788 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m05:16:29.018556 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m05:16:29.019345 [info ] [Thread-1 (]: 9 of 22 START test source_not_null_raw_products_price .......................... [RUN]
[0m05:16:29.020504 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_category.9265557239, now test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b)
[0m05:16:29.021022 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m05:16:29.028186 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m05:16:29.028904 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m05:16:29.031951 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m05:16:29.033127 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m05:16:29.033731 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: BEGIN
[0m05:16:29.034302 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.059809 [debug] [Thread-1 (]: SQL status: BEGIN in 0.025 seconds
[0m05:16:29.060323 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m05:16:29.062026 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select price
from "datamart"."raw"."products"
where price is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.065951 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m05:16:29.068181 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: ROLLBACK
[0m05:16:29.069828 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: Close
[0m05:16:29.071530 [info ] [Thread-1 (]: 9 of 22 PASS source_not_null_raw_products_price ................................ [[32mPASS[0m in 0.05s]
[0m05:16:29.073383 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m05:16:29.074562 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m05:16:29.075175 [info ] [Thread-1 (]: 10 of 22 START test source_not_null_raw_products_product_id .................... [RUN]
[0m05:16:29.075769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b, now test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae)
[0m05:16:29.076396 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m05:16:29.082645 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m05:16:29.084516 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m05:16:29.087652 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m05:16:29.089057 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m05:16:29.089750 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: BEGIN
[0m05:16:29.090172 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.120579 [debug] [Thread-1 (]: SQL status: BEGIN in 0.028 seconds
[0m05:16:29.120956 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m05:16:29.122412 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."products"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.128987 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.004 seconds
[0m05:16:29.133803 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: ROLLBACK
[0m05:16:29.136012 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: Close
[0m05:16:29.138309 [info ] [Thread-1 (]: 10 of 22 PASS source_not_null_raw_products_product_id .......................... [[32mPASS[0m in 0.06s]
[0m05:16:29.140654 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m05:16:29.141211 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m05:16:29.141746 [info ] [Thread-1 (]: 11 of 22 START test source_not_null_raw_products_product_name .................. [RUN]
[0m05:16:29.143029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae, now test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51)
[0m05:16:29.144179 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m05:16:29.151960 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m05:16:29.153143 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m05:16:29.156499 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m05:16:29.158239 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m05:16:29.158819 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: BEGIN
[0m05:16:29.158819 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.181657 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:16:29.182340 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m05:16:29.182932 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_name
from "datamart"."raw"."products"
where product_name is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.185529 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.187288 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: ROLLBACK
[0m05:16:29.188581 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: Close
[0m05:16:29.189177 [info ] [Thread-1 (]: 11 of 22 PASS source_not_null_raw_products_product_name ........................ [[32mPASS[0m in 0.05s]
[0m05:16:29.190300 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m05:16:29.191139 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m05:16:29.191653 [info ] [Thread-1 (]: 12 of 22 START test source_not_null_raw_raw_data_customer_id ................... [RUN]
[0m05:16:29.192347 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51, now test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3)
[0m05:16:29.193123 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m05:16:29.198996 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m05:16:29.200229 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m05:16:29.203272 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m05:16:29.204454 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m05:16:29.205030 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: BEGIN
[0m05:16:29.205030 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.228173 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:16:29.228628 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m05:16:29.229555 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."raw_data"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.232341 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.233722 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: ROLLBACK
[0m05:16:29.235682 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: Close
[0m05:16:29.237551 [info ] [Thread-1 (]: 12 of 22 PASS source_not_null_raw_raw_data_customer_id ......................... [[32mPASS[0m in 0.04s]
[0m05:16:29.239430 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m05:16:29.240044 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m05:16:29.240641 [info ] [Thread-1 (]: 13 of 22 START test source_not_null_raw_raw_data_order_date .................... [RUN]
[0m05:16:29.241230 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3, now test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc)
[0m05:16:29.241834 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m05:16:29.249598 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m05:16:29.251016 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m05:16:29.254402 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m05:16:29.255699 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m05:16:29.256014 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: BEGIN
[0m05:16:29.256720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.276760 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m05:16:29.277480 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m05:16:29.278570 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_date
from "datamart"."raw"."raw_data"
where order_date is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.281186 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.283982 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: ROLLBACK
[0m05:16:29.285454 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: Close
[0m05:16:29.287083 [info ] [Thread-1 (]: 13 of 22 PASS source_not_null_raw_raw_data_order_date .......................... [[32mPASS[0m in 0.05s]
[0m05:16:29.288319 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m05:16:29.289476 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m05:16:29.290206 [info ] [Thread-1 (]: 14 of 22 START test source_not_null_raw_raw_data_order_id ...................... [RUN]
[0m05:16:29.290924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc, now test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6)
[0m05:16:29.291675 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m05:16:29.297223 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m05:16:29.298725 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m05:16:29.302117 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m05:16:29.303374 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m05:16:29.303374 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: BEGIN
[0m05:16:29.304048 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.337217 [debug] [Thread-1 (]: SQL status: BEGIN in 0.033 seconds
[0m05:16:29.338583 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m05:16:29.339219 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."raw_data"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.341955 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.343955 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: ROLLBACK
[0m05:16:29.345898 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: Close
[0m05:16:29.347146 [info ] [Thread-1 (]: 14 of 22 PASS source_not_null_raw_raw_data_order_id ............................ [[32mPASS[0m in 0.06s]
[0m05:16:29.348709 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m05:16:29.349370 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m05:16:29.350823 [info ] [Thread-1 (]: 15 of 22 START test source_not_null_raw_raw_data_product_id .................... [RUN]
[0m05:16:29.351471 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6, now test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e)
[0m05:16:29.352231 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m05:16:29.357283 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m05:16:29.358698 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m05:16:29.362260 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m05:16:29.363565 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m05:16:29.364269 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: BEGIN
[0m05:16:29.364269 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.387659 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:16:29.388771 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m05:16:29.388771 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."raw_data"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.390637 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "product_id" does not exist
LINE 16: select product_id
                ^

[0m05:16:29.392329 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: ROLLBACK
[0m05:16:29.393687 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: Close
[0m05:16:29.400076 [debug] [Thread-1 (]: Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql
[0m05:16:29.400702 [error] [Thread-1 (]: 15 of 22 ERROR source_not_null_raw_raw_data_product_id ......................... [[31mERROR[0m in 0.05s]
[0m05:16:29.402121 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m05:16:29.402826 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m05:16:29.404370 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e' to be skipped because of status 'error'.  Reason: Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql.
[0m05:16:29.402826 [info ] [Thread-1 (]: 16 of 22 START test source_not_null_raw_raw_data_total_amount .................. [RUN]
[0m05:16:29.405708 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e, now test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e)
[0m05:16:29.405708 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m05:16:29.412160 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m05:16:29.413172 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m05:16:29.416646 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m05:16:29.418040 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m05:16:29.420455 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: BEGIN
[0m05:16:29.421631 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.445966 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m05:16:29.446477 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m05:16:29.447233 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_amount
from "datamart"."raw"."raw_data"
where total_amount is null



  
  
      
    ) dbt_internal_test
[0m05:16:29.449410 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.451234 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: ROLLBACK
[0m05:16:29.453094 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: Close
[0m05:16:29.455145 [info ] [Thread-1 (]: 16 of 22 PASS source_not_null_raw_raw_data_total_amount ........................ [[32mPASS[0m in 0.05s]
[0m05:16:29.456929 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m05:16:29.456929 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m05:16:29.457584 [info ] [Thread-1 (]: 17 of 22 START test source_positive_values_raw_products_price .................. [RUN]
[0m05:16:29.458321 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e, now test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9)
[0m05:16:29.459033 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m05:16:29.467267 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m05:16:29.467875 [error] [Thread-1 (]: 17 of 22 ERROR source_positive_values_raw_products_price ....................... [[31mERROR[0m in 0.01s]
[0m05:16:29.469098 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9
[0m05:16:29.469770 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m05:16:29.470554 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m05:16:29.471595 [info ] [Thread-1 (]: 18 of 22 START test source_positive_values_raw_raw_data_total_amount ........... [RUN]
[0m05:16:29.472176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_products_price.d8701c9fb9, now test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3)
[0m05:16:29.472608 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m05:16:29.479853 [debug] [Thread-1 (]: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m05:16:29.480630 [error] [Thread-1 (]: 18 of 22 ERROR source_positive_values_raw_raw_data_total_amount ................ [[31mERROR[0m in 0.01s]
[0m05:16:29.481347 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3
[0m05:16:29.482155 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m05:16:29.482858 [debug] [Thread-4 (]: Marking all children of 'test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3' to be skipped because of status 'error'.  Reason: Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m05:16:29.483592 [info ] [Thread-1 (]: 19 of 22 START test source_unique_raw_customers_source_customer_id ............. [RUN]
[0m05:16:29.484442 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_positive_values_raw_raw_data_total_amount.fe8c89a6b3, now test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c)
[0m05:16:29.485014 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m05:16:29.493220 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m05:16:29.494816 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m05:16:29.499197 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m05:16:29.499924 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m05:16:29.500646 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: BEGIN
[0m05:16:29.501254 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.532233 [debug] [Thread-1 (]: SQL status: BEGIN in 0.031 seconds
[0m05:16:29.532973 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m05:16:29.533684 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m05:16:29.535910 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.538089 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: ROLLBACK
[0m05:16:29.539586 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: Close
[0m05:16:29.540377 [info ] [Thread-1 (]: 19 of 22 PASS source_unique_raw_customers_source_customer_id ................... [[32mPASS[0m in 0.06s]
[0m05:16:29.542438 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m05:16:29.542896 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m05:16:29.544077 [info ] [Thread-1 (]: 20 of 22 START test source_unique_raw_customers_source_email ................... [RUN]
[0m05:16:29.544789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c, now test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b)
[0m05:16:29.545580 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m05:16:29.550650 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m05:16:29.552770 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m05:16:29.555686 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m05:16:29.557838 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m05:16:29.558656 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: BEGIN
[0m05:16:29.559385 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.582186 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:16:29.582886 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m05:16:29.582886 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    email as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where email is not null
group by email
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m05:16:29.585759 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.588383 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: ROLLBACK
[0m05:16:29.590194 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: Close
[0m05:16:29.591508 [info ] [Thread-1 (]: 20 of 22 PASS source_unique_raw_customers_source_email ......................... [[32mPASS[0m in 0.05s]
[0m05:16:29.592602 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m05:16:29.593114 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m05:16:29.593820 [info ] [Thread-1 (]: 21 of 22 START test source_unique_raw_payments_payment_id ...................... [RUN]
[0m05:16:29.595158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b, now test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533)
[0m05:16:29.595808 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m05:16:29.601262 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m05:16:29.602651 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m05:16:29.607130 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m05:16:29.608496 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m05:16:29.609190 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: BEGIN
[0m05:16:29.610067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.632987 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:16:29.633739 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m05:16:29.634424 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."payments"
where payment_id is not null
group by payment_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m05:16:29.636453 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.638938 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: ROLLBACK
[0m05:16:29.641004 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: Close
[0m05:16:29.642152 [info ] [Thread-1 (]: 21 of 22 PASS source_unique_raw_payments_payment_id ............................ [[32mPASS[0m in 0.05s]
[0m05:16:29.643368 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m05:16:29.643368 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m05:16:29.644215 [info ] [Thread-1 (]: 22 of 22 START test source_unique_raw_products_product_id ...................... [RUN]
[0m05:16:29.645682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533, now test.data_pipeline.source_unique_raw_products_product_id.518dac90ba)
[0m05:16:29.646390 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m05:16:29.651625 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m05:16:29.653161 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m05:16:29.656384 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m05:16:29.657570 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m05:16:29.657570 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: BEGIN
[0m05:16:29.658373 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:16:29.693390 [debug] [Thread-1 (]: SQL status: BEGIN in 0.034 seconds
[0m05:16:29.693967 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m05:16:29.694659 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."products"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m05:16:29.697046 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m05:16:29.698503 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: ROLLBACK
[0m05:16:29.699739 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: Close
[0m05:16:29.700900 [info ] [Thread-1 (]: 22 of 22 PASS source_unique_raw_products_product_id ............................ [[32mPASS[0m in 0.06s]
[0m05:16:29.702156 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m05:16:29.703925 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:29.704522 [debug] [MainThread]: On master: BEGIN
[0m05:16:29.705115 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:16:29.723694 [debug] [MainThread]: SQL status: BEGIN in 0.018 seconds
[0m05:16:29.724415 [debug] [MainThread]: On master: COMMIT
[0m05:16:29.724415 [debug] [MainThread]: Using postgres connection "master"
[0m05:16:29.725135 [debug] [MainThread]: On master: COMMIT
[0m05:16:29.726586 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:16:29.727259 [debug] [MainThread]: On master: Close
[0m05:16:29.727873 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:16:29.727873 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m05:16:29.727873 [debug] [MainThread]: Connection 'test.data_pipeline.source_unique_raw_products_product_id.518dac90ba' was properly closed.
[0m05:16:29.730111 [info ] [MainThread]: 
[0m05:16:29.730627 [info ] [MainThread]: Finished running 22 data tests in 0 hours 0 minutes and 1.53 seconds (1.53s).
[0m05:16:29.736517 [debug] [MainThread]: Command end result
[0m05:16:29.777902 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:16:29.781651 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:16:29.790470 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m05:16:29.790470 [info ] [MainThread]: 
[0m05:16:29.791652 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 0 warnings:[0m
[0m05:16:29.792268 [info ] [MainThread]: 
[0m05:16:29.792873 [error] [MainThread]:   Database Error in test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending (../../models\sources.yml)
  column "transaction_payment_status" does not exist
  LINE 17:         transaction_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_b9c989267c20c7ce6e61eefb643b9c68.sql
[0m05:16:29.794075 [info ] [MainThread]: 
[0m05:16:29.794678 [error] [MainThread]:   Database Error in test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded (../../models\sources.yml)
  column "order_level_payment_status" does not exist
  LINE 17:         order_level_payment_status as value_field,
                   ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_accepted_values_raw_raw_2c7045386a3842912406eccccc35de24.sql
[0m05:16:29.795280 [info ] [MainThread]: 
[0m05:16:29.795855 [error] [MainThread]:   Database Error in test source_not_null_raw_raw_data_product_id (../../models\sources.yml)
  column "product_id" does not exist
  LINE 16: select product_id
                  ^
  compiled code at target\run\data_pipeline\../../models\sources.yml\source_not_null_raw_raw_data_product_id.sql
[0m05:16:29.795855 [info ] [MainThread]: 
[0m05:16:29.796455 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_products_price (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m05:16:29.797679 [info ] [MainThread]: 
[0m05:16:29.798790 [error] [MainThread]:   Compilation Error in test source_positive_values_raw_raw_data_total_amount (../../models\sources.yml)
  'test_positive_values' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m05:16:29.799405 [info ] [MainThread]: 
[0m05:16:29.799743 [info ] [MainThread]: Done. PASS=17 WARN=0 ERROR=5 SKIP=0 TOTAL=22
[0m05:16:29.801352 [debug] [MainThread]: Command `dbt test` failed at 05:16:29.801352 after 5.37 seconds
[0m05:16:29.801928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E712E9FB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E712E9C7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E712E7E930>]}
[0m05:16:29.802495 [debug] [MainThread]: Flushing usage events
[0m05:16:30.177711 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:30:38.129260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE2CA22A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE5215160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE5215040>]}


============================== 05:30:38.134802 | 775f8323-4276-4f19-80c7-6e8b46f083e8 ==============================
[0m05:30:38.134802 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:30:38.135985 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:30:38.401830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE71AFE60>]}
[0m05:30:38.480222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE59C1460>]}
[0m05:30:38.481402 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:30:38.840304 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:30:39.078081 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:30:39.079913 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\sources.yml
[0m05:30:39.806122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7BBF500>]}
[0m05:30:39.978321 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:30:40.033062 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:30:40.067879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7BBEC30>]}
[0m05:30:40.069150 [info ] [MainThread]: Found 6 models, 20 data tests, 4 sources, 766 macros
[0m05:30:40.070023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE77D53A0>]}
[0m05:30:40.073357 [info ] [MainThread]: 
[0m05:30:40.074188 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:30:40.077067 [info ] [MainThread]: 
[0m05:30:40.078293 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:30:40.084155 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m05:30:40.193354 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m05:30:40.194399 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m05:30:40.194399 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:30:40.222907 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.028 seconds
[0m05:30:40.224719 [debug] [ThreadPool]: On list_datamart: Close
[0m05:30:40.229482 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m05:30:40.237291 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:30:40.237967 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m05:30:40.238620 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:30:40.264723 [debug] [ThreadPool]: SQL status: BEGIN in 0.026 seconds
[0m05:30:40.265324 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:30:40.265984 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:30:40.270371 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.004 seconds
[0m05:30:40.272895 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m05:30:40.274205 [debug] [ThreadPool]: On list_datamart_public: Close
[0m05:30:40.281144 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.281463 [debug] [MainThread]: On master: BEGIN
[0m05:30:40.281972 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:30:40.304687 [debug] [MainThread]: SQL status: BEGIN in 0.022 seconds
[0m05:30:40.305197 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.305197 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m05:30:40.311049 [debug] [MainThread]: SQL status: SELECT 0 in 0.005 seconds
[0m05:30:40.313116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7B748F0>]}
[0m05:30:40.313757 [debug] [MainThread]: On master: ROLLBACK
[0m05:30:40.315728 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.316294 [debug] [MainThread]: On master: BEGIN
[0m05:30:40.318805 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m05:30:40.319366 [debug] [MainThread]: On master: COMMIT
[0m05:30:40.319969 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.320329 [debug] [MainThread]: On master: COMMIT
[0m05:30:40.321434 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:30:40.321957 [debug] [MainThread]: On master: Close
[0m05:30:40.330129 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m05:30:40.331217 [info ] [Thread-1 (]: 1 of 6 START sql view model public.stg_payments ................................ [RUN]
[0m05:30:40.333016 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.stg_payments'
[0m05:30:40.334103 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m05:30:40.347151 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m05:30:40.349182 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m05:30:40.394573 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m05:30:40.396025 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:30:40.396762 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m05:30:40.397470 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:30:40.418041 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m05:30:40.419244 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:30:40.419244 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql

-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        payment_status -- This is the 'payment_status' for the transaction itself
    FROM
        "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(payment_status AS VARCHAR) AS status -- Renaming to 'status' for consistency if needed later
FROM
    source_payments
WHERE
    payment_id IS NOT NULL
  );
[0m05:30:40.423176 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m05:30:40.431375 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:30:40.431936 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m05:30:40.434098 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:30:40.458710 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m05:30:40.460147 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:30:40.461581 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m05:30:40.464472 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:30:40.476192 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m05:30:40.486153 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:30:40.487298 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m05:30:40.489377 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m05:30:40.493277 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m05:30:40.495728 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE2D66540>]}
[0m05:30:40.496369 [info ] [Thread-1 (]: 1 of 6 OK created sql view model public.stg_payments ........................... [[32mCREATE VIEW[0m in 0.16s]
[0m05:30:40.498604 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m05:30:40.498604 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m05:30:40.499808 [info ] [Thread-1 (]: 2 of 6 START sql view model public.stg_products ................................ [RUN]
[0m05:30:40.500584 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m05:30:40.501277 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m05:30:40.505250 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m05:30:40.506009 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m05:30:40.514284 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m05:30:40.516342 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:30:40.517452 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m05:30:40.518126 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:30:40.542453 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m05:30:40.542993 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:30:40.543533 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name AS name, -- Renaming to match schema.yml 'name'
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(name AS VARCHAR) AS name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m05:30:40.548394 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:30:40.552739 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:30:40.553338 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m05:30:40.555115 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:30:40.556291 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:30:40.556879 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:30:40.557465 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:30:40.559899 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m05:30:40.564779 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m05:30:40.565562 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:30:40.566467 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m05:30:40.568190 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:30:40.570000 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m05:30:40.570716 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7F18650>]}
[0m05:30:40.571964 [info ] [Thread-1 (]: 2 of 6 OK created sql view model public.stg_products ........................... [[32mCREATE VIEW[0m in 0.07s]
[0m05:30:40.573272 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m05:30:40.573272 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m05:30:40.574573 [info ] [Thread-1 (]: 3 of 6 START sql view model public.stg_raw_data ................................ [RUN]
[0m05:30:40.575211 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m05:30:40.575211 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m05:30:40.578873 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m05:30:40.580644 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m05:30:40.584804 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m05:30:40.585976 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:30:40.586581 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m05:30:40.586581 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:30:40.608806 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:30:40.609429 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:30:40.610040 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m05:30:40.611894 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "order_level_payment_status" does not exist
LINE 19:         order_level_payment_status,
                 ^

[0m05:30:40.612530 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: ROLLBACK
[0m05:30:40.613723 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m05:30:40.621244 [debug] [Thread-1 (]: Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "order_level_payment_status" does not exist
  LINE 19:         order_level_payment_status,
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql
[0m05:30:40.621864 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '775f8323-4276-4f19-80c7-6e8b46f083e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7BA5550>]}
[0m05:30:40.623156 [error] [Thread-1 (]: 3 of 6 ERROR creating sql view model public.stg_raw_data ....................... [[31mERROR[0m in 0.05s]
[0m05:30:40.624470 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m05:30:40.625269 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.stg_raw_data' to be skipped because of status 'error'.  Reason: Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "order_level_payment_status" does not exist
  LINE 19:         order_level_payment_status,
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql.
[0m05:30:40.626608 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers
[0m05:30:40.627221 [info ] [Thread-1 (]: 4 of 6 SKIP relation public.stg_customers ...................................... [[33mSKIP[0m]
[0m05:30:40.627830 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers
[0m05:30:40.628441 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m05:30:40.629027 [info ] [Thread-1 (]: 5 of 6 SKIP relation public.stg_orders ......................................... [[33mSKIP[0m]
[0m05:30:40.629606 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m05:30:40.630196 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m05:30:40.631145 [info ] [Thread-1 (]: 6 of 6 SKIP relation public.raw_to_normalized .................................. [[33mSKIP[0m]
[0m05:30:40.631731 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m05:30:40.633175 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.633790 [debug] [MainThread]: On master: BEGIN
[0m05:30:40.634389 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:30:40.656575 [debug] [MainThread]: SQL status: BEGIN in 0.022 seconds
[0m05:30:40.657208 [debug] [MainThread]: On master: COMMIT
[0m05:30:40.657818 [debug] [MainThread]: Using postgres connection "master"
[0m05:30:40.658131 [debug] [MainThread]: On master: COMMIT
[0m05:30:40.659299 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:30:40.659897 [debug] [MainThread]: On master: Close
[0m05:30:40.660547 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:30:40.661643 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m05:30:40.662155 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m05:30:40.662905 [debug] [MainThread]: Connection 'model.data_pipeline.stg_raw_data' was properly closed.
[0m05:30:40.663416 [info ] [MainThread]: 
[0m05:30:40.664037 [info ] [MainThread]: Finished running 6 view models in 0 hours 0 minutes and 0.58 seconds (0.58s).
[0m05:30:40.665913 [debug] [MainThread]: Command end result
[0m05:30:40.705277 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:30:40.708733 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:30:40.716192 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m05:30:40.717560 [info ] [MainThread]: 
[0m05:30:40.718098 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m05:30:40.718725 [info ] [MainThread]: 
[0m05:30:40.719303 [error] [MainThread]:   Database Error in model stg_raw_data (../../models\staging\stg_raw_data.sql)
  column "order_level_payment_status" does not exist
  LINE 19:         order_level_payment_status,
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_raw_data.sql
[0m05:30:40.719893 [info ] [MainThread]: 
[0m05:30:40.720441 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=3 TOTAL=6
[0m05:30:40.721529 [debug] [MainThread]: Command `dbt run` failed at 05:30:40.721529 after 2.85 seconds
[0m05:30:40.722062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE4C23E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE5215070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019FE7B4CF20>]}
[0m05:30:40.722610 [debug] [MainThread]: Flushing usage events
[0m05:30:41.118953 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:34:56.544947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550BD24BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550C564170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550C02FAD0>]}


============================== 05:34:56.552323 | 783e8512-ad41-4e5e-84be-e8aeb6be2a5b ==============================
[0m05:34:56.552323 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:34:56.554481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:34:56.909775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015509D1B440>]}
[0m05:34:56.992471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550C191190>]}
[0m05:34:56.994590 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:34:57.328213 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:34:57.574298 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:34:57.575003 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:34:57.629533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550E2A3620>]}
[0m05:34:57.775916 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:34:57.855007 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:34:57.905838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550E78AAB0>]}
[0m05:34:57.906388 [info ] [MainThread]: Found 6 models, 20 data tests, 4 sources, 766 macros
[0m05:34:57.907636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550E3D8680>]}
[0m05:34:57.910682 [info ] [MainThread]: 
[0m05:34:57.911294 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:34:57.912568 [info ] [MainThread]: 
[0m05:34:57.913309 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:34:57.919562 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m05:34:58.025740 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m05:34:58.026324 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m05:34:58.026924 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:34:58.062061 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.035 seconds
[0m05:34:58.064238 [debug] [ThreadPool]: On list_datamart: Close
[0m05:34:58.067881 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m05:34:58.077427 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:34:58.078027 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m05:34:58.078614 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:34:58.098503 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m05:34:58.099031 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:34:58.099568 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:34:58.103965 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.004 seconds
[0m05:34:58.105625 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m05:34:58.106812 [debug] [ThreadPool]: On list_datamart_public: Close
[0m05:34:58.114498 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.115128 [debug] [MainThread]: On master: BEGIN
[0m05:34:58.115657 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:34:58.154914 [debug] [MainThread]: SQL status: BEGIN in 0.039 seconds
[0m05:34:58.155492 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.156017 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m05:34:58.162796 [debug] [MainThread]: SQL status: SELECT 0 in 0.006 seconds
[0m05:34:58.164469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550E2A1B80>]}
[0m05:34:58.165666 [debug] [MainThread]: On master: ROLLBACK
[0m05:34:58.166217 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.167403 [debug] [MainThread]: On master: BEGIN
[0m05:34:58.169753 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m05:34:58.170423 [debug] [MainThread]: On master: COMMIT
[0m05:34:58.171062 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.171672 [debug] [MainThread]: On master: COMMIT
[0m05:34:58.172884 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:34:58.172884 [debug] [MainThread]: On master: Close
[0m05:34:58.180567 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m05:34:58.181206 [info ] [Thread-1 (]: 1 of 6 START sql view model public.stg_payments ................................ [RUN]
[0m05:34:58.182271 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.stg_payments'
[0m05:34:58.182873 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m05:34:58.193540 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m05:34:58.195194 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m05:34:58.237307 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m05:34:58.240190 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:34:58.241448 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m05:34:58.242564 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:34:58.260615 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m05:34:58.261305 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:34:58.262529 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql

-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        payment_status -- This is the 'payment_status' for the transaction itself
    FROM
        "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(payment_status AS VARCHAR) AS status -- Renaming to 'status' for consistency if needed later
FROM
    source_payments
WHERE
    payment_id IS NOT NULL
  );
[0m05:34:58.264187 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "payment_status" does not exist
LINE 16:         payment_status -- This is the 'payment_status' for t...
                 ^

[0m05:34:58.264911 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: ROLLBACK
[0m05:34:58.266321 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m05:34:58.272619 [debug] [Thread-1 (]: Database Error in model stg_payments (../../models\staging\stg_payments.sql)
  column "payment_status" does not exist
  LINE 16:         payment_status -- This is the 'payment_status' for t...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_payments.sql
[0m05:34:58.274959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550E2A3D70>]}
[0m05:34:58.275528 [error] [Thread-1 (]: 1 of 6 ERROR creating sql view model public.stg_payments ....................... [[31mERROR[0m in 0.09s]
[0m05:34:58.276659 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m05:34:58.277279 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m05:34:58.278418 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.stg_payments' to be skipped because of status 'error'.  Reason: Database Error in model stg_payments (../../models\staging\stg_payments.sql)
  column "payment_status" does not exist
  LINE 16:         payment_status -- This is the 'payment_status' for t...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_payments.sql.
[0m05:34:58.279012 [info ] [Thread-1 (]: 2 of 6 START sql view model public.stg_products ................................ [RUN]
[0m05:34:58.280341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m05:34:58.281271 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m05:34:58.285198 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m05:34:58.287059 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m05:34:58.291273 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m05:34:58.292412 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:34:58.293595 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m05:34:58.294195 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:34:58.317013 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:34:58.317607 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:34:58.318197 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name AS name, -- Renaming to match schema.yml 'name'
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(name AS VARCHAR) AS name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m05:34:58.322008 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m05:34:58.331064 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:34:58.331660 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m05:34:58.334017 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:34:58.350994 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:34:58.351469 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:34:58.352051 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:34:58.361041 [debug] [Thread-1 (]: SQL status: COMMIT in 0.009 seconds
[0m05:34:58.370089 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m05:34:58.376806 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:34:58.377409 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m05:34:58.379239 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:34:58.382203 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m05:34:58.383563 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015509D1A570>]}
[0m05:34:58.384232 [info ] [Thread-1 (]: 2 of 6 OK created sql view model public.stg_products ........................... [[32mCREATE VIEW[0m in 0.10s]
[0m05:34:58.385858 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m05:34:58.386951 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m05:34:58.387544 [info ] [Thread-1 (]: 3 of 6 START sql view model public.stg_raw_data ................................ [RUN]
[0m05:34:58.388195 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m05:34:58.388793 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m05:34:58.392330 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m05:34:58.393537 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m05:34:58.397844 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m05:34:58.399385 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:34:58.399989 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m05:34:58.400581 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:34:58.424854 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m05:34:58.426882 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:34:58.428032 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m05:34:58.432686 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m05:34:58.438655 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:34:58.439268 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m05:34:58.440970 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:34:58.443276 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m05:34:58.443856 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:34:58.444451 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m05:34:58.448030 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:34:58.452599 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m05:34:58.453489 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:34:58.454067 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m05:34:58.455772 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:34:58.457550 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m05:34:58.458735 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550ED7FB90>]}
[0m05:34:58.459862 [info ] [Thread-1 (]: 3 of 6 OK created sql view model public.stg_raw_data ........................... [[32mCREATE VIEW[0m in 0.07s]
[0m05:34:58.461012 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m05:34:58.462348 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers
[0m05:34:58.462964 [info ] [Thread-1 (]: 4 of 6 START sql view model public.stg_customers ............................... [RUN]
[0m05:34:58.463651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.stg_customers)
[0m05:34:58.464242 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers
[0m05:34:58.468177 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers"
[0m05:34:58.469709 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers
[0m05:34:58.475535 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers"
[0m05:34:58.477717 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:34:58.478835 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: BEGIN
[0m05:34:58.479324 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:34:58.501283 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:34:58.501880 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:34:58.502458 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */

  create view "datamart"."public"."stg_customers__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m05:34:58.507327 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:34:58.512448 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:34:58.512995 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
alter table "datamart"."public"."stg_customers__dbt_tmp" rename to "stg_customers"
[0m05:34:58.514767 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:34:58.516538 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m05:34:58.517141 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:34:58.517141 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m05:34:58.520069 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m05:34:58.523522 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers__dbt_backup"
[0m05:34:58.524620 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:34:58.525167 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
drop view if exists "datamart"."public"."stg_customers__dbt_backup" cascade
[0m05:34:58.526259 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:34:58.528151 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: Close
[0m05:34:58.529261 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '783e8512-ad41-4e5e-84be-e8aeb6be2a5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550ED43140>]}
[0m05:34:58.530475 [info ] [Thread-1 (]: 4 of 6 OK created sql view model public.stg_customers .......................... [[32mCREATE VIEW[0m in 0.07s]
[0m05:34:58.531624 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers
[0m05:34:58.532212 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m05:34:58.532803 [info ] [Thread-1 (]: 5 of 6 SKIP relation public.stg_orders ......................................... [[33mSKIP[0m]
[0m05:34:58.533430 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m05:34:58.534781 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m05:34:58.535299 [info ] [Thread-1 (]: 6 of 6 SKIP relation public.raw_to_normalized .................................. [[33mSKIP[0m]
[0m05:34:58.535906 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m05:34:58.537969 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.537969 [debug] [MainThread]: On master: BEGIN
[0m05:34:58.538558 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:34:58.560922 [debug] [MainThread]: SQL status: BEGIN in 0.022 seconds
[0m05:34:58.562308 [debug] [MainThread]: On master: COMMIT
[0m05:34:58.562308 [debug] [MainThread]: Using postgres connection "master"
[0m05:34:58.562912 [debug] [MainThread]: On master: COMMIT
[0m05:34:58.564119 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:34:58.564719 [debug] [MainThread]: On master: Close
[0m05:34:58.565338 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:34:58.565932 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m05:34:58.566542 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m05:34:58.566542 [debug] [MainThread]: Connection 'model.data_pipeline.stg_customers' was properly closed.
[0m05:34:58.567281 [info ] [MainThread]: 
[0m05:34:58.568193 [info ] [MainThread]: Finished running 6 view models in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m05:34:58.570308 [debug] [MainThread]: Command end result
[0m05:34:58.678584 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:34:58.682043 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:34:58.689952 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m05:34:58.690538 [info ] [MainThread]: 
[0m05:34:58.691207 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m05:34:58.692543 [info ] [MainThread]: 
[0m05:34:58.693135 [error] [MainThread]:   Database Error in model stg_payments (../../models\staging\stg_payments.sql)
  column "payment_status" does not exist
  LINE 16:         payment_status -- This is the 'payment_status' for t...
                   ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_payments.sql
[0m05:34:58.693844 [info ] [MainThread]: 
[0m05:34:58.694579 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=2 TOTAL=6
[0m05:34:58.695946 [debug] [MainThread]: Command `dbt run` failed at 05:34:58.695946 after 2.42 seconds
[0m05:34:58.696540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550C388470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550C46DAC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001550ED5E360>]}
[0m05:34:58.697220 [debug] [MainThread]: Flushing usage events
[0m05:34:59.128894 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m05:43:25.607069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BA878380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BA87BF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BA87ABD0>]}


============================== 05:43:25.613371 | 4e664549-94e8-409c-82b5-678a85fda407 ==============================
[0m05:43:25.613371 [info ] [MainThread]: Running with dbt=1.9.6
[0m05:43:25.615265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m05:43:25.870118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BA72F230>]}
[0m05:43:25.958379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BADBFFB0>]}
[0m05:43:25.959726 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m05:43:26.339935 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m05:43:26.572543 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:43:26.574559 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_payments.sql
[0m05:43:26.918698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC9D7440>]}
[0m05:43:27.035449 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:43:27.099065 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:43:27.136837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC5C1880>]}
[0m05:43:27.138028 [info ] [MainThread]: Found 6 models, 20 data tests, 4 sources, 766 macros
[0m05:43:27.139150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC82AAB0>]}
[0m05:43:27.143148 [info ] [MainThread]: 
[0m05:43:27.143908 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m05:43:27.145632 [info ] [MainThread]: 
[0m05:43:27.146703 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m05:43:27.152720 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m05:43:27.270892 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m05:43:27.271515 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m05:43:27.272135 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:43:27.302006 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.030 seconds
[0m05:43:27.304838 [debug] [ThreadPool]: On list_datamart: Close
[0m05:43:27.308759 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m05:43:27.317607 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:43:27.318291 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m05:43:27.319050 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m05:43:27.341596 [debug] [ThreadPool]: SQL status: BEGIN in 0.023 seconds
[0m05:43:27.342481 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m05:43:27.342959 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m05:43:27.347834 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.005 seconds
[0m05:43:27.349983 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m05:43:27.351949 [debug] [ThreadPool]: On list_datamart_public: Close
[0m05:43:27.361085 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:27.361655 [debug] [MainThread]: On master: BEGIN
[0m05:43:27.362223 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:43:27.400805 [debug] [MainThread]: SQL status: BEGIN in 0.039 seconds
[0m05:43:27.401481 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:27.401990 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m05:43:27.410507 [debug] [MainThread]: SQL status: SELECT 4 in 0.007 seconds
[0m05:43:27.412741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC9C2420>]}
[0m05:43:27.413257 [debug] [MainThread]: On master: ROLLBACK
[0m05:43:27.415378 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:27.416089 [debug] [MainThread]: On master: BEGIN
[0m05:43:27.419293 [debug] [MainThread]: SQL status: BEGIN in 0.003 seconds
[0m05:43:27.419847 [debug] [MainThread]: On master: COMMIT
[0m05:43:27.420391 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:27.420391 [debug] [MainThread]: On master: COMMIT
[0m05:43:27.422636 [debug] [MainThread]: SQL status: COMMIT in 0.002 seconds
[0m05:43:27.423382 [debug] [MainThread]: On master: Close
[0m05:43:27.433355 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m05:43:27.433863 [info ] [Thread-1 (]: 1 of 6 START sql view model public.stg_payments ................................ [RUN]
[0m05:43:27.435274 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.stg_payments'
[0m05:43:27.436341 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m05:43:27.447861 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m05:43:27.449623 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m05:43:27.492296 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m05:43:27.493771 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:43:27.494532 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m05:43:27.494532 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m05:43:27.514487 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m05:43:27.515100 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:43:27.515763 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m05:43:27.519589 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m05:43:27.529375 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:43:27.530091 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m05:43:27.532806 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.548160 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m05:43:27.548840 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:43:27.549543 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m05:43:27.552997 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:43:27.562630 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m05:43:27.569349 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m05:43:27.569919 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m05:43:27.571911 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:43:27.576276 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m05:43:27.579529 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192B7E0A720>]}
[0m05:43:27.580267 [info ] [Thread-1 (]: 1 of 6 OK created sql view model public.stg_payments ........................... [[32mCREATE VIEW[0m in 0.14s]
[0m05:43:27.581792 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m05:43:27.583033 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m05:43:27.583631 [info ] [Thread-1 (]: 2 of 6 START sql view model public.stg_products ................................ [RUN]
[0m05:43:27.584233 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m05:43:27.584928 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m05:43:27.668283 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m05:43:27.669640 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m05:43:27.675507 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m05:43:27.677275 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.677879 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m05:43:27.678589 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:43:27.703229 [debug] [Thread-1 (]: SQL status: BEGIN in 0.025 seconds
[0m05:43:27.703865 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.704575 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name AS name, -- Renaming to match schema.yml 'name'
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(name AS VARCHAR) AS name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m05:43:27.710144 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.005 seconds
[0m05:43:27.715426 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.715892 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products" rename to "stg_products__dbt_backup"
[0m05:43:27.718826 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.722761 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.723333 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m05:43:27.725313 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m05:43:27.730922 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:43:27.731718 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.732515 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m05:43:27.735884 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:43:27.740599 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m05:43:27.741898 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m05:43:27.742524 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m05:43:27.747365 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.004 seconds
[0m05:43:27.749899 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m05:43:27.751084 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BCB6DC10>]}
[0m05:43:27.752786 [info ] [Thread-1 (]: 2 of 6 OK created sql view model public.stg_products ........................... [[32mCREATE VIEW[0m in 0.17s]
[0m05:43:27.754184 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m05:43:27.754934 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m05:43:27.756433 [info ] [Thread-1 (]: 3 of 6 START sql view model public.stg_raw_data ................................ [RUN]
[0m05:43:27.757307 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m05:43:27.758067 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m05:43:27.762920 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m05:43:27.764112 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m05:43:27.769226 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m05:43:27.769770 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.771302 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m05:43:27.771850 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:43:27.805190 [debug] [Thread-1 (]: SQL status: BEGIN in 0.034 seconds
[0m05:43:27.806473 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.807215 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m05:43:27.812074 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:43:27.819473 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.820480 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m05:43:27.822533 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.827243 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.827243 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m05:43:27.829571 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.832515 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m05:43:27.833199 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.833199 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m05:43:27.837335 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:43:27.841021 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m05:43:27.842480 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m05:43:27.843219 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m05:43:27.848122 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.005 seconds
[0m05:43:27.850098 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m05:43:27.852117 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BCEF1E20>]}
[0m05:43:27.853216 [info ] [Thread-1 (]: 3 of 6 OK created sql view model public.stg_raw_data ........................... [[32mCREATE VIEW[0m in 0.09s]
[0m05:43:27.854214 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m05:43:27.855951 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers
[0m05:43:27.856711 [info ] [Thread-1 (]: 4 of 6 START sql view model public.stg_customers ............................... [RUN]
[0m05:43:27.858135 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.stg_customers)
[0m05:43:27.858135 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers
[0m05:43:27.863856 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers"
[0m05:43:27.865377 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers
[0m05:43:27.869875 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers"
[0m05:43:27.871939 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:43:27.872270 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: BEGIN
[0m05:43:27.873172 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:43:27.895314 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:43:27.896092 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:43:27.896599 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */

  create view "datamart"."public"."stg_customers__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m05:43:27.900526 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:43:27.905140 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:43:27.905757 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
alter table "datamart"."public"."stg_customers__dbt_tmp" rename to "stg_customers"
[0m05:43:27.907474 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.909579 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m05:43:27.909579 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:43:27.910325 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: COMMIT
[0m05:43:27.913267 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m05:43:27.916525 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers__dbt_backup"
[0m05:43:27.917205 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers"
[0m05:43:27.917205 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers"} */
drop view if exists "datamart"."public"."stg_customers__dbt_backup" cascade
[0m05:43:27.919119 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:43:27.921145 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers: Close
[0m05:43:27.922351 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC945F70>]}
[0m05:43:27.923249 [info ] [Thread-1 (]: 4 of 6 OK created sql view model public.stg_customers .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m05:43:27.925062 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers
[0m05:43:27.925725 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m05:43:27.926265 [info ] [Thread-1 (]: 5 of 6 START sql view model public.stg_orders .................................. [RUN]
[0m05:43:27.927469 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers, now model.data_pipeline.stg_orders)
[0m05:43:27.927469 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m05:43:27.931775 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m05:43:27.932975 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m05:43:27.940389 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m05:43:27.941829 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m05:43:27.942588 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m05:43:27.943727 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:43:27.966924 [debug] [Thread-1 (]: SQL status: BEGIN in 0.023 seconds
[0m05:43:27.967478 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m05:43:27.968083 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m05:43:27.972826 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m05:43:27.978021 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m05:43:27.978727 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m05:43:27.980903 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m05:43:27.982619 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m05:43:27.983151 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m05:43:27.983684 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m05:43:27.987492 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m05:43:27.991376 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m05:43:27.991960 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m05:43:27.992541 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m05:43:27.994293 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m05:43:27.996222 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m05:43:27.997396 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BCB76600>]}
[0m05:43:27.998691 [info ] [Thread-1 (]: 5 of 6 OK created sql view model public.stg_orders ............................. [[32mCREATE VIEW[0m in 0.07s]
[0m05:43:28.000047 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m05:43:28.001243 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m05:43:28.001869 [info ] [Thread-1 (]: 6 of 6 START sql view model public.raw_to_normalized ........................... [RUN]
[0m05:43:28.002455 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.raw_to_normalized)
[0m05:43:28.003383 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.raw_to_normalized
[0m05:43:28.009511 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.raw_to_normalized"
[0m05:43:28.011158 [debug] [Thread-1 (]: Began executing node model.data_pipeline.raw_to_normalized
[0m05:43:28.015210 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.raw_to_normalized"
[0m05:43:28.016970 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m05:43:28.017510 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: BEGIN
[0m05:43:28.018047 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m05:43:28.040635 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m05:43:28.041241 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m05:43:28.041784 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */

  create view "datamart"."public"."raw_to_normalized__dbt_tmp"
    
    
  as (
    -- models/raw_to_normalized.sql

-- This model combines data from the staged customer, order, product, and payment tables
-- to create a comprehensive, normalized view of the e-commerce data.
-- It serves as the primary data source for analytics and reporting.
-- models/raw_to_normalized.sql

WITH customers AS (
    SELECT * FROM "datamart"."public"."stg_customers"
),
orders AS (
    SELECT * FROM "datamart"."public"."stg_orders"
),
products AS (
    SELECT * FROM "datamart"."public"."stg_products"
),
payments AS (
    SELECT * FROM "datamart"."public"."stg_payments"
)

SELECT
    o.order_id,
    o.order_date,
    o.total_amount,
    o.status AS order_status,
    o.payment_method,
    o.shipping_address,

    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    c.phone,
    c.address AS customer_address,
    c.created_at AS customer_created_at,

    srd.product_id,
    p.product_name,
    p.category AS product_category,
    p.price AS product_price,

    pay.payment_id,
    pay.transaction_payment_status AS payment_transaction_status

FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
LEFT JOIN "datamart"."public"."stg_raw_data" srd ON o.order_id = srd.order_id
LEFT JOIN products p ON srd.product_id = p.product_id
LEFT JOIN payments pay ON o.order_id = pay.order_id
  );
[0m05:43:28.044961 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column p.product_name does not exist
LINE 44:     p.product_name,
             ^

[0m05:43:28.044961 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: ROLLBACK
[0m05:43:28.046900 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: Close
[0m05:43:28.054254 [debug] [Thread-1 (]: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.product_name does not exist
  LINE 44:     p.product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m05:43:28.055389 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e664549-94e8-409c-82b5-678a85fda407', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BC55FAD0>]}
[0m05:43:28.056704 [error] [Thread-1 (]: 6 of 6 ERROR creating sql view model public.raw_to_normalized .................. [[31mERROR[0m in 0.05s]
[0m05:43:28.058613 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m05:43:28.059241 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.raw_to_normalized' to be skipped because of status 'error'.  Reason: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.product_name does not exist
  LINE 44:     p.product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql.
[0m05:43:28.062396 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:28.062396 [debug] [MainThread]: On master: BEGIN
[0m05:43:28.063165 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m05:43:28.084505 [debug] [MainThread]: SQL status: BEGIN in 0.021 seconds
[0m05:43:28.085220 [debug] [MainThread]: On master: COMMIT
[0m05:43:28.085935 [debug] [MainThread]: Using postgres connection "master"
[0m05:43:28.085935 [debug] [MainThread]: On master: COMMIT
[0m05:43:28.087461 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m05:43:28.088073 [debug] [MainThread]: On master: Close
[0m05:43:28.089551 [debug] [MainThread]: Connection 'master' was properly closed.
[0m05:43:28.090132 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m05:43:28.090715 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m05:43:28.091362 [debug] [MainThread]: Connection 'model.data_pipeline.raw_to_normalized' was properly closed.
[0m05:43:28.091936 [info ] [MainThread]: 
[0m05:43:28.093108 [info ] [MainThread]: Finished running 6 view models in 0 hours 0 minutes and 0.95 seconds (0.95s).
[0m05:43:28.094758 [debug] [MainThread]: Command end result
[0m05:43:28.135419 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m05:43:28.139986 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m05:43:28.148565 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m05:43:28.149689 [info ] [MainThread]: 
[0m05:43:28.150781 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m05:43:28.152474 [info ] [MainThread]: 
[0m05:43:28.153586 [error] [MainThread]:   Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.product_name does not exist
  LINE 44:     p.product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m05:43:28.154368 [info ] [MainThread]: 
[0m05:43:28.156274 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m05:43:28.158861 [debug] [MainThread]: Command `dbt run` failed at 05:43:28.158861 after 2.97 seconds
[0m05:43:28.159421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BA7AACC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BAB28C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000192BAB288C0>]}
[0m05:43:28.160519 [debug] [MainThread]: Flushing usage events
[0m05:43:28.628102 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:43:02.024379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E7D32E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E5842570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E8531CA0>]}


============================== 12:43:02.034131 | 76338005-6c36-4fd0-b04d-579ed2b5d1bd ==============================
[0m12:43:02.034131 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:43:02.035637 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m12:43:02.260937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '76338005-6c36-4fd0-b04d-579ed2b5d1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E884B1A0>]}
[0m12:43:02.316756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '76338005-6c36-4fd0-b04d-579ed2b5d1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E8862840>]}
[0m12:43:02.320326 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:43:02.547621 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:43:02.741814 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 3 files changed.
[0m12:43:02.741814 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\reviews.sql
[0m12:43:02.742378 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\customers.sql
[0m12:43:02.742378 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\orders.sql
[0m12:43:02.742929 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\products.sql
[0m12:43:02.743482 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\schema.yml
[0m12:43:02.743482 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\raw_to_normalized.sql
[0m12:43:02.743482 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_products.sql
[0m12:43:03.061883 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.data_pipeline.customers' (../../models\marts\customers.sql) depends on a node named 'stg_customers_source' which was not found
[0m12:43:03.062943 [debug] [MainThread]: Command `dbt run` failed at 12:43:03.062943 after 1.24 seconds
[0m12:43:03.063529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5E7D32E70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5EA448BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5EA6A3B30>]}
[0m12:43:03.063529 [debug] [MainThread]: Flushing usage events
[0m12:43:03.492980 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:44:18.332934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487269A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048726A030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048726AA50>]}


============================== 12:44:18.336165 | 2be74d70-5c82-4787-823e-8ab2e0d61afd ==============================
[0m12:44:18.336165 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:44:18.336709 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:44:18.488760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487526090>]}
[0m12:44:18.533692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487094EC0>]}
[0m12:44:18.534749 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:44:18.826498 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:44:18.991403 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 5 files added, 3 files changed.
[0m12:44:18.991403 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\products.sql
[0m12:44:18.991951 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\reviews.sql
[0m12:44:18.991951 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\customers.sql
[0m12:44:18.992522 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\staging\stg_customers_source.sql
[0m12:44:18.992522 [debug] [MainThread]: Partial parsing: added file: data_pipeline://../../models\marts\orders.sql
[0m12:44:18.993081 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\schema.yml
[0m12:44:18.993637 [debug] [MainThread]: Partial parsing: deleted file: data_pipeline://../../models\staging\stg_customers.sql
[0m12:44:18.993637 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_products.sql
[0m12:44:18.994198 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\raw_to_normalized.sql
[0m12:44:19.329121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048939CAA0>]}
[0m12:44:19.406275 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:44:19.465214 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:44:19.590054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204894D3770>]}
[0m12:44:19.590610 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:44:19.591173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048958DEE0>]}
[0m12:44:19.592633 [info ] [MainThread]: 
[0m12:44:19.593172 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:44:19.593725 [info ] [MainThread]: 
[0m12:44:19.594352 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:44:19.598737 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m12:44:19.687382 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m12:44:19.687987 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m12:44:19.687987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:19.716780 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.029 seconds
[0m12:44:19.718392 [debug] [ThreadPool]: On list_datamart: Close
[0m12:44:19.722050 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:44:19.729111 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:44:19.729111 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:44:19.729717 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:19.768631 [debug] [ThreadPool]: SQL status: BEGIN in 0.040 seconds
[0m12:44:19.770538 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:44:19.771116 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:44:19.787662 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.015 seconds
[0m12:44:19.790501 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:44:19.791717 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:44:19.797766 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:19.797766 [debug] [MainThread]: On master: BEGIN
[0m12:44:19.798775 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:44:19.850027 [debug] [MainThread]: SQL status: BEGIN in 0.051 seconds
[0m12:44:19.850549 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:19.850549 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:44:19.858627 [debug] [MainThread]: SQL status: SELECT 7 in 0.008 seconds
[0m12:44:19.860726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487294B90>]}
[0m12:44:19.861251 [debug] [MainThread]: On master: ROLLBACK
[0m12:44:19.862292 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:19.862803 [debug] [MainThread]: On master: BEGIN
[0m12:44:19.863854 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:44:19.863854 [debug] [MainThread]: On master: COMMIT
[0m12:44:19.864374 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:19.864374 [debug] [MainThread]: On master: COMMIT
[0m12:44:19.864902 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:44:19.865427 [debug] [MainThread]: On master: Close
[0m12:44:19.873843 [debug] [Thread-1 (]: Began running node model.data_pipeline.reviews
[0m12:44:19.874392 [info ] [Thread-1 (]: 1 of 10 START sql view model public.reviews .................................... [RUN]
[0m12:44:19.875011 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.reviews'
[0m12:44:19.875011 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.reviews
[0m12:44:19.879514 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.reviews"
[0m12:44:19.880625 [debug] [Thread-1 (]: Began executing node model.data_pipeline.reviews
[0m12:44:19.917400 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.reviews"
[0m12:44:19.918497 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:44:19.918497 [debug] [Thread-1 (]: On model.data_pipeline.reviews: BEGIN
[0m12:44:19.919387 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:44:19.932294 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m12:44:19.932294 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:44:19.932856 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */

  create view "datamart"."public"."reviews__dbt_tmp"
    
    
  as (
    -- models/marts/reviews.sql
-- Create an empty or placeholder model for reviews if not yet ingested
SELECT NULL::INTEGER AS review_id,
       NULL::INTEGER AS product_id,
       NULL::INTEGER AS customer_id,
       NULL::INTEGER AS rating,
       NULL::TEXT AS review_text,
       NULL::TIMESTAMP AS review_date
WHERE 1=0
  );
[0m12:44:19.941871 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.009 seconds
[0m12:44:19.952074 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:44:19.952674 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews__dbt_tmp" rename to "reviews"
[0m12:44:19.956328 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:44:19.969347 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:44:19.969347 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:44:19.969927 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:44:19.972172 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:44:19.976252 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."reviews__dbt_backup"
[0m12:44:19.979768 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:44:19.980326 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
drop view if exists "datamart"."public"."reviews__dbt_backup" cascade
[0m12:44:19.981506 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:44:19.983195 [debug] [Thread-1 (]: On model.data_pipeline.reviews: Close
[0m12:44:19.988393 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048477E450>]}
[0m12:44:19.988979 [info ] [Thread-1 (]: 1 of 10 OK created sql view model public.reviews ............................... [[32mCREATE VIEW[0m in 0.11s]
[0m12:44:19.989685 [debug] [Thread-1 (]: Finished running node model.data_pipeline.reviews
[0m12:44:19.990873 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m12:44:19.991989 [info ] [Thread-1 (]: 2 of 10 START sql view model public.stg_payments ............................... [RUN]
[0m12:44:19.992552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.reviews, now model.data_pipeline.stg_payments)
[0m12:44:19.993206 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m12:44:19.998256 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m12:44:20.000468 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m12:44:20.004386 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m12:44:20.005738 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.005738 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m12:44:20.006428 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.021669 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:44:20.022430 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.023000 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m12:44:20.026478 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:44:20.029242 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.029242 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m12:44:20.030966 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:44:20.033192 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.033192 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m12:44:20.034300 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:44:20.037223 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:44:20.037785 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.037785 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:44:20.040004 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:44:20.041725 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m12:44:20.042312 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:44:20.042869 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m12:44:20.049348 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.007 seconds
[0m12:44:20.050496 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m12:44:20.051602 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020489274B60>]}
[0m12:44:20.052611 [info ] [Thread-1 (]: 2 of 10 OK created sql view model public.stg_payments .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:44:20.059132 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m12:44:20.059132 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m12:44:20.059797 [info ] [Thread-1 (]: 3 of 10 START sql view model public.stg_products ............................... [RUN]
[0m12:44:20.061647 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m12:44:20.061647 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m12:44:20.063961 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m12:44:20.065099 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m12:44:20.067345 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m12:44:20.067914 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:44:20.068461 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m12:44:20.069020 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.083518 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:44:20.084073 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:44:20.084073 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(product_name AS VARCHAR) AS product_name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m12:44:20.086653 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "name" does not exist
LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                         ^

[0m12:44:20.087301 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: ROLLBACK
[0m12:44:20.088303 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m12:44:20.113801 [debug] [Thread-1 (]: Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql
[0m12:44:20.114372 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020489854950>]}
[0m12:44:20.114949 [error] [Thread-1 (]: 3 of 10 ERROR creating sql view model public.stg_products ...................... [[31mERROR[0m in 0.05s]
[0m12:44:20.115502 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m12:44:20.116043 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m12:44:20.116043 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.stg_products' to be skipped because of status 'error'.  Reason: Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql.
[0m12:44:20.116604 [info ] [Thread-1 (]: 4 of 10 START sql view model public.stg_raw_data ............................... [RUN]
[0m12:44:20.117761 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m12:44:20.117761 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m12:44:20.120851 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m12:44:20.121437 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m12:44:20.124911 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m12:44:20.126027 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.126581 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m12:44:20.127130 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.140733 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:44:20.141279 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.141862 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m12:44:20.145774 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:44:20.150412 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.150950 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m12:44:20.153338 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:44:20.156802 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.157381 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m12:44:20.158524 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:44:20.159670 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:44:20.160229 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.160229 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:44:20.161991 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:44:20.163681 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m12:44:20.164257 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:44:20.164818 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m12:44:20.168258 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:44:20.170536 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m12:44:20.171667 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020489406E10>]}
[0m12:44:20.172949 [info ] [Thread-1 (]: 4 of 10 OK created sql view model public.stg_raw_data .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:44:20.174097 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m12:44:20.174690 [debug] [Thread-1 (]: Began running node model.data_pipeline.products
[0m12:44:20.175273 [info ] [Thread-1 (]: 5 of 10 SKIP relation public.products .......................................... [[33mSKIP[0m]
[0m12:44:20.176426 [debug] [Thread-1 (]: Finished running node model.data_pipeline.products
[0m12:44:20.176969 [debug] [Thread-1 (]: Began running node model.data_pipeline.orders
[0m12:44:20.177943 [info ] [Thread-1 (]: 6 of 10 START sql view model public.orders ..................................... [RUN]
[0m12:44:20.178500 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.orders)
[0m12:44:20.178500 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.orders
[0m12:44:20.180781 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.orders"
[0m12:44:20.181887 [debug] [Thread-1 (]: Began executing node model.data_pipeline.orders
[0m12:44:20.184117 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.orders"
[0m12:44:20.185235 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:44:20.185235 [debug] [Thread-1 (]: On model.data_pipeline.orders: BEGIN
[0m12:44:20.185803 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.202572 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:44:20.203800 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:44:20.204388 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */

  create view "datamart"."public"."orders__dbt_tmp"
    
    
  as (
    -- models/marts/orders.sql
WITH orders_raw AS (
    SELECT DISTINCT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_payment_status AS status
    FROM "datamart"."public"."stg_raw_data"
),
orders_payment AS (
    SELECT order_id, MIN(payment_method) AS payment_method
    FROM "datamart"."public"."stg_payments"
    GROUP BY order_id
)
SELECT
    o.order_id,
    o.customer_id,
    o.order_date,
    o.status,
    o.total_amount,
    COALESCE(p.payment_method, '') AS payment_method,
    ''::text AS shipping_address  -- Default to empty string as shipping address is not provided
FROM orders_raw o
LEFT JOIN orders_payment p ON o.order_id = p.order_id
  );
[0m12:44:20.213763 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.009 seconds
[0m12:44:20.218270 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:44:20.218829 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
alter table "datamart"."public"."orders__dbt_tmp" rename to "orders"
[0m12:44:20.220645 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:44:20.221940 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:44:20.221940 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:44:20.222502 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:44:20.224202 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:44:20.228455 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."orders__dbt_backup"
[0m12:44:20.229138 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:44:20.229710 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
drop view if exists "datamart"."public"."orders__dbt_backup" cascade
[0m12:44:20.230887 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:44:20.232023 [debug] [Thread-1 (]: On model.data_pipeline.orders: Close
[0m12:44:20.233138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020489945550>]}
[0m12:44:20.234125 [info ] [Thread-1 (]: 6 of 10 OK created sql view model public.orders ................................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:44:20.236320 [debug] [Thread-1 (]: Finished running node model.data_pipeline.orders
[0m12:44:20.236824 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers_source
[0m12:44:20.237374 [info ] [Thread-1 (]: 7 of 10 START sql view model public.stg_customers_source ....................... [RUN]
[0m12:44:20.238523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.orders, now model.data_pipeline.stg_customers_source)
[0m12:44:20.239073 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers_source
[0m12:44:20.242402 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers_source"
[0m12:44:20.243510 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers_source
[0m12:44:20.246603 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers_source"
[0m12:44:20.247147 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:44:20.247710 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: BEGIN
[0m12:44:20.247710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.261597 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:44:20.262231 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:44:20.262819 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */

  create view "datamart"."public"."stg_customers_source__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m12:44:20.272343 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.009 seconds
[0m12:44:20.275882 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:44:20.276451 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
alter table "datamart"."public"."stg_customers_source__dbt_tmp" rename to "stg_customers_source"
[0m12:44:20.279159 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:44:20.281310 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:44:20.281310 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:44:20.281887 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:44:20.283647 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:44:20.285944 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers_source__dbt_backup"
[0m12:44:20.286449 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:44:20.287179 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
drop view if exists "datamart"."public"."stg_customers_source__dbt_backup" cascade
[0m12:44:20.288885 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:44:20.290029 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: Close
[0m12:44:20.290956 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204898923C0>]}
[0m12:44:20.291973 [info ] [Thread-1 (]: 7 of 10 OK created sql view model public.stg_customers_source .................. [[32mCREATE VIEW[0m in 0.05s]
[0m12:44:20.293148 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers_source
[0m12:44:20.293787 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m12:44:20.294573 [info ] [Thread-1 (]: 8 of 10 START sql view model public.stg_orders ................................. [RUN]
[0m12:44:20.295045 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers_source, now model.data_pipeline.stg_orders)
[0m12:44:20.295661 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m12:44:20.299024 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m12:44:20.300094 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m12:44:20.303995 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m12:44:20.305063 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:44:20.305486 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m12:44:20.305486 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.330439 [debug] [Thread-1 (]: SQL status: BEGIN in 0.025 seconds
[0m12:44:20.331015 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:44:20.331597 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m12:44:20.340022 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.008 seconds
[0m12:44:20.344619 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:44:20.344619 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m12:44:20.346461 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:44:20.348143 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:44:20.348707 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:44:20.348707 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:44:20.350282 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:44:20.353097 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m12:44:20.354347 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:44:20.354347 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m12:44:20.355425 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:44:20.356471 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m12:44:20.357309 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020489988080>]}
[0m12:44:20.357815 [info ] [Thread-1 (]: 8 of 10 OK created sql view model public.stg_orders ............................ [[32mCREATE VIEW[0m in 0.06s]
[0m12:44:20.358992 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m12:44:20.358992 [debug] [Thread-1 (]: Began running node model.data_pipeline.customers
[0m12:44:20.359665 [info ] [Thread-1 (]: 9 of 10 START sql view model public.customers .................................. [RUN]
[0m12:44:20.360209 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.customers)
[0m12:44:20.360776 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.customers
[0m12:44:20.363887 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.customers"
[0m12:44:20.364989 [debug] [Thread-1 (]: Began executing node model.data_pipeline.customers
[0m12:44:20.367772 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.customers"
[0m12:44:20.368298 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:44:20.369060 [debug] [Thread-1 (]: On model.data_pipeline.customers: BEGIN
[0m12:44:20.369060 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:44:20.382942 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:44:20.383485 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:44:20.383485 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */

  create view "datamart"."public"."customers__dbt_tmp"
    
    
  as (
    -- models/marts/customers.sql
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    phone,
    address,
    signup_date AS created_at
FROM "datamart"."public"."stg_customers_source"
  );
[0m12:44:20.385262 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "signup_date" does not exist
LINE 15:     signup_date AS created_at
             ^

[0m12:44:20.385813 [debug] [Thread-1 (]: On model.data_pipeline.customers: ROLLBACK
[0m12:44:20.386538 [debug] [Thread-1 (]: On model.data_pipeline.customers: Close
[0m12:44:20.389399 [debug] [Thread-1 (]: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:44:20.389985 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2be74d70-5c82-4787-823e-8ab2e0d61afd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002048994DDF0>]}
[0m12:44:20.391190 [error] [Thread-1 (]: 9 of 10 ERROR creating sql view model public.customers ......................... [[31mERROR[0m in 0.03s]
[0m12:44:20.392267 [debug] [Thread-1 (]: Finished running node model.data_pipeline.customers
[0m12:44:20.392267 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.customers' to be skipped because of status 'error'.  Reason: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql.
[0m12:44:20.393361 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m12:44:20.393947 [info ] [Thread-1 (]: 10 of 10 SKIP relation public.raw_to_normalized ................................ [[33mSKIP[0m]
[0m12:44:20.394750 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m12:44:20.396089 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:20.396633 [debug] [MainThread]: On master: BEGIN
[0m12:44:20.397179 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:44:20.410232 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m12:44:20.410789 [debug] [MainThread]: On master: COMMIT
[0m12:44:20.410789 [debug] [MainThread]: Using postgres connection "master"
[0m12:44:20.411390 [debug] [MainThread]: On master: COMMIT
[0m12:44:20.411966 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:44:20.411966 [debug] [MainThread]: On master: Close
[0m12:44:20.413054 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:44:20.413054 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m12:44:20.413054 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:44:20.413965 [debug] [MainThread]: Connection 'model.data_pipeline.customers' was properly closed.
[0m12:44:20.413965 [info ] [MainThread]: 
[0m12:44:20.414471 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 0.82 seconds (0.82s).
[0m12:44:20.415141 [debug] [MainThread]: Command end result
[0m12:44:20.441684 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:44:20.444457 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:44:20.449141 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:44:20.449707 [info ] [MainThread]: 
[0m12:44:20.450260 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:44:20.450830 [info ] [MainThread]: 
[0m12:44:20.450830 [error] [MainThread]:   Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql
[0m12:44:20.451380 [info ] [MainThread]: 
[0m12:44:20.452247 [error] [MainThread]:   Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:44:20.452571 [info ] [MainThread]: 
[0m12:44:20.453077 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 TOTAL=10
[0m12:44:20.454183 [debug] [MainThread]: Command `dbt run` failed at 12:44:20.454183 after 2.35 seconds
[0m12:44:20.454759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020486B8BEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020486B8A0C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020486B89D90>]}
[0m12:44:20.455914 [debug] [MainThread]: Flushing usage events
[0m12:44:20.913201 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:47:44.994220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB5004860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB8E5A7E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB8E5A690>]}


============================== 12:47:44.998252 | c0e901b1-ca0c-493d-b176-28b8f925d21b ==============================
[0m12:47:44.998252 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:47:44.998781 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:47:45.135893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB9285DC0>]}
[0m12:47:45.180293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB87DA2A0>]}
[0m12:47:45.180867 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:47:45.343174 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:47:45.492644 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:47:45.493764 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\schema.yml
[0m12:47:45.493764 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\marts\products.sql
[0m12:47:45.789783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBAFEE480>]}
[0m12:47:45.859075 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:47:45.886908 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:47:45.913232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB1197C0>]}
[0m12:47:45.913824 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:47:45.914343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBAD0B890>]}
[0m12:47:45.916102 [info ] [MainThread]: 
[0m12:47:45.916657 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:47:45.916657 [info ] [MainThread]: 
[0m12:47:45.917203 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:47:45.921167 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m12:47:46.039145 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m12:47:46.039731 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m12:47:46.040301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:46.064703 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.024 seconds
[0m12:47:46.065810 [debug] [ThreadPool]: On list_datamart: Close
[0m12:47:46.068628 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:47:46.074009 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:47:46.074009 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:47:46.074583 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:46.085868 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m12:47:46.086458 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:47:46.086458 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:47:46.092263 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.005 seconds
[0m12:47:46.094504 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:47:46.095659 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:47:46.100957 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.101510 [debug] [MainThread]: On master: BEGIN
[0m12:47:46.101510 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:47:46.120580 [debug] [MainThread]: SQL status: BEGIN in 0.019 seconds
[0m12:47:46.121749 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.122332 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:47:46.132123 [debug] [MainThread]: SQL status: SELECT 9 in 0.009 seconds
[0m12:47:46.133948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB4B0530>]}
[0m12:47:46.134519 [debug] [MainThread]: On master: ROLLBACK
[0m12:47:46.134519 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.135831 [debug] [MainThread]: On master: BEGIN
[0m12:47:46.136938 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:47:46.137503 [debug] [MainThread]: On master: COMMIT
[0m12:47:46.137503 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.137503 [debug] [MainThread]: On master: COMMIT
[0m12:47:46.138663 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:47:46.138663 [debug] [MainThread]: On master: Close
[0m12:47:46.146989 [debug] [Thread-1 (]: Began running node model.data_pipeline.reviews
[0m12:47:46.147543 [info ] [Thread-1 (]: 1 of 10 START sql view model public.reviews .................................... [RUN]
[0m12:47:46.148128 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.reviews'
[0m12:47:46.148707 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.reviews
[0m12:47:46.155436 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.reviews"
[0m12:47:46.156598 [debug] [Thread-1 (]: Began executing node model.data_pipeline.reviews
[0m12:47:46.184620 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.reviews"
[0m12:47:46.185847 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.186397 [debug] [Thread-1 (]: On model.data_pipeline.reviews: BEGIN
[0m12:47:46.186397 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:47:46.197882 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m12:47:46.198498 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.198498 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */

  create view "datamart"."public"."reviews__dbt_tmp"
    
    
  as (
    -- models/marts/reviews.sql
-- Create an empty or placeholder model for reviews if not yet ingested
SELECT NULL::INTEGER AS review_id,
       NULL::INTEGER AS product_id,
       NULL::INTEGER AS customer_id,
       NULL::INTEGER AS rating,
       NULL::TEXT AS review_text,
       NULL::TIMESTAMP AS review_date
WHERE 1=0
  );
[0m12:47:46.203001 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:47:46.207749 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.208288 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews" rename to "reviews__dbt_backup"
[0m12:47:46.209398 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.216295 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.216857 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews__dbt_tmp" rename to "reviews"
[0m12:47:46.218016 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.229832 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:47:46.230396 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.230396 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:47:46.233236 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:47:46.237936 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."reviews__dbt_backup"
[0m12:47:46.244131 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:47:46.244730 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
drop view if exists "datamart"."public"."reviews__dbt_backup" cascade
[0m12:47:46.248765 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.004 seconds
[0m12:47:46.251650 [debug] [Thread-1 (]: On model.data_pipeline.reviews: Close
[0m12:47:46.254561 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBA828C20>]}
[0m12:47:46.255716 [info ] [Thread-1 (]: 1 of 10 OK created sql view model public.reviews ............................... [[32mCREATE VIEW[0m in 0.10s]
[0m12:47:46.256891 [debug] [Thread-1 (]: Finished running node model.data_pipeline.reviews
[0m12:47:46.257441 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m12:47:46.258023 [info ] [Thread-1 (]: 2 of 10 START sql view model public.stg_payments ............................... [RUN]
[0m12:47:46.258580 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.reviews, now model.data_pipeline.stg_payments)
[0m12:47:46.259130 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m12:47:46.261396 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m12:47:46.261953 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m12:47:46.264884 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m12:47:46.265446 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.265446 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m12:47:46.266009 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.286219 [debug] [Thread-1 (]: SQL status: BEGIN in 0.020 seconds
[0m12:47:46.286766 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.286766 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m12:47:46.290926 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:47:46.294373 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.294938 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m12:47:46.297559 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:47:46.300601 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.300601 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m12:47:46.301765 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.303468 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:47:46.303468 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.304156 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:47:46.306550 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:47:46.308321 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m12:47:46.308886 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:47:46.309424 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m12:47:46.318090 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.008 seconds
[0m12:47:46.320351 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m12:47:46.321536 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB4B0050>]}
[0m12:47:46.323607 [info ] [Thread-1 (]: 2 of 10 OK created sql view model public.stg_payments .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:47:46.324762 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m12:47:46.325882 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m12:47:46.326437 [info ] [Thread-1 (]: 3 of 10 START sql view model public.stg_products ............................... [RUN]
[0m12:47:46.327021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m12:47:46.327613 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m12:47:46.333322 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m12:47:46.335057 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m12:47:46.341195 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m12:47:46.342559 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:47:46.343300 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m12:47:46.344156 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.362373 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m12:47:46.362998 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:47:46.363692 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(product_name AS VARCHAR) AS product_name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, name, category, price -- Grouping to ensure distinct products
  );
[0m12:47:46.366202 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "name" does not exist
LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                         ^

[0m12:47:46.366747 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: ROLLBACK
[0m12:47:46.367839 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m12:47:46.372023 [debug] [Thread-1 (]: Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql
[0m12:47:46.372572 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB469D00>]}
[0m12:47:46.373124 [error] [Thread-1 (]: 3 of 10 ERROR creating sql view model public.stg_products ...................... [[31mERROR[0m in 0.05s]
[0m12:47:46.373676 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m12:47:46.374245 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m12:47:46.374794 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.stg_products' to be skipped because of status 'error'.  Reason: Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql.
[0m12:47:46.374794 [info ] [Thread-1 (]: 4 of 10 START sql view model public.stg_raw_data ............................... [RUN]
[0m12:47:46.375980 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m12:47:46.376525 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m12:47:46.378728 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m12:47:46.379896 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m12:47:46.382139 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m12:47:46.383322 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.383322 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m12:47:46.383868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.395142 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m12:47:46.395719 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.395719 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m12:47:46.397966 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:47:46.401313 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.401621 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m12:47:46.402774 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.404638 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.405243 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m12:47:46.406506 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.407616 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:47:46.408165 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.408165 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:47:46.411645 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:47:46.416027 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m12:47:46.417230 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:47:46.417230 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m12:47:46.422828 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.005 seconds
[0m12:47:46.423887 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m12:47:46.424938 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBAF2C170>]}
[0m12:47:46.425723 [info ] [Thread-1 (]: 4 of 10 OK created sql view model public.stg_raw_data .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:47:46.426858 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m12:47:46.426858 [debug] [Thread-1 (]: Began running node model.data_pipeline.products
[0m12:47:46.427417 [info ] [Thread-1 (]: 5 of 10 SKIP relation public.products .......................................... [[33mSKIP[0m]
[0m12:47:46.427972 [debug] [Thread-1 (]: Finished running node model.data_pipeline.products
[0m12:47:46.427972 [debug] [Thread-1 (]: Began running node model.data_pipeline.orders
[0m12:47:46.428975 [info ] [Thread-1 (]: 6 of 10 START sql view model public.orders ..................................... [RUN]
[0m12:47:46.429527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.orders)
[0m12:47:46.430154 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.orders
[0m12:47:46.432409 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.orders"
[0m12:47:46.432951 [debug] [Thread-1 (]: Began executing node model.data_pipeline.orders
[0m12:47:46.437458 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.orders"
[0m12:47:46.438558 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:47:46.439132 [debug] [Thread-1 (]: On model.data_pipeline.orders: BEGIN
[0m12:47:46.439729 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.454332 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:47:46.454635 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:47:46.455139 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */

  create view "datamart"."public"."orders__dbt_tmp"
    
    
  as (
    -- models/marts/orders.sql
WITH orders_raw AS (
    SELECT DISTINCT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_payment_status AS status
    FROM "datamart"."public"."stg_raw_data"
),
orders_payment AS (
    SELECT order_id, MIN(payment_method) AS payment_method
    FROM "datamart"."public"."stg_payments"
    GROUP BY order_id
)
SELECT
    o.order_id,
    o.customer_id,
    o.order_date,
    o.status,
    o.total_amount,
    COALESCE(p.payment_method, '') AS payment_method,
    ''::text AS shipping_address  -- Default to empty string as shipping address is not provided
FROM orders_raw o
LEFT JOIN orders_payment p ON o.order_id = p.order_id
  );
[0m12:47:46.460357 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.005 seconds
[0m12:47:46.463539 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:47:46.464084 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
alter table "datamart"."public"."orders__dbt_tmp" rename to "orders"
[0m12:47:46.465896 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.467192 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:47:46.467731 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:47:46.467731 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:47:46.470455 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:47:46.472818 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."orders__dbt_backup"
[0m12:47:46.473388 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:47:46.473940 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
drop view if exists "datamart"."public"."orders__dbt_backup" cascade
[0m12:47:46.475036 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:47:46.476082 [debug] [Thread-1 (]: On model.data_pipeline.orders: Close
[0m12:47:46.476625 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB50CC20>]}
[0m12:47:46.477139 [info ] [Thread-1 (]: 6 of 10 OK created sql view model public.orders ................................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:47:46.477650 [debug] [Thread-1 (]: Finished running node model.data_pipeline.orders
[0m12:47:46.478202 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers_source
[0m12:47:46.478912 [info ] [Thread-1 (]: 7 of 10 START sql view model public.stg_customers_source ....................... [RUN]
[0m12:47:46.479493 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.orders, now model.data_pipeline.stg_customers_source)
[0m12:47:46.480115 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers_source
[0m12:47:46.482853 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers_source"
[0m12:47:46.483386 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers_source
[0m12:47:46.486307 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers_source"
[0m12:47:46.487440 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:47:46.487440 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: BEGIN
[0m12:47:46.487840 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.499917 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m12:47:46.500485 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:47:46.500485 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */

  create view "datamart"."public"."stg_customers_source__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m12:47:46.505213 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:47:46.507454 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:47:46.508027 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
alter table "datamart"."public"."stg_customers_source__dbt_tmp" rename to "stg_customers_source"
[0m12:47:46.509098 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.510486 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:47:46.510486 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:47:46.511280 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:47:46.513593 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:47:46.515950 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers_source__dbt_backup"
[0m12:47:46.516538 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:47:46.516538 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
drop view if exists "datamart"."public"."stg_customers_source__dbt_backup" cascade
[0m12:47:46.517803 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:47:46.518781 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: Close
[0m12:47:46.519925 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBB51C470>]}
[0m12:47:46.519925 [info ] [Thread-1 (]: 7 of 10 OK created sql view model public.stg_customers_source .................. [[32mCREATE VIEW[0m in 0.04s]
[0m12:47:46.521470 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers_source
[0m12:47:46.522374 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m12:47:46.522878 [info ] [Thread-1 (]: 8 of 10 START sql view model public.stg_orders ................................. [RUN]
[0m12:47:46.523456 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers_source, now model.data_pipeline.stg_orders)
[0m12:47:46.524004 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m12:47:46.526392 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m12:47:46.526949 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m12:47:46.534151 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m12:47:46.535278 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:47:46.535879 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m12:47:46.535879 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.549957 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:47:46.551097 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:47:46.551661 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m12:47:46.555366 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:47:46.559576 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:47:46.560146 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m12:47:46.561893 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:47:46.564067 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:47:46.564578 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:47:46.565183 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:47:46.567930 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:47:46.570537 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m12:47:46.571381 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:47:46.571947 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m12:47:46.573133 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:47:46.574268 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m12:47:46.574921 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB877A9F0>]}
[0m12:47:46.575513 [info ] [Thread-1 (]: 8 of 10 OK created sql view model public.stg_orders ............................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:47:46.576222 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m12:47:46.576798 [debug] [Thread-1 (]: Began running node model.data_pipeline.customers
[0m12:47:46.577163 [info ] [Thread-1 (]: 9 of 10 START sql view model public.customers .................................. [RUN]
[0m12:47:46.577668 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.customers)
[0m12:47:46.578227 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.customers
[0m12:47:46.580708 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.customers"
[0m12:47:46.581255 [debug] [Thread-1 (]: Began executing node model.data_pipeline.customers
[0m12:47:46.584029 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.customers"
[0m12:47:46.584597 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:47:46.585136 [debug] [Thread-1 (]: On model.data_pipeline.customers: BEGIN
[0m12:47:46.585136 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:47:46.611889 [debug] [Thread-1 (]: SQL status: BEGIN in 0.026 seconds
[0m12:47:46.613030 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:47:46.613030 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */

  create view "datamart"."public"."customers__dbt_tmp"
    
    
  as (
    -- models/marts/customers.sql
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    phone,
    address,
    signup_date AS created_at
FROM "datamart"."public"."stg_customers_source"
  );
[0m12:47:46.614755 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "signup_date" does not exist
LINE 15:     signup_date AS created_at
             ^

[0m12:47:46.615371 [debug] [Thread-1 (]: On model.data_pipeline.customers: ROLLBACK
[0m12:47:46.616565 [debug] [Thread-1 (]: On model.data_pipeline.customers: Close
[0m12:47:46.620109 [debug] [Thread-1 (]: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:47:46.621036 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0e901b1-ca0c-493d-b176-28b8f925d21b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FBAABAA20>]}
[0m12:47:46.622142 [error] [Thread-1 (]: 9 of 10 ERROR creating sql view model public.customers ......................... [[31mERROR[0m in 0.04s]
[0m12:47:46.623329 [debug] [Thread-1 (]: Finished running node model.data_pipeline.customers
[0m12:47:46.623889 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.customers' to be skipped because of status 'error'.  Reason: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql.
[0m12:47:46.624483 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m12:47:46.625064 [info ] [Thread-1 (]: 10 of 10 SKIP relation public.raw_to_normalized ................................ [[33mSKIP[0m]
[0m12:47:46.625624 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m12:47:46.626738 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.626738 [debug] [MainThread]: On master: BEGIN
[0m12:47:46.627286 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:47:46.639034 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m12:47:46.639608 [debug] [MainThread]: On master: COMMIT
[0m12:47:46.640165 [debug] [MainThread]: Using postgres connection "master"
[0m12:47:46.640165 [debug] [MainThread]: On master: COMMIT
[0m12:47:46.641285 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:47:46.641832 [debug] [MainThread]: On master: Close
[0m12:47:46.642439 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:47:46.642903 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m12:47:46.643498 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:47:46.643498 [debug] [MainThread]: Connection 'model.data_pipeline.customers' was properly closed.
[0m12:47:46.644156 [info ] [MainThread]: 
[0m12:47:46.644698 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 0.73 seconds (0.73s).
[0m12:47:46.646337 [debug] [MainThread]: Command end result
[0m12:47:46.671411 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:47:46.673721 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:47:46.679053 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:47:46.680160 [info ] [MainThread]: 
[0m12:47:46.680719 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m12:47:46.681276 [info ] [MainThread]: 
[0m12:47:46.681872 [error] [MainThread]:   Database Error in model stg_products (../../models\staging\stg_products.sql)
  column "name" does not exist
  LINE 35:     product_id, name, category, price -- Grouping to ensure ...
                           ^
  compiled code at target\run\data_pipeline\../../models\staging\stg_products.sql
[0m12:47:46.682497 [info ] [MainThread]: 
[0m12:47:46.682497 [error] [MainThread]:   Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:47:46.683072 [info ] [MainThread]: 
[0m12:47:46.683072 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=2 SKIP=2 TOTAL=10
[0m12:47:46.684148 [debug] [MainThread]: Command `dbt run` failed at 12:47:46.684148 after 1.87 seconds
[0m12:47:46.684148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB5004860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB92B8770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FB9116C60>]}
[0m12:47:46.684722 [debug] [MainThread]: Flushing usage events
[0m12:47:47.135161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:48:17.258813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF5412240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF78D8680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF78D8C50>]}


============================== 12:48:17.262407 | b7d4c373-a4da-4d33-bdbc-e6f546b2403c ==============================
[0m12:48:17.262407 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:48:17.264140 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:48:17.433029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF98D1610>]}
[0m12:48:17.481198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF84BF7A0>]}
[0m12:48:17.482330 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:48:17.665047 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:48:17.819224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:48:17.819790 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_products.sql
[0m12:48:18.054506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA0489E0>]}
[0m12:48:18.126248 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:48:18.154415 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:48:18.177555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA1B0440>]}
[0m12:48:18.178112 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:48:18.178680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF98F8950>]}
[0m12:48:18.181937 [info ] [MainThread]: 
[0m12:48:18.182494 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:48:18.183055 [info ] [MainThread]: 
[0m12:48:18.183614 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:48:18.187783 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m12:48:18.259444 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m12:48:18.259444 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m12:48:18.260013 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:48:18.278722 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.019 seconds
[0m12:48:18.281173 [debug] [ThreadPool]: On list_datamart: Close
[0m12:48:18.284178 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:48:18.290075 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:48:18.290075 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:48:18.290640 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:48:18.305469 [debug] [ThreadPool]: SQL status: BEGIN in 0.015 seconds
[0m12:48:18.306106 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:48:18.306106 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:48:18.310213 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.004 seconds
[0m12:48:18.311874 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:48:18.312895 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:48:18.320571 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:18.320571 [debug] [MainThread]: On master: BEGIN
[0m12:48:18.322219 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:48:18.337258 [debug] [MainThread]: SQL status: BEGIN in 0.015 seconds
[0m12:48:18.337258 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:18.337910 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:48:18.344364 [debug] [MainThread]: SQL status: SELECT 9 in 0.006 seconds
[0m12:48:18.346752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA1BBCE0>]}
[0m12:48:18.347272 [debug] [MainThread]: On master: ROLLBACK
[0m12:48:18.347909 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:18.347909 [debug] [MainThread]: On master: BEGIN
[0m12:48:18.349190 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:48:18.349798 [debug] [MainThread]: On master: COMMIT
[0m12:48:18.349798 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:18.350125 [debug] [MainThread]: On master: COMMIT
[0m12:48:18.350631 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:48:18.350631 [debug] [MainThread]: On master: Close
[0m12:48:18.393793 [debug] [Thread-1 (]: Began running node model.data_pipeline.reviews
[0m12:48:18.394800 [info ] [Thread-1 (]: 1 of 10 START sql view model public.reviews .................................... [RUN]
[0m12:48:18.397222 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.reviews'
[0m12:48:18.397543 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.reviews
[0m12:48:18.403676 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.reviews"
[0m12:48:18.404395 [debug] [Thread-1 (]: Began executing node model.data_pipeline.reviews
[0m12:48:18.428662 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.reviews"
[0m12:48:18.437571 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.438154 [debug] [Thread-1 (]: On model.data_pipeline.reviews: BEGIN
[0m12:48:18.438770 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:48:18.464613 [debug] [Thread-1 (]: SQL status: BEGIN in 0.026 seconds
[0m12:48:18.465173 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.465173 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */

  create view "datamart"."public"."reviews__dbt_tmp"
    
    
  as (
    -- models/marts/reviews.sql
-- Create an empty or placeholder model for reviews if not yet ingested
SELECT NULL::INTEGER AS review_id,
       NULL::INTEGER AS product_id,
       NULL::INTEGER AS customer_id,
       NULL::INTEGER AS rating,
       NULL::TEXT AS review_text,
       NULL::TIMESTAMP AS review_date
WHERE 1=0
  );
[0m12:48:18.467762 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:48:18.473670 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.473670 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews" rename to "reviews__dbt_backup"
[0m12:48:18.474790 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.547112 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.547689 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews__dbt_tmp" rename to "reviews"
[0m12:48:18.548804 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.564966 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:48:18.565504 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.566044 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:48:18.568706 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:48:18.576851 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."reviews__dbt_backup"
[0m12:48:18.581337 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:48:18.581909 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
drop view if exists "datamart"."public"."reviews__dbt_backup" cascade
[0m12:48:18.585144 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:48:18.588449 [debug] [Thread-1 (]: On model.data_pipeline.reviews: Close
[0m12:48:18.590685 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF78DA840>]}
[0m12:48:18.591885 [info ] [Thread-1 (]: 1 of 10 OK created sql view model public.reviews ............................... [[32mCREATE VIEW[0m in 0.19s]
[0m12:48:18.593155 [debug] [Thread-1 (]: Finished running node model.data_pipeline.reviews
[0m12:48:18.593669 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m12:48:18.594201 [info ] [Thread-1 (]: 2 of 10 START sql view model public.stg_payments ............................... [RUN]
[0m12:48:18.594739 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.reviews, now model.data_pipeline.stg_payments)
[0m12:48:18.594739 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m12:48:18.597430 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m12:48:18.598455 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m12:48:18.600612 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m12:48:18.601502 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.602011 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m12:48:18.602011 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.626575 [debug] [Thread-1 (]: SQL status: BEGIN in 0.024 seconds
[0m12:48:18.627109 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.627109 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m12:48:18.629125 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:48:18.632333 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.633429 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m12:48:18.634582 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.638289 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.638824 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m12:48:18.640433 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.641502 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:48:18.642017 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.642017 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:48:18.644685 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:48:18.648970 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m12:48:18.650033 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:48:18.650563 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m12:48:18.653771 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:48:18.656019 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m12:48:18.657060 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA5415E0>]}
[0m12:48:18.657772 [info ] [Thread-1 (]: 2 of 10 OK created sql view model public.stg_payments .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:48:18.658343 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m12:48:18.658866 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m12:48:18.659379 [info ] [Thread-1 (]: 3 of 10 START sql view model public.stg_products ............................... [RUN]
[0m12:48:18.659945 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m12:48:18.660710 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m12:48:18.666002 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m12:48:18.667153 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m12:48:18.671078 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m12:48:18.671649 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.672216 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m12:48:18.672795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.689427 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:48:18.690190 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.690190 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(product_name AS VARCHAR) AS product_name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, product_name, category, price -- Grouping to ensure distinct products
  );
[0m12:48:18.694643 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:48:18.697982 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.698536 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products" rename to "stg_products__dbt_backup"
[0m12:48:18.699671 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.702053 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.702579 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m12:48:18.703678 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.705139 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:48:18.705807 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.706170 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:48:18.708295 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:48:18.711335 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m12:48:18.711874 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:48:18.711874 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m12:48:18.714591 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:48:18.715672 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m12:48:18.716200 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA5403E0>]}
[0m12:48:18.716980 [info ] [Thread-1 (]: 3 of 10 OK created sql view model public.stg_products .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:48:18.718077 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m12:48:18.718077 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m12:48:18.718876 [info ] [Thread-1 (]: 4 of 10 START sql view model public.stg_raw_data ............................... [RUN]
[0m12:48:18.719381 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m12:48:18.719689 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m12:48:18.722848 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m12:48:18.723937 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m12:48:18.727037 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m12:48:18.728626 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.728626 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m12:48:18.729385 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.761771 [debug] [Thread-1 (]: SQL status: BEGIN in 0.033 seconds
[0m12:48:18.762550 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.763079 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m12:48:18.766302 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:48:18.769107 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.769698 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m12:48:18.771590 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:48:18.775077 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.775077 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m12:48:18.776177 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.777297 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:48:18.777297 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.777877 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:48:18.779551 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:48:18.782986 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m12:48:18.784233 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:48:18.784800 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m12:48:18.787054 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m12:48:18.787647 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m12:48:18.788236 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA5614C0>]}
[0m12:48:18.789412 [info ] [Thread-1 (]: 4 of 10 OK created sql view model public.stg_raw_data .......................... [[32mCREATE VIEW[0m in 0.07s]
[0m12:48:18.790006 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m12:48:18.790612 [debug] [Thread-1 (]: Began running node model.data_pipeline.products
[0m12:48:18.791162 [info ] [Thread-1 (]: 5 of 10 START sql view model public.products ................................... [RUN]
[0m12:48:18.792269 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.products)
[0m12:48:18.792817 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.products
[0m12:48:18.795641 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.products"
[0m12:48:18.797770 [debug] [Thread-1 (]: Began executing node model.data_pipeline.products
[0m12:48:18.806687 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.products"
[0m12:48:18.809210 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:48:18.809718 [debug] [Thread-1 (]: On model.data_pipeline.products: BEGIN
[0m12:48:18.810326 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.826662 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:48:18.827242 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:48:18.827648 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */

  create view "datamart"."public"."products__dbt_tmp"
    
    
  as (
    -- models/marts/products.sql
WITH base AS (
    SELECT
        product_id,
        product_name AS product_name,  -- Rename product_name to name
        '' AS description,     -- Placeholder if description is not provided
        price,
        category,
        0 AS stock_quantity,   -- Default value if not provided
        CURRENT_TIMESTAMP AS created_at  -- Or another logic for created_at
    FROM "datamart"."public"."stg_products"
)
SELECT * FROM base
  );
[0m12:48:18.830444 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:48:18.833786 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:48:18.834430 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
alter table "datamart"."public"."products__dbt_tmp" rename to "products"
[0m12:48:18.836344 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.838085 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:48:18.838085 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:48:18.838775 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:48:18.843571 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m12:48:18.847579 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."products__dbt_backup"
[0m12:48:18.848846 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:48:18.849151 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
drop view if exists "datamart"."public"."products__dbt_backup" cascade
[0m12:48:18.851189 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:48:18.853085 [debug] [Thread-1 (]: On model.data_pipeline.products: Close
[0m12:48:18.853593 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF9FE8440>]}
[0m12:48:18.854602 [info ] [Thread-1 (]: 5 of 10 OK created sql view model public.products .............................. [[32mCREATE VIEW[0m in 0.06s]
[0m12:48:18.856877 [debug] [Thread-1 (]: Finished running node model.data_pipeline.products
[0m12:48:18.857856 [debug] [Thread-1 (]: Began running node model.data_pipeline.orders
[0m12:48:18.858363 [info ] [Thread-1 (]: 6 of 10 START sql view model public.orders ..................................... [RUN]
[0m12:48:18.858892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.products, now model.data_pipeline.orders)
[0m12:48:18.858892 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.orders
[0m12:48:18.862799 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.orders"
[0m12:48:18.863370 [debug] [Thread-1 (]: Began executing node model.data_pipeline.orders
[0m12:48:18.866066 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.orders"
[0m12:48:18.867253 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:48:18.867887 [debug] [Thread-1 (]: On model.data_pipeline.orders: BEGIN
[0m12:48:18.867887 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.885008 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:48:18.885515 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:48:18.886042 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */

  create view "datamart"."public"."orders__dbt_tmp"
    
    
  as (
    -- models/marts/orders.sql
WITH orders_raw AS (
    SELECT DISTINCT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_payment_status AS status
    FROM "datamart"."public"."stg_raw_data"
),
orders_payment AS (
    SELECT order_id, MIN(payment_method) AS payment_method
    FROM "datamart"."public"."stg_payments"
    GROUP BY order_id
)
SELECT
    o.order_id,
    o.customer_id,
    o.order_date,
    o.status,
    o.total_amount,
    COALESCE(p.payment_method, '') AS payment_method,
    ''::text AS shipping_address  -- Default to empty string as shipping address is not provided
FROM orders_raw o
LEFT JOIN orders_payment p ON o.order_id = p.order_id
  );
[0m12:48:18.889894 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:48:18.893644 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:48:18.894186 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
alter table "datamart"."public"."orders__dbt_tmp" rename to "orders"
[0m12:48:18.895943 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.897010 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:48:18.897546 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:48:18.897546 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:48:18.899124 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:48:18.901368 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."orders__dbt_backup"
[0m12:48:18.901900 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:48:18.901900 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
drop view if exists "datamart"."public"."orders__dbt_backup" cascade
[0m12:48:18.902946 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:48:18.903843 [debug] [Thread-1 (]: On model.data_pipeline.orders: Close
[0m12:48:18.904443 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA67AFC0>]}
[0m12:48:18.904984 [info ] [Thread-1 (]: 6 of 10 OK created sql view model public.orders ................................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:48:18.906096 [debug] [Thread-1 (]: Finished running node model.data_pipeline.orders
[0m12:48:18.906096 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers_source
[0m12:48:18.906635 [info ] [Thread-1 (]: 7 of 10 START sql view model public.stg_customers_source ....................... [RUN]
[0m12:48:18.907162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.orders, now model.data_pipeline.stg_customers_source)
[0m12:48:18.907689 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers_source
[0m12:48:18.910444 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers_source"
[0m12:48:18.910989 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers_source
[0m12:48:18.913694 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers_source"
[0m12:48:18.914245 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:48:18.914795 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: BEGIN
[0m12:48:18.914795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.935359 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m12:48:18.935923 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:48:18.936476 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */

  create view "datamart"."public"."stg_customers_source__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS created_at
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m12:48:18.940923 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:48:18.944240 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:48:18.944881 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
alter table "datamart"."public"."stg_customers_source__dbt_tmp" rename to "stg_customers_source"
[0m12:48:18.946266 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:18.947871 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:48:18.947871 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:48:18.948401 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:48:18.950539 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:48:18.953613 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers_source__dbt_backup"
[0m12:48:18.954183 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:48:18.954702 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
drop view if exists "datamart"."public"."stg_customers_source__dbt_backup" cascade
[0m12:48:18.955808 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:48:18.956969 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: Close
[0m12:48:18.957526 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA558680>]}
[0m12:48:18.958989 [info ] [Thread-1 (]: 7 of 10 OK created sql view model public.stg_customers_source .................. [[32mCREATE VIEW[0m in 0.05s]
[0m12:48:18.959789 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers_source
[0m12:48:18.959789 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m12:48:18.960377 [info ] [Thread-1 (]: 8 of 10 START sql view model public.stg_orders ................................. [RUN]
[0m12:48:18.960973 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers_source, now model.data_pipeline.stg_orders)
[0m12:48:18.961592 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m12:48:18.965049 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m12:48:18.966084 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m12:48:18.973010 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m12:48:18.974802 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:48:18.975133 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m12:48:18.975653 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:18.997102 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m12:48:18.998800 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:48:18.999429 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m12:48:19.011299 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m12:48:19.016041 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:48:19.016620 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m12:48:19.017772 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:48:19.019560 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:48:19.020252 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:48:19.020644 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:48:19.027542 [debug] [Thread-1 (]: SQL status: COMMIT in 0.006 seconds
[0m12:48:19.030948 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m12:48:19.031465 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:48:19.032027 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m12:48:19.032586 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:48:19.034747 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m12:48:19.035927 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA02A060>]}
[0m12:48:19.036596 [info ] [Thread-1 (]: 8 of 10 OK created sql view model public.stg_orders ............................ [[32mCREATE VIEW[0m in 0.07s]
[0m12:48:19.037951 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m12:48:19.038514 [debug] [Thread-1 (]: Began running node model.data_pipeline.customers
[0m12:48:19.038514 [info ] [Thread-1 (]: 9 of 10 START sql view model public.customers .................................. [RUN]
[0m12:48:19.039036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.customers)
[0m12:48:19.039787 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.customers
[0m12:48:19.042407 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.customers"
[0m12:48:19.043478 [debug] [Thread-1 (]: Began executing node model.data_pipeline.customers
[0m12:48:19.046242 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.customers"
[0m12:48:19.046822 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:48:19.047533 [debug] [Thread-1 (]: On model.data_pipeline.customers: BEGIN
[0m12:48:19.048112 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:48:19.077067 [debug] [Thread-1 (]: SQL status: BEGIN in 0.029 seconds
[0m12:48:19.077638 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:48:19.077638 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */

  create view "datamart"."public"."customers__dbt_tmp"
    
    
  as (
    -- models/marts/customers.sql
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    phone,
    address,
    signup_date AS created_at
FROM "datamart"."public"."stg_customers_source"
  );
[0m12:48:19.079264 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "signup_date" does not exist
LINE 15:     signup_date AS created_at
             ^

[0m12:48:19.079264 [debug] [Thread-1 (]: On model.data_pipeline.customers: ROLLBACK
[0m12:48:19.080420 [debug] [Thread-1 (]: On model.data_pipeline.customers: Close
[0m12:48:19.084539 [debug] [Thread-1 (]: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:48:19.085111 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b7d4c373-a4da-4d33-bdbc-e6f546b2403c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF9FE3710>]}
[0m12:48:19.085715 [error] [Thread-1 (]: 9 of 10 ERROR creating sql view model public.customers ......................... [[31mERROR[0m in 0.05s]
[0m12:48:19.086395 [debug] [Thread-1 (]: Finished running node model.data_pipeline.customers
[0m12:48:19.086953 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.customers' to be skipped because of status 'error'.  Reason: Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql.
[0m12:48:19.088121 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m12:48:19.088718 [info ] [Thread-1 (]: 10 of 10 SKIP relation public.raw_to_normalized ................................ [[33mSKIP[0m]
[0m12:48:19.089322 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m12:48:19.090465 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:19.091041 [debug] [MainThread]: On master: BEGIN
[0m12:48:19.091041 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:48:19.110472 [debug] [MainThread]: SQL status: BEGIN in 0.020 seconds
[0m12:48:19.111624 [debug] [MainThread]: On master: COMMIT
[0m12:48:19.112448 [debug] [MainThread]: Using postgres connection "master"
[0m12:48:19.112448 [debug] [MainThread]: On master: COMMIT
[0m12:48:19.114320 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:48:19.115321 [debug] [MainThread]: On master: Close
[0m12:48:19.115827 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:48:19.116372 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m12:48:19.117117 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:48:19.117117 [debug] [MainThread]: Connection 'model.data_pipeline.customers' was properly closed.
[0m12:48:19.117823 [info ] [MainThread]: 
[0m12:48:19.118301 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 0.93 seconds (0.93s).
[0m12:48:19.119564 [debug] [MainThread]: Command end result
[0m12:48:19.143648 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:48:19.146568 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:48:19.152055 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:48:19.152055 [info ] [MainThread]: 
[0m12:48:19.152584 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:48:19.153104 [info ] [MainThread]: 
[0m12:48:19.153591 [error] [MainThread]:   Database Error in model customers (../../models\marts\customers.sql)
  column "signup_date" does not exist
  LINE 15:     signup_date AS created_at
               ^
  compiled code at target\run\data_pipeline\../../models\marts\customers.sql
[0m12:48:19.153591 [info ] [MainThread]: 
[0m12:48:19.154141 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=1 TOTAL=10
[0m12:48:19.155204 [debug] [MainThread]: Command `dbt run` failed at 12:48:19.154660 after 2.07 seconds
[0m12:48:19.155204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CF9B0D880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA1B22D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017CFA5D81A0>]}
[0m12:48:19.155745 [debug] [MainThread]: Flushing usage events
[0m12:48:19.631577 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:49:39.857968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BD87C860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BD87D790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231B7FF1490>]}


============================== 12:49:39.861177 | f774de72-b172-4512-bc2c-d7f5479467b4 ==============================
[0m12:49:39.861177 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:49:39.862277 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:49:40.000912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF584890>]}
[0m12:49:40.042489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF9FAF30>]}
[0m12:49:40.043605 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:49:40.253704 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:49:40.394591 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:49:40.395186 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\staging\stg_customers_source.sql
[0m12:49:40.580192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C0233DD0>]}
[0m12:49:40.654654 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:49:40.684879 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:49:40.704267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BFB79970>]}
[0m12:49:40.704866 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:49:40.704866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BFC7EF00>]}
[0m12:49:40.706596 [info ] [MainThread]: 
[0m12:49:40.707151 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:49:40.707151 [info ] [MainThread]: 
[0m12:49:40.707867 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:49:40.711247 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m12:49:40.768412 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m12:49:40.768974 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m12:49:40.768974 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:40.787694 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.019 seconds
[0m12:49:40.789634 [debug] [ThreadPool]: On list_datamart: Close
[0m12:49:40.791933 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:49:40.796478 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:49:40.796478 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:49:40.797225 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:49:40.808860 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m12:49:40.809466 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:49:40.810017 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:49:40.813290 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.003 seconds
[0m12:49:40.814496 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:49:40.815102 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:49:40.821795 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:40.822356 [debug] [MainThread]: On master: BEGIN
[0m12:49:40.822356 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:49:40.835668 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m12:49:40.836250 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:40.836250 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:49:40.840563 [debug] [MainThread]: SQL status: SELECT 10 in 0.004 seconds
[0m12:49:40.843956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BFBB6300>]}
[0m12:49:40.844508 [debug] [MainThread]: On master: ROLLBACK
[0m12:49:40.845680 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:40.846405 [debug] [MainThread]: On master: BEGIN
[0m12:49:40.848077 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:49:40.848077 [debug] [MainThread]: On master: COMMIT
[0m12:49:40.848696 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:40.848696 [debug] [MainThread]: On master: COMMIT
[0m12:49:40.849772 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:49:40.850283 [debug] [MainThread]: On master: Close
[0m12:49:40.854219 [debug] [Thread-1 (]: Began running node model.data_pipeline.reviews
[0m12:49:40.854774 [info ] [Thread-1 (]: 1 of 10 START sql view model public.reviews .................................... [RUN]
[0m12:49:40.855306 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.reviews'
[0m12:49:40.855830 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.reviews
[0m12:49:40.860469 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.reviews"
[0m12:49:40.861588 [debug] [Thread-1 (]: Began executing node model.data_pipeline.reviews
[0m12:49:40.885514 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.reviews"
[0m12:49:40.886562 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:40.887089 [debug] [Thread-1 (]: On model.data_pipeline.reviews: BEGIN
[0m12:49:40.887435 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:49:40.902260 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:49:40.902795 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:40.902795 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */

  create view "datamart"."public"."reviews__dbt_tmp"
    
    
  as (
    -- models/marts/reviews.sql
-- Create an empty or placeholder model for reviews if not yet ingested
SELECT NULL::INTEGER AS review_id,
       NULL::INTEGER AS product_id,
       NULL::INTEGER AS customer_id,
       NULL::INTEGER AS rating,
       NULL::TEXT AS review_text,
       NULL::TIMESTAMP AS review_date
WHERE 1=0
  );
[0m12:49:40.905610 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:40.910731 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:40.911267 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews" rename to "reviews__dbt_backup"
[0m12:49:40.912900 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:40.986526 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:40.987618 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews__dbt_tmp" rename to "reviews"
[0m12:49:40.989495 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:49:41.003589 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:49:41.004117 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:41.004117 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:49:41.007158 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:49:41.013042 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."reviews__dbt_backup"
[0m12:49:41.016870 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:49:41.016870 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
drop view if exists "datamart"."public"."reviews__dbt_backup" cascade
[0m12:49:41.020062 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:49:41.022238 [debug] [Thread-1 (]: On model.data_pipeline.reviews: Close
[0m12:49:41.024526 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C068A900>]}
[0m12:49:41.026315 [info ] [Thread-1 (]: 1 of 10 OK created sql view model public.reviews ............................... [[32mCREATE VIEW[0m in 0.17s]
[0m12:49:41.026891 [debug] [Thread-1 (]: Finished running node model.data_pipeline.reviews
[0m12:49:41.027432 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m12:49:41.027985 [info ] [Thread-1 (]: 2 of 10 START sql view model public.stg_payments ............................... [RUN]
[0m12:49:41.027985 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.reviews, now model.data_pipeline.stg_payments)
[0m12:49:41.028530 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m12:49:41.030576 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m12:49:41.031737 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m12:49:41.034566 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m12:49:41.035112 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.035650 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m12:49:41.035650 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.050695 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:49:41.051260 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.051783 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m12:49:41.053965 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:41.057575 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.058169 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m12:49:41.059344 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.062057 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.062619 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m12:49:41.063930 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.066406 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:49:41.066406 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.066987 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:49:41.070376 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:49:41.072782 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m12:49:41.073288 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:49:41.073903 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m12:49:41.076834 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:49:41.079126 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m12:49:41.079959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C05CC980>]}
[0m12:49:41.079959 [info ] [Thread-1 (]: 2 of 10 OK created sql view model public.stg_payments .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.083421 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m12:49:41.083421 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m12:49:41.084159 [info ] [Thread-1 (]: 3 of 10 START sql view model public.stg_products ............................... [RUN]
[0m12:49:41.085295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m12:49:41.085821 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m12:49:41.089727 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m12:49:41.090264 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m12:49:41.093012 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m12:49:41.093564 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.093564 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m12:49:41.094095 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.108553 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:49:41.109112 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.109694 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(product_name AS VARCHAR) AS product_name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, product_name, category, price -- Grouping to ensure distinct products
  );
[0m12:49:41.111924 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:41.115224 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.115664 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products" rename to "stg_products__dbt_backup"
[0m12:49:41.116956 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.119679 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.120209 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m12:49:41.122243 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.123870 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:49:41.123870 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.124449 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:49:41.127246 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:49:41.128934 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m12:49:41.130254 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:49:41.130853 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m12:49:41.134222 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:49:41.135925 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m12:49:41.136440 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C05CD2B0>]}
[0m12:49:41.137022 [info ] [Thread-1 (]: 3 of 10 OK created sql view model public.stg_products .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.138737 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m12:49:41.139445 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m12:49:41.140190 [info ] [Thread-1 (]: 4 of 10 START sql view model public.stg_raw_data ............................... [RUN]
[0m12:49:41.140755 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m12:49:41.141379 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m12:49:41.144490 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m12:49:41.144994 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m12:49:41.148563 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m12:49:41.149926 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.149926 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m12:49:41.150434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.165681 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:49:41.166276 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.166276 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m12:49:41.169114 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:41.171161 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.172240 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m12:49:41.173847 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.177390 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.177999 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m12:49:41.179130 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.180716 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:49:41.180716 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.181348 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:49:41.185813 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m12:49:41.188544 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m12:49:41.189117 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:49:41.189667 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m12:49:41.192653 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:49:41.193786 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m12:49:41.194333 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C00428D0>]}
[0m12:49:41.195907 [info ] [Thread-1 (]: 4 of 10 OK created sql view model public.stg_raw_data .......................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.196832 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m12:49:41.196832 [debug] [Thread-1 (]: Began running node model.data_pipeline.products
[0m12:49:41.197437 [info ] [Thread-1 (]: 5 of 10 START sql view model public.products ................................... [RUN]
[0m12:49:41.197990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.products)
[0m12:49:41.198557 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.products
[0m12:49:41.200738 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.products"
[0m12:49:41.201313 [debug] [Thread-1 (]: Began executing node model.data_pipeline.products
[0m12:49:41.205466 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.products"
[0m12:49:41.206041 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:49:41.206648 [debug] [Thread-1 (]: On model.data_pipeline.products: BEGIN
[0m12:49:41.207176 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.225998 [debug] [Thread-1 (]: SQL status: BEGIN in 0.019 seconds
[0m12:49:41.226624 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:49:41.226624 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */

  create view "datamart"."public"."products__dbt_tmp"
    
    
  as (
    -- models/marts/products.sql
WITH base AS (
    SELECT
        product_id,
        product_name AS product_name,  -- Rename product_name to name
        '' AS description,     -- Placeholder if description is not provided
        price,
        category,
        0 AS stock_quantity,   -- Default value if not provided
        CURRENT_TIMESTAMP AS created_at  -- Or another logic for created_at
    FROM "datamart"."public"."stg_products"
)
SELECT * FROM base
  );
[0m12:49:41.228885 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:41.232159 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:49:41.232766 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
alter table "datamart"."public"."products__dbt_tmp" rename to "products"
[0m12:49:41.233881 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.235524 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:49:41.236094 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:49:41.236094 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:49:41.239609 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:49:41.241994 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."products__dbt_backup"
[0m12:49:41.242594 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:49:41.242594 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
drop view if exists "datamart"."public"."products__dbt_backup" cascade
[0m12:49:41.243717 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:49:41.244921 [debug] [Thread-1 (]: On model.data_pipeline.products: Close
[0m12:49:41.245531 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF584FE0>]}
[0m12:49:41.246364 [info ] [Thread-1 (]: 5 of 10 OK created sql view model public.products .............................. [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.247796 [debug] [Thread-1 (]: Finished running node model.data_pipeline.products
[0m12:49:41.247796 [debug] [Thread-1 (]: Began running node model.data_pipeline.orders
[0m12:49:41.248520 [info ] [Thread-1 (]: 6 of 10 START sql view model public.orders ..................................... [RUN]
[0m12:49:41.249094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.products, now model.data_pipeline.orders)
[0m12:49:41.249601 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.orders
[0m12:49:41.252547 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.orders"
[0m12:49:41.253136 [debug] [Thread-1 (]: Began executing node model.data_pipeline.orders
[0m12:49:41.256938 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.orders"
[0m12:49:41.258075 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:49:41.258629 [debug] [Thread-1 (]: On model.data_pipeline.orders: BEGIN
[0m12:49:41.259160 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.316036 [debug] [Thread-1 (]: SQL status: BEGIN in 0.057 seconds
[0m12:49:41.316566 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:49:41.317123 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */

  create view "datamart"."public"."orders__dbt_tmp"
    
    
  as (
    -- models/marts/orders.sql
WITH orders_raw AS (
    SELECT DISTINCT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_payment_status AS status
    FROM "datamart"."public"."stg_raw_data"
),
orders_payment AS (
    SELECT order_id, MIN(payment_method) AS payment_method
    FROM "datamart"."public"."stg_payments"
    GROUP BY order_id
)
SELECT
    o.order_id,
    o.customer_id,
    o.order_date,
    o.status,
    o.total_amount,
    COALESCE(p.payment_method, '') AS payment_method,
    ''::text AS shipping_address  -- Default to empty string as shipping address is not provided
FROM orders_raw o
LEFT JOIN orders_payment p ON o.order_id = p.order_id
  );
[0m12:49:41.320381 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:49:41.325208 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:49:41.325753 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
alter table "datamart"."public"."orders__dbt_tmp" rename to "orders"
[0m12:49:41.327513 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.328598 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:49:41.329185 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:49:41.329185 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:49:41.331085 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:49:41.332941 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."orders__dbt_backup"
[0m12:49:41.333482 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:49:41.334045 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
drop view if exists "datamart"."public"."orders__dbt_backup" cascade
[0m12:49:41.334567 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:49:41.336329 [debug] [Thread-1 (]: On model.data_pipeline.orders: Close
[0m12:49:41.336329 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C06EFAA0>]}
[0m12:49:41.336329 [info ] [Thread-1 (]: 6 of 10 OK created sql view model public.orders ................................ [[32mCREATE VIEW[0m in 0.09s]
[0m12:49:41.338338 [debug] [Thread-1 (]: Finished running node model.data_pipeline.orders
[0m12:49:41.339070 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers_source
[0m12:49:41.339714 [info ] [Thread-1 (]: 7 of 10 START sql view model public.stg_customers_source ....................... [RUN]
[0m12:49:41.340252 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.orders, now model.data_pipeline.stg_customers_source)
[0m12:49:41.340252 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers_source
[0m12:49:41.343185 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers_source"
[0m12:49:41.343730 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers_source
[0m12:49:41.346042 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers_source"
[0m12:49:41.347288 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:49:41.347288 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: BEGIN
[0m12:49:41.347822 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.360301 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m12:49:41.360301 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:49:41.360831 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */

  create view "datamart"."public"."stg_customers_source__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS signup_date
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m12:49:41.363919 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:49:41.366192 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:49:41.366712 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
alter table "datamart"."public"."stg_customers_source__dbt_tmp" rename to "stg_customers_source"
[0m12:49:41.367874 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.369031 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:49:41.369607 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:49:41.369607 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:49:41.373261 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:49:41.375637 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers_source__dbt_backup"
[0m12:49:41.376699 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:49:41.376699 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
drop view if exists "datamart"."public"."stg_customers_source__dbt_backup" cascade
[0m12:49:41.377797 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:49:41.380025 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: Close
[0m12:49:41.380696 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C06C7650>]}
[0m12:49:41.380696 [info ] [Thread-1 (]: 7 of 10 OK created sql view model public.stg_customers_source .................. [[32mCREATE VIEW[0m in 0.04s]
[0m12:49:41.382311 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers_source
[0m12:49:41.382311 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m12:49:41.382859 [info ] [Thread-1 (]: 8 of 10 START sql view model public.stg_orders ................................. [RUN]
[0m12:49:41.383424 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers_source, now model.data_pipeline.stg_orders)
[0m12:49:41.383976 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m12:49:41.386169 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m12:49:41.387226 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m12:49:41.390988 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m12:49:41.391996 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:49:41.392503 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m12:49:41.392503 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.414341 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m12:49:41.414919 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:49:41.415429 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m12:49:41.418777 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:49:41.421068 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:49:41.422359 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m12:49:41.423930 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.425070 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:49:41.425630 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:49:41.425630 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:49:41.428397 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:49:41.430478 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m12:49:41.431725 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:49:41.431725 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m12:49:41.432899 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:49:41.434589 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m12:49:41.436165 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF9D7980>]}
[0m12:49:41.436165 [info ] [Thread-1 (]: 8 of 10 OK created sql view model public.stg_orders ............................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.439124 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m12:49:41.439124 [debug] [Thread-1 (]: Began running node model.data_pipeline.customers
[0m12:49:41.439733 [info ] [Thread-1 (]: 9 of 10 START sql view model public.customers .................................. [RUN]
[0m12:49:41.440317 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.customers)
[0m12:49:41.440317 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.customers
[0m12:49:41.442792 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.customers"
[0m12:49:41.443872 [debug] [Thread-1 (]: Began executing node model.data_pipeline.customers
[0m12:49:41.446406 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.customers"
[0m12:49:41.447764 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:49:41.448118 [debug] [Thread-1 (]: On model.data_pipeline.customers: BEGIN
[0m12:49:41.448118 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.470146 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m12:49:41.471164 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:49:41.471164 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */

  create view "datamart"."public"."customers__dbt_tmp"
    
    
  as (
    -- models/marts/customers.sql
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    phone,
    address,
    signup_date AS created_at
FROM "datamart"."public"."stg_customers_source"
  );
[0m12:49:41.473236 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:49:41.476108 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:49:41.476667 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */
alter table "datamart"."public"."customers__dbt_tmp" rename to "customers"
[0m12:49:41.477774 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:49:41.478451 [debug] [Thread-1 (]: On model.data_pipeline.customers: COMMIT
[0m12:49:41.479209 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:49:41.479713 [debug] [Thread-1 (]: On model.data_pipeline.customers: COMMIT
[0m12:49:41.483370 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m12:49:41.485842 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."customers__dbt_backup"
[0m12:49:41.486971 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:49:41.487545 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */
drop view if exists "datamart"."public"."customers__dbt_backup" cascade
[0m12:49:41.489073 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:49:41.490501 [debug] [Thread-1 (]: On model.data_pipeline.customers: Close
[0m12:49:41.491066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF9EB6B0>]}
[0m12:49:41.491677 [info ] [Thread-1 (]: 9 of 10 OK created sql view model public.customers ............................. [[32mCREATE VIEW[0m in 0.05s]
[0m12:49:41.492267 [debug] [Thread-1 (]: Finished running node model.data_pipeline.customers
[0m12:49:41.493393 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m12:49:41.493954 [info ] [Thread-1 (]: 10 of 10 START sql view model public.raw_to_normalized ......................... [RUN]
[0m12:49:41.494549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.customers, now model.data_pipeline.raw_to_normalized)
[0m12:49:41.495161 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.raw_to_normalized
[0m12:49:41.497601 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.raw_to_normalized"
[0m12:49:41.498690 [debug] [Thread-1 (]: Began executing node model.data_pipeline.raw_to_normalized
[0m12:49:41.502593 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.raw_to_normalized"
[0m12:49:41.503153 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:49:41.503726 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: BEGIN
[0m12:49:41.503726 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:49:41.518784 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:49:41.519308 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:49:41.519308 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */

  create view "datamart"."public"."raw_to_normalized__dbt_tmp"
    
    
  as (
    -- models/raw_to_normalized.sql

-- This model combines data from the staged customer, order, product, and payment tables
-- to create a comprehensive, normalized view of the e-commerce data.
-- It serves as the primary data source for analytics and reporting.
-- models/raw_to_normalized.sql

WITH orders AS (
    SELECT * FROM "datamart"."public"."orders"
),
customers AS (
    SELECT * FROM "datamart"."public"."customers"
),
products AS (
    SELECT * FROM "datamart"."public"."products"
),
payments AS (
    SELECT * FROM "datamart"."public"."stg_payments"
)
SELECT
    o.order_id,
    o.order_date,
    o.total_amount,
    o.status,
    o.payment_method,
    o.shipping_address,
    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    c.phone,
    c.address AS customer_address,
    r.product_id,
    p.name AS product_name,
    p.category,
    p.price
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
LEFT JOIN "datamart"."public"."stg_raw_data" r ON o.order_id = r.order_id
LEFT JOIN products p ON r.product_id = p.product_id
  );
[0m12:49:41.522497 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column p.name does not exist
LINE 40:     p.name AS product_name,
             ^

[0m12:49:41.523064 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: ROLLBACK
[0m12:49:41.524533 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: Close
[0m12:49:41.528584 [debug] [Thread-1 (]: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.name does not exist
  LINE 40:     p.name AS product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m12:49:41.529201 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f774de72-b172-4512-bc2c-d7f5479467b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C06FFE30>]}
[0m12:49:41.529908 [error] [Thread-1 (]: 10 of 10 ERROR creating sql view model public.raw_to_normalized ................ [[31mERROR[0m in 0.03s]
[0m12:49:41.530615 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m12:49:41.530986 [debug] [Thread-4 (]: Marking all children of 'model.data_pipeline.raw_to_normalized' to be skipped because of status 'error'.  Reason: Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.name does not exist
  LINE 40:     p.name AS product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql.
[0m12:49:41.531741 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:41.532946 [debug] [MainThread]: On master: BEGIN
[0m12:49:41.532946 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:49:41.546479 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m12:49:41.546988 [debug] [MainThread]: On master: COMMIT
[0m12:49:41.546988 [debug] [MainThread]: Using postgres connection "master"
[0m12:49:41.547566 [debug] [MainThread]: On master: COMMIT
[0m12:49:41.548643 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:49:41.548643 [debug] [MainThread]: On master: Close
[0m12:49:41.549649 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:49:41.549649 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m12:49:41.549649 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:49:41.549649 [debug] [MainThread]: Connection 'model.data_pipeline.raw_to_normalized' was properly closed.
[0m12:49:41.550648 [info ] [MainThread]: 
[0m12:49:41.551500 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 0.84 seconds (0.84s).
[0m12:49:41.552703 [debug] [MainThread]: Command end result
[0m12:49:41.575616 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:49:41.578317 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:49:41.583153 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:49:41.583657 [info ] [MainThread]: 
[0m12:49:41.584209 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:49:41.584209 [info ] [MainThread]: 
[0m12:49:41.584773 [error] [MainThread]:   Database Error in model raw_to_normalized (../../models\raw_to_normalized.sql)
  column p.name does not exist
  LINE 40:     p.name AS product_name,
               ^
  compiled code at target\run\data_pipeline\../../models\raw_to_normalized.sql
[0m12:49:41.585319 [info ] [MainThread]: 
[0m12:49:41.585319 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=1 SKIP=0 TOTAL=10
[0m12:49:41.585876 [debug] [MainThread]: Command `dbt run` failed at 12:49:41.585876 after 1.89 seconds
[0m12:49:41.586422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BDEB1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BBE6DB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BFBB6300>]}
[0m12:49:41.587026 [debug] [MainThread]: Flushing usage events
[0m12:49:42.073067 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:50:29.658684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B390793A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B39078FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B39078BF0>]}


============================== 12:50:29.662050 | 4f7a6dec-48ea-4470-9255-a3dc7c3944ed ==============================
[0m12:50:29.662050 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:50:29.662603 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:50:29.815761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B39332060>]}
[0m12:50:29.860760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B39694FE0>]}
[0m12:50:29.861956 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:50:30.064183 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:50:30.230531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:50:30.231100 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\marts\orders.sql
[0m12:50:30.231664 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\raw_to_normalized.sql
[0m12:50:30.445687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3ABFCD10>]}
[0m12:50:30.501168 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:50:30.531261 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:50:30.553475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B3B28D0>]}
[0m12:50:30.554034 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:50:30.554591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3A7DBB90>]}
[0m12:50:30.556527 [info ] [MainThread]: 
[0m12:50:30.557050 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:50:30.557050 [info ] [MainThread]: 
[0m12:50:30.557678 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:50:30.560573 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m12:50:30.611546 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m12:50:30.611546 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m12:50:30.612187 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:50:30.644910 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.033 seconds
[0m12:50:30.646103 [debug] [ThreadPool]: On list_datamart: Close
[0m12:50:30.647835 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:50:30.652745 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:50:30.652745 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:50:30.653294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:50:30.689172 [debug] [ThreadPool]: SQL status: BEGIN in 0.036 seconds
[0m12:50:30.689752 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:50:30.690394 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:50:30.694684 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.004 seconds
[0m12:50:30.695925 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:50:30.697310 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:50:30.705255 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:30.705832 [debug] [MainThread]: On master: BEGIN
[0m12:50:30.705832 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:50:30.718908 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m12:50:30.718908 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:30.719499 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:50:30.724177 [debug] [MainThread]: SQL status: SELECT 11 in 0.004 seconds
[0m12:50:30.725801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B1089E0>]}
[0m12:50:30.726311 [debug] [MainThread]: On master: ROLLBACK
[0m12:50:30.726873 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:30.727470 [debug] [MainThread]: On master: BEGIN
[0m12:50:30.728038 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:50:30.728603 [debug] [MainThread]: On master: COMMIT
[0m12:50:30.728603 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:30.729178 [debug] [MainThread]: On master: COMMIT
[0m12:50:30.729643 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:50:30.730148 [debug] [MainThread]: On master: Close
[0m12:50:30.735649 [debug] [Thread-1 (]: Began running node model.data_pipeline.reviews
[0m12:50:30.735649 [info ] [Thread-1 (]: 1 of 10 START sql view model public.reviews .................................... [RUN]
[0m12:50:30.736748 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.data_pipeline.reviews'
[0m12:50:30.736748 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.reviews
[0m12:50:30.741657 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.reviews"
[0m12:50:30.742162 [debug] [Thread-1 (]: Began executing node model.data_pipeline.reviews
[0m12:50:30.766390 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.reviews"
[0m12:50:30.766916 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.767451 [debug] [Thread-1 (]: On model.data_pipeline.reviews: BEGIN
[0m12:50:30.767451 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:50:30.797991 [debug] [Thread-1 (]: SQL status: BEGIN in 0.030 seconds
[0m12:50:30.798531 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.799064 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */

  create view "datamart"."public"."reviews__dbt_tmp"
    
    
  as (
    -- models/marts/reviews.sql
-- Create an empty or placeholder model for reviews if not yet ingested
SELECT NULL::INTEGER AS review_id,
       NULL::INTEGER AS product_id,
       NULL::INTEGER AS customer_id,
       NULL::INTEGER AS rating,
       NULL::TEXT AS review_text,
       NULL::TIMESTAMP AS review_date
WHERE 1=0
  );
[0m12:50:30.800651 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:30.805729 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.806268 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews" rename to "reviews__dbt_backup"
[0m12:50:30.807565 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:30.856966 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.857580 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
alter table "datamart"."public"."reviews__dbt_tmp" rename to "reviews"
[0m12:50:30.860519 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:50:30.877195 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:50:30.878307 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.878874 [debug] [Thread-1 (]: On model.data_pipeline.reviews: COMMIT
[0m12:50:30.881162 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:50:30.885553 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."reviews__dbt_backup"
[0m12:50:30.890234 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.reviews"
[0m12:50:30.890234 [debug] [Thread-1 (]: On model.data_pipeline.reviews: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.reviews"} */
drop view if exists "datamart"."public"."reviews__dbt_backup" cascade
[0m12:50:30.893035 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m12:50:30.894736 [debug] [Thread-1 (]: On model.data_pipeline.reviews: Close
[0m12:50:30.897010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B895970>]}
[0m12:50:30.897515 [info ] [Thread-1 (]: 1 of 10 OK created sql view model public.reviews ............................... [[32mCREATE VIEW[0m in 0.16s]
[0m12:50:30.898720 [debug] [Thread-1 (]: Finished running node model.data_pipeline.reviews
[0m12:50:30.898720 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_payments
[0m12:50:30.899300 [info ] [Thread-1 (]: 2 of 10 START sql view model public.stg_payments ............................... [RUN]
[0m12:50:30.899873 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.reviews, now model.data_pipeline.stg_payments)
[0m12:50:30.899873 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_payments
[0m12:50:30.903160 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_payments"
[0m12:50:30.904665 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_payments
[0m12:50:30.907119 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_payments"
[0m12:50:30.907738 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.908309 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: BEGIN
[0m12:50:30.908647 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:30.933863 [debug] [Thread-1 (]: SQL status: BEGIN in 0.025 seconds
[0m12:50:30.934885 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.934885 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */

  create view "datamart"."public"."stg_payments__dbt_tmp"
    
    
  as (
    -- models/staging/stg_payments.sql
-- This staging model extracts distinct payment transaction information from the 'payments' source.

WITH source_payments AS (
    SELECT
        payment_id,
        order_id,
        payment_method,
        transaction_payment_status  -- Use the correct field name as defined in your sources.yml
    FROM "datamart"."raw"."payments"
)

SELECT
    CAST(payment_id AS INTEGER) AS payment_id,
    CAST(order_id AS INTEGER) AS order_id,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(transaction_payment_status AS VARCHAR) AS status  -- Aliasing to 'status' for consistency downstream
FROM 
    source_payments
WHERE 
    payment_id IS NOT NULL
  );
[0m12:50:30.938690 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:50:30.941092 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.941684 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments" rename to "stg_payments__dbt_backup"
[0m12:50:30.942790 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:30.945069 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.945069 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
alter table "datamart"."public"."stg_payments__dbt_tmp" rename to "stg_payments"
[0m12:50:30.947609 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:50:30.949321 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:50:30.949921 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.950430 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: COMMIT
[0m12:50:30.952886 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:50:30.955054 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_payments__dbt_backup"
[0m12:50:30.955587 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_payments"
[0m12:50:30.955587 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_payments"} */
drop view if exists "datamart"."public"."stg_payments__dbt_backup" cascade
[0m12:50:30.957738 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m12:50:30.959308 [debug] [Thread-1 (]: On model.data_pipeline.stg_payments: Close
[0m12:50:30.959844 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3ABFCB90>]}
[0m12:50:30.959844 [info ] [Thread-1 (]: 2 of 10 OK created sql view model public.stg_payments .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:50:30.960948 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_payments
[0m12:50:30.961454 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_products
[0m12:50:30.961454 [info ] [Thread-1 (]: 3 of 10 START sql view model public.stg_products ............................... [RUN]
[0m12:50:30.962133 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_payments, now model.data_pipeline.stg_products)
[0m12:50:30.962714 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_products
[0m12:50:30.965861 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_products"
[0m12:50:30.966382 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_products
[0m12:50:30.969148 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_products"
[0m12:50:30.969720 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:30.970166 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: BEGIN
[0m12:50:30.970166 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:30.981562 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m12:50:30.982119 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:30.982673 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */

  create view "datamart"."public"."stg_products__dbt_tmp"
    
    
  as (
    -- models/staging/stg_products.sql

-- This staging model extracts distinct product information from the 'products' source.
-- It ensures each product appears once with its core details.

WITH source_products AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM
        "datamart"."raw"."products"
)

SELECT
    CAST(product_id AS INTEGER) AS product_id,
    CAST(product_name AS VARCHAR) AS product_name,
    CAST(category AS VARCHAR) AS category,
    CAST(price AS NUMERIC(10, 2)) AS price, -- Assuming 2 decimal places for currency
    -- Add a placeholder for created_at, as it's not in source.
    -- In a real scenario, this would come from the raw product data's ingestion timestamp.
    NOW() AS created_at -- Using current timestamp as a placeholder
FROM
    source_products
WHERE
    product_id IS NOT NULL -- Ensure product_id is not null for distinctness
GROUP BY
    product_id, product_name, category, price -- Grouping to ensure distinct products
  );
[0m12:50:30.984885 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:30.988579 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:30.989055 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products" rename to "stg_products__dbt_backup"
[0m12:50:30.990103 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:30.991814 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:30.992832 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
alter table "datamart"."public"."stg_products__dbt_tmp" rename to "stg_products"
[0m12:50:30.993409 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:30.994504 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:50:30.995059 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:30.995059 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: COMMIT
[0m12:50:30.997270 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:50:30.999372 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_products__dbt_backup"
[0m12:50:30.999968 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_products"
[0m12:50:31.000553 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_products"} */
drop view if exists "datamart"."public"."stg_products__dbt_backup" cascade
[0m12:50:31.002781 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m12:50:31.003479 [debug] [Thread-1 (]: On model.data_pipeline.stg_products: Close
[0m12:50:31.004607 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B7C9910>]}
[0m12:50:31.005620 [info ] [Thread-1 (]: 3 of 10 OK created sql view model public.stg_products .......................... [[32mCREATE VIEW[0m in 0.04s]
[0m12:50:31.006811 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_products
[0m12:50:31.006811 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_raw_data
[0m12:50:31.008071 [info ] [Thread-1 (]: 4 of 10 START sql view model public.stg_raw_data ............................... [RUN]
[0m12:50:31.008685 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_products, now model.data_pipeline.stg_raw_data)
[0m12:50:31.008685 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_raw_data
[0m12:50:31.011555 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_raw_data"
[0m12:50:31.012116 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_raw_data
[0m12:50:31.014905 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_raw_data"
[0m12:50:31.016058 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.016058 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: BEGIN
[0m12:50:31.016676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.032216 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:31.032782 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.033370 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */

  create view "datamart"."public"."stg_raw_data__dbt_tmp"
    
    
  as (
    -- models/staging/stg_raw_data.sql

-- This staging model selects all columns from the 'raw_data' source.
-- It serves as a foundational layer, ensuring consistent column naming
-- and initial data type consistency before further transformations.

WITH source_data AS (
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_level_payment_status,
        product_id
    FROM "datamart"."raw"."raw_data"
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(order_level_payment_status AS VARCHAR) AS order_payment_status,
    CAST(product_id AS INTEGER) AS product_id
FROM source_data
-- This staging model prepares the raw data for further transformations
-- by ensuring that all necessary columns are present and correctly typed.
  );
[0m12:50:31.035885 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:31.039309 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.039309 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data" rename to "stg_raw_data__dbt_backup"
[0m12:50:31.041934 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.045122 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.045685 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
alter table "datamart"."public"."stg_raw_data__dbt_tmp" rename to "stg_raw_data"
[0m12:50:31.047044 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.048188 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:50:31.048739 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.048739 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: COMMIT
[0m12:50:31.056442 [debug] [Thread-1 (]: SQL status: COMMIT in 0.007 seconds
[0m12:50:31.058151 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_raw_data__dbt_backup"
[0m12:50:31.058692 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_raw_data"
[0m12:50:31.059299 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_raw_data"} */
drop view if exists "datamart"."public"."stg_raw_data__dbt_backup" cascade
[0m12:50:31.062738 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m12:50:31.064310 [debug] [Thread-1 (]: On model.data_pipeline.stg_raw_data: Close
[0m12:50:31.065440 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B266BD0>]}
[0m12:50:31.066275 [info ] [Thread-1 (]: 4 of 10 OK created sql view model public.stg_raw_data .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m12:50:31.066275 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_raw_data
[0m12:50:31.066958 [debug] [Thread-1 (]: Began running node model.data_pipeline.products
[0m12:50:31.067507 [info ] [Thread-1 (]: 5 of 10 START sql view model public.products ................................... [RUN]
[0m12:50:31.068017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_raw_data, now model.data_pipeline.products)
[0m12:50:31.068577 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.products
[0m12:50:31.070241 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.products"
[0m12:50:31.071730 [debug] [Thread-1 (]: Began executing node model.data_pipeline.products
[0m12:50:31.073954 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.products"
[0m12:50:31.074494 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:50:31.075073 [debug] [Thread-1 (]: On model.data_pipeline.products: BEGIN
[0m12:50:31.075073 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.089157 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:50:31.090437 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:50:31.091021 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */

  create view "datamart"."public"."products__dbt_tmp"
    
    
  as (
    -- models/marts/products.sql
WITH base AS (
    SELECT
        product_id,
        product_name AS product_name,  -- Rename product_name to name
        '' AS description,     -- Placeholder if description is not provided
        price,
        category,
        0 AS stock_quantity,   -- Default value if not provided
        CURRENT_TIMESTAMP AS created_at  -- Or another logic for created_at
    FROM "datamart"."public"."stg_products"
)
SELECT * FROM base
  );
[0m12:50:31.093234 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:31.097217 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:50:31.097953 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
alter table "datamart"."public"."products__dbt_tmp" rename to "products"
[0m12:50:31.099161 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.100208 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:50:31.100764 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:50:31.100764 [debug] [Thread-1 (]: On model.data_pipeline.products: COMMIT
[0m12:50:31.102393 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:50:31.104667 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."products__dbt_backup"
[0m12:50:31.105250 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.products"
[0m12:50:31.105805 [debug] [Thread-1 (]: On model.data_pipeline.products: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.products"} */
drop view if exists "datamart"."public"."products__dbt_backup" cascade
[0m12:50:31.106970 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.107512 [debug] [Thread-1 (]: On model.data_pipeline.products: Close
[0m12:50:31.108751 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3AD6BBF0>]}
[0m12:50:31.109318 [info ] [Thread-1 (]: 5 of 10 OK created sql view model public.products .............................. [[32mCREATE VIEW[0m in 0.04s]
[0m12:50:31.109318 [debug] [Thread-1 (]: Finished running node model.data_pipeline.products
[0m12:50:31.110040 [debug] [Thread-1 (]: Began running node model.data_pipeline.orders
[0m12:50:31.110591 [info ] [Thread-1 (]: 6 of 10 START sql view model public.orders ..................................... [RUN]
[0m12:50:31.111136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.products, now model.data_pipeline.orders)
[0m12:50:31.111681 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.orders
[0m12:50:31.114244 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.orders"
[0m12:50:31.114794 [debug] [Thread-1 (]: Began executing node model.data_pipeline.orders
[0m12:50:31.118205 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.orders"
[0m12:50:31.119335 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:50:31.119335 [debug] [Thread-1 (]: On model.data_pipeline.orders: BEGIN
[0m12:50:31.119901 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.134099 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:31.134648 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:50:31.135199 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */

  create view "datamart"."public"."orders__dbt_tmp"
    
    
  as (
    -- models/marts/orders.sql
WITH orders_raw AS (
    SELECT DISTINCT
        order_id,
        customer_id,
        order_date,
        total_amount,
        order_payment_status AS status
    FROM "datamart"."public"."stg_raw_data"
),
orders_payment AS (
    SELECT order_id, MIN(payment_method) AS payment_method
    FROM "datamart"."public"."stg_payments"
    GROUP BY order_id
)
SELECT
    o.order_id,
    o.customer_id,
    o.order_date,
    o.status,
    o.total_amount,
    COALESCE(p.payment_method, '') AS payment_method,
    '1 Sevran France 93240'::text AS shipping_address  -- Default to empty string as shipping address is not provided
FROM orders_raw o
LEFT JOIN orders_payment p ON o.order_id = p.order_id
  );
[0m12:50:31.138136 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:31.140727 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:50:31.141326 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
alter table "datamart"."public"."orders__dbt_tmp" rename to "orders"
[0m12:50:31.142200 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.144654 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:50:31.145204 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:50:31.146275 [debug] [Thread-1 (]: On model.data_pipeline.orders: COMMIT
[0m12:50:31.151439 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m12:50:31.153272 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."orders__dbt_backup"
[0m12:50:31.154365 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.orders"
[0m12:50:31.154365 [debug] [Thread-1 (]: On model.data_pipeline.orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.orders"} */
drop view if exists "datamart"."public"."orders__dbt_backup" cascade
[0m12:50:31.156513 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.157755 [debug] [Thread-1 (]: On model.data_pipeline.orders: Close
[0m12:50:31.158318 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B7D4E00>]}
[0m12:50:31.159343 [info ] [Thread-1 (]: 6 of 10 OK created sql view model public.orders ................................ [[32mCREATE VIEW[0m in 0.05s]
[0m12:50:31.159864 [debug] [Thread-1 (]: Finished running node model.data_pipeline.orders
[0m12:50:31.160527 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_customers_source
[0m12:50:31.161109 [info ] [Thread-1 (]: 7 of 10 START sql view model public.stg_customers_source ....................... [RUN]
[0m12:50:31.162957 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.orders, now model.data_pipeline.stg_customers_source)
[0m12:50:31.163613 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_customers_source
[0m12:50:31.166384 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_customers_source"
[0m12:50:31.167478 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_customers_source
[0m12:50:31.169796 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_customers_source"
[0m12:50:31.170333 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:50:31.170850 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: BEGIN
[0m12:50:31.171320 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.185412 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:50:31.185966 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:50:31.185966 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */

  create view "datamart"."public"."stg_customers_source__dbt_tmp"
    
    
  as (
    -- models/staging/stg_customers.sql

-- This staging model creates distinct customer records by joining
-- order data with a dedicated raw customer source for detailed information.

WITH distinct_customers AS (
    SELECT DISTINCT
        customer_id,
        MIN(order_date) AS first_order_date -- Capture first order date as a proxy for customer creation
    FROM
        "datamart"."public"."stg_raw_data" -- Referencing the staging raw_data model (for customer_id and order_date)
    WHERE
        customer_id IS NOT NULL
    GROUP BY
        customer_id
),

raw_customers AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        phone,
        address,
        signup_date -- Assuming signup_date is available in the raw customer source
    FROM
        "datamart"."raw"."customers_source" -- NEW: Referencing the dedicated raw customer source
)

SELECT
    CAST(dc.customer_id AS INTEGER) AS customer_id,
    CAST(rc.first_name AS VARCHAR) AS first_name,
    CAST(rc.last_name AS VARCHAR) AS last_name,
    CAST(rc.email AS VARCHAR) AS email,
    CAST(rc.phone AS VARCHAR) AS phone,
    CAST(rc.address AS TEXT) AS address,
    -- Prioritize signup_date from raw_customers if available, otherwise use first_order_date
    COALESCE(CAST(rc.signup_date AS TIMESTAMP), CAST(dc.first_order_date AS TIMESTAMP)) AS signup_date
FROM
    distinct_customers dc
LEFT JOIN
    raw_customers rc ON dc.customer_id = rc.customer_id
  );
[0m12:50:31.192953 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.007 seconds
[0m12:50:31.196532 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:50:31.196532 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
alter table "datamart"."public"."stg_customers_source__dbt_tmp" rename to "stg_customers_source"
[0m12:50:31.198543 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.199684 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:50:31.200229 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:50:31.200229 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: COMMIT
[0m12:50:31.208873 [debug] [Thread-1 (]: SQL status: COMMIT in 0.008 seconds
[0m12:50:31.210720 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_customers_source__dbt_backup"
[0m12:50:31.211321 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_customers_source"
[0m12:50:31.211948 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_customers_source"} */
drop view if exists "datamart"."public"."stg_customers_source__dbt_backup" cascade
[0m12:50:31.213616 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.214711 [debug] [Thread-1 (]: On model.data_pipeline.stg_customers_source: Close
[0m12:50:31.215252 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B8622A0>]}
[0m12:50:31.215252 [info ] [Thread-1 (]: 7 of 10 OK created sql view model public.stg_customers_source .................. [[32mCREATE VIEW[0m in 0.05s]
[0m12:50:31.217559 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_customers_source
[0m12:50:31.218063 [debug] [Thread-1 (]: Began running node model.data_pipeline.stg_orders
[0m12:50:31.218613 [info ] [Thread-1 (]: 8 of 10 START sql view model public.stg_orders ................................. [RUN]
[0m12:50:31.219170 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_customers_source, now model.data_pipeline.stg_orders)
[0m12:50:31.219816 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.stg_orders
[0m12:50:31.221752 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.stg_orders"
[0m12:50:31.223022 [debug] [Thread-1 (]: Began executing node model.data_pipeline.stg_orders
[0m12:50:31.227548 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.stg_orders"
[0m12:50:31.228947 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:50:31.228947 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: BEGIN
[0m12:50:31.229708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.262902 [debug] [Thread-1 (]: SQL status: BEGIN in 0.033 seconds
[0m12:50:31.262902 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:50:31.263573 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */

  create view "datamart"."public"."stg_orders__dbt_tmp"
    
    
  as (
    -- models/staging/stg_orders.sql

-- This staging model prepares the 'orders' data, combining information
-- from raw order transactions and payments.

WITH orders_data AS (
    SELECT
        srd.order_id,
        srd.customer_id,
        srd.order_date,
        srd.order_payment_status AS status, -- Mapping raw_data's payment_status to orders.status
        srd.total_amount,
        spm.payment_method,
        NULL AS shipping_address -- Placeholder: No source for shipping_address in current raw data
    FROM
        "datamart"."public"."stg_raw_data" srd -- Referencing the staging raw_data model
    LEFT JOIN
        "datamart"."public"."stg_payments" spm ON srd.order_id = spm.order_id
)

SELECT
    CAST(order_id AS INTEGER) AS order_id,
    CAST(customer_id AS INTEGER) AS customer_id,
    CAST(order_date AS TIMESTAMP) AS order_date,
    CAST(status AS VARCHAR) AS status,
    CAST(total_amount AS NUMERIC(10, 2)) AS total_amount,
    CAST(payment_method AS VARCHAR) AS payment_method,
    CAST(shipping_address AS TEXT) AS shipping_address
FROM
    orders_data
WHERE
    order_id IS NOT NULL -- Ensure order_id is not null
GROUP BY -- Grouping to handle potential multiple entries per order_id if joins result in duplicates
    order_id, customer_id, order_date, status, total_amount, payment_method, shipping_address
  );
[0m12:50:31.266998 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.003 seconds
[0m12:50:31.269341 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:50:31.269899 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
alter table "datamart"."public"."stg_orders__dbt_tmp" rename to "stg_orders"
[0m12:50:31.271020 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.272888 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:50:31.272888 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:50:31.273470 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: COMMIT
[0m12:50:31.275323 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:50:31.277417 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."stg_orders__dbt_backup"
[0m12:50:31.278023 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.stg_orders"
[0m12:50:31.278600 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.stg_orders"} */
drop view if exists "datamart"."public"."stg_orders__dbt_backup" cascade
[0m12:50:31.279719 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.280225 [debug] [Thread-1 (]: On model.data_pipeline.stg_orders: Close
[0m12:50:31.281535 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B900BC0>]}
[0m12:50:31.282299 [info ] [Thread-1 (]: 8 of 10 OK created sql view model public.stg_orders ............................ [[32mCREATE VIEW[0m in 0.06s]
[0m12:50:31.282708 [debug] [Thread-1 (]: Finished running node model.data_pipeline.stg_orders
[0m12:50:31.283213 [debug] [Thread-1 (]: Began running node model.data_pipeline.customers
[0m12:50:31.283755 [info ] [Thread-1 (]: 9 of 10 START sql view model public.customers .................................. [RUN]
[0m12:50:31.284333 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.stg_orders, now model.data_pipeline.customers)
[0m12:50:31.284924 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.customers
[0m12:50:31.287822 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.customers"
[0m12:50:31.288948 [debug] [Thread-1 (]: Began executing node model.data_pipeline.customers
[0m12:50:31.291360 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.customers"
[0m12:50:31.292455 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:50:31.292455 [debug] [Thread-1 (]: On model.data_pipeline.customers: BEGIN
[0m12:50:31.293026 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.307174 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:50:31.307866 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:50:31.308450 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */

  create view "datamart"."public"."customers__dbt_tmp"
    
    
  as (
    -- models/marts/customers.sql
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    phone,
    address,
    signup_date AS created_at
FROM "datamart"."public"."stg_customers_source"
  );
[0m12:50:31.310192 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.002 seconds
[0m12:50:31.313683 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:50:31.314335 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */
alter table "datamart"."public"."customers__dbt_tmp" rename to "customers"
[0m12:50:31.315391 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.317079 [debug] [Thread-1 (]: On model.data_pipeline.customers: COMMIT
[0m12:50:31.317079 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:50:31.317675 [debug] [Thread-1 (]: On model.data_pipeline.customers: COMMIT
[0m12:50:31.319440 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:50:31.321549 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."customers__dbt_backup"
[0m12:50:31.321549 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.customers"
[0m12:50:31.322462 [debug] [Thread-1 (]: On model.data_pipeline.customers: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.customers"} */
drop view if exists "datamart"."public"."customers__dbt_backup" cascade
[0m12:50:31.323558 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.324721 [debug] [Thread-1 (]: On model.data_pipeline.customers: Close
[0m12:50:31.325302 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3AC08D10>]}
[0m12:50:31.325874 [info ] [Thread-1 (]: 9 of 10 OK created sql view model public.customers ............................. [[32mCREATE VIEW[0m in 0.04s]
[0m12:50:31.326963 [debug] [Thread-1 (]: Finished running node model.data_pipeline.customers
[0m12:50:31.327510 [debug] [Thread-1 (]: Began running node model.data_pipeline.raw_to_normalized
[0m12:50:31.328258 [info ] [Thread-1 (]: 10 of 10 START sql view model public.raw_to_normalized ......................... [RUN]
[0m12:50:31.328258 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.data_pipeline.customers, now model.data_pipeline.raw_to_normalized)
[0m12:50:31.328820 [debug] [Thread-1 (]: Began compiling node model.data_pipeline.raw_to_normalized
[0m12:50:31.332717 [debug] [Thread-1 (]: Writing injected SQL for node "model.data_pipeline.raw_to_normalized"
[0m12:50:31.333857 [debug] [Thread-1 (]: Began executing node model.data_pipeline.raw_to_normalized
[0m12:50:31.338360 [debug] [Thread-1 (]: Writing runtime sql for node "model.data_pipeline.raw_to_normalized"
[0m12:50:31.338950 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:50:31.339521 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: BEGIN
[0m12:50:31.339521 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:31.353638 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:50:31.354208 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:50:31.354822 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */

  create view "datamart"."public"."raw_to_normalized__dbt_tmp"
    
    
  as (
    -- models/raw_to_normalized.sql

-- This model combines data from the staged customer, order, product, and payment tables
-- to create a comprehensive, normalized view of the e-commerce data.
-- It serves as the primary data source for analytics and reporting.
-- models/raw_to_normalized.sql

WITH orders AS (
    SELECT * FROM "datamart"."public"."orders"
),
customers AS (
    SELECT * FROM "datamart"."public"."customers"
),
products AS (
    SELECT * FROM "datamart"."public"."products"
),
payments AS (
    SELECT * FROM "datamart"."public"."stg_payments"
)
SELECT
    o.order_id,
    o.order_date,
    o.total_amount,
    o.status,
    o.payment_method,
    o.shipping_address,
    c.customer_id,
    c.first_name,
    c.last_name,
    c.email,
    c.phone,
    c.address AS customer_address,
    r.product_id,
    p.product_name AS product_name,
    p.category,
    p.price
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id
LEFT JOIN "datamart"."public"."stg_raw_data" r ON o.order_id = r.order_id
LEFT JOIN products p ON r.product_id = p.product_id
  );
[0m12:50:31.359373 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.004 seconds
[0m12:50:31.362838 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:50:31.362838 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */
alter table "datamart"."public"."raw_to_normalized__dbt_tmp" rename to "raw_to_normalized"
[0m12:50:31.364829 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:50:31.366085 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: COMMIT
[0m12:50:31.366641 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:50:31.366641 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: COMMIT
[0m12:50:31.368863 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:50:31.371024 [debug] [Thread-1 (]: Applying DROP to: "datamart"."public"."raw_to_normalized__dbt_backup"
[0m12:50:31.371587 [debug] [Thread-1 (]: Using postgres connection "model.data_pipeline.raw_to_normalized"
[0m12:50:31.371587 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "model.data_pipeline.raw_to_normalized"} */
drop view if exists "datamart"."public"."raw_to_normalized__dbt_backup" cascade
[0m12:50:31.372691 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:50:31.374533 [debug] [Thread-1 (]: On model.data_pipeline.raw_to_normalized: Close
[0m12:50:31.375095 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f7a6dec-48ea-4470-9255-a3dc7c3944ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B8F8830>]}
[0m12:50:31.375723 [info ] [Thread-1 (]: 10 of 10 OK created sql view model public.raw_to_normalized .................... [[32mCREATE VIEW[0m in 0.05s]
[0m12:50:31.376589 [debug] [Thread-1 (]: Finished running node model.data_pipeline.raw_to_normalized
[0m12:50:31.377655 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:31.378226 [debug] [MainThread]: On master: BEGIN
[0m12:50:31.378226 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:50:31.391305 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m12:50:31.391956 [debug] [MainThread]: On master: COMMIT
[0m12:50:31.392545 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:31.393109 [debug] [MainThread]: On master: COMMIT
[0m12:50:31.394299 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:50:31.394915 [debug] [MainThread]: On master: Close
[0m12:50:31.395502 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:50:31.395502 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m12:50:31.396518 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:50:31.396863 [debug] [MainThread]: Connection 'model.data_pipeline.raw_to_normalized' was properly closed.
[0m12:50:31.396863 [info ] [MainThread]: 
[0m12:50:31.398500 [info ] [MainThread]: Finished running 10 view models in 0 hours 0 minutes and 0.84 seconds (0.84s).
[0m12:50:31.399971 [debug] [MainThread]: Command end result
[0m12:50:31.423291 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:50:31.425782 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:50:31.430583 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:50:31.430583 [info ] [MainThread]: 
[0m12:50:31.431167 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:50:31.431913 [info ] [MainThread]: 
[0m12:50:31.431913 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=0 SKIP=0 TOTAL=10
[0m12:50:31.433246 [debug] [MainThread]: Command `dbt run` succeeded at 12:50:31.433246 after 1.93 seconds
[0m12:50:31.433246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B38E16930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B364320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024B3B85E6C0>]}
[0m12:50:31.433868 [debug] [MainThread]: Flushing usage events
[0m12:50:31.875272 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:50:56.106854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB3C49A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB35934A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB3592F30>]}


============================== 12:50:56.111474 | 6c1f4a7e-545b-4d9a-906d-8fc2c129887c ==============================
[0m12:50:56.111474 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:50:56.113194 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:50:56.298414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB40AB620>]}
[0m12:50:56.343213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB40C3320>]}
[0m12:50:56.344309 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:50:56.508159 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:50:56.661578 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:50:56.661578 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:50:56.693309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB57D4380>]}
[0m12:50:56.766977 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:50:56.802430 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:50:56.845370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB5CF6900>]}
[0m12:50:56.846032 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:50:56.848050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB5BA9100>]}
[0m12:50:56.849757 [info ] [MainThread]: 
[0m12:50:56.850350 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:50:56.850350 [info ] [MainThread]: 
[0m12:50:56.850989 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:50:56.854352 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:50:56.904985 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:50:56.904985 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:50:56.905595 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:50:56.923467 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m12:50:56.924060 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:50:56.924060 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:50:56.927551 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.003 seconds
[0m12:50:56.929355 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:50:56.931270 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:50:56.936607 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:56.937185 [debug] [MainThread]: On master: BEGIN
[0m12:50:56.937185 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:50:56.949230 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m12:50:56.949817 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:56.949817 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:50:56.954302 [debug] [MainThread]: SQL status: SELECT 16 in 0.004 seconds
[0m12:50:56.956607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c1f4a7e-545b-4d9a-906d-8fc2c129887c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB56758E0>]}
[0m12:50:56.957187 [debug] [MainThread]: On master: ROLLBACK
[0m12:50:56.958354 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:56.958911 [debug] [MainThread]: On master: BEGIN
[0m12:50:56.960133 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:50:56.960133 [debug] [MainThread]: On master: COMMIT
[0m12:50:56.960699 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:56.960699 [debug] [MainThread]: On master: COMMIT
[0m12:50:56.961853 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:50:56.961853 [debug] [MainThread]: On master: Close
[0m12:50:56.967729 [debug] [Thread-1 (]: Began running node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:50:56.968280 [info ] [Thread-1 (]: 1 of 23 START test not_null_reviews_product_id ................................. [RUN]
[0m12:50:56.968846 [debug] [Thread-1 (]: Acquiring new postgres connection 'test.data_pipeline.not_null_reviews_product_id.8432b9024c'
[0m12:50:56.969420 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:50:56.984167 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:50:56.985325 [debug] [Thread-1 (]: Began executing node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:50:57.003823 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:50:57.004923 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:50:57.005443 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: BEGIN
[0m12:50:57.005443 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:50:57.019854 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:50:57.019854 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:50:57.020429 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.not_null_reviews_product_id.8432b9024c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."public"."reviews"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.023327 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.003 seconds
[0m12:50:57.027613 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: ROLLBACK
[0m12:50:57.028877 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: Close
[0m12:50:57.029450 [info ] [Thread-1 (]: 1 of 23 PASS not_null_reviews_product_id ....................................... [[32mPASS[0m in 0.06s]
[0m12:50:57.032751 [debug] [Thread-1 (]: Finished running node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:50:57.033260 [debug] [Thread-1 (]: Began running node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:50:57.033260 [info ] [Thread-1 (]: 2 of 23 START test relationships_reviews_product_id__customer_id__ref_customers_  [RUN]
[0m12:50:57.034447 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.not_null_reviews_product_id.8432b9024c, now test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449)
[0m12:50:57.034447 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:50:57.041921 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:50:57.043070 [debug] [Thread-1 (]: Began executing node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:50:57.044739 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:50:57.046533 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:50:57.046533 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: BEGIN
[0m12:50:57.046533 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.064100 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:50:57.065314 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:50:57.065864 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select product_id as from_field
    from "datamart"."public"."reviews"
    where product_id is not null
),

parent as (
    select customer_id as to_field
    from "datamart"."public"."customers"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.071432 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.005 seconds
[0m12:50:57.073196 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: ROLLBACK
[0m12:50:57.074946 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: Close
[0m12:50:57.075964 [info ] [Thread-1 (]: 2 of 23 PASS relationships_reviews_product_id__customer_id__ref_customers_ ..... [[32mPASS[0m in 0.04s]
[0m12:50:57.077193 [debug] [Thread-1 (]: Finished running node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:50:57.078318 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:50:57.078875 [info ] [Thread-1 (]: 3 of 23 START test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [RUN]
[0m12:50:57.079797 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449, now test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6)
[0m12:50:57.080504 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:50:57.092541 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:50:57.094860 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:50:57.097508 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:50:57.099052 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:50:57.099736 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: BEGIN
[0m12:50:57.100376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.117114 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:50:57.117680 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:50:57.118347 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_method

)

select *
from all_values
where value_field not in (
    'Credit Card','PayPal','Bank Transfer'
)



  
  
      
    ) dbt_internal_test
[0m12:50:57.120093 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.121889 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: ROLLBACK
[0m12:50:57.122992 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: Close
[0m12:50:57.124141 [info ] [Thread-1 (]: 3 of 23 PASS source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [[32mPASS[0m in 0.04s]
[0m12:50:57.126198 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:50:57.126198 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m12:50:57.126769 [info ] [Thread-1 (]: 4 of 23 START test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending  [RUN]
[0m12:50:57.127895 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6, now test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700)
[0m12:50:57.127895 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m12:50:57.132440 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m12:50:57.134116 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m12:50:57.135789 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m12:50:57.136357 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m12:50:57.136357 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: BEGIN
[0m12:50:57.136842 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.158673 [debug] [Thread-1 (]: SQL status: BEGIN in 0.022 seconds
[0m12:50:57.159907 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"
[0m12:50:57.160520 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        transaction_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by transaction_payment_status

)

select *
from all_values
where value_field not in (
    'Completed','Failed','Pending'
)



  
  
      
    ) dbt_internal_test
[0m12:50:57.162850 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.164621 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: ROLLBACK
[0m12:50:57.165851 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700: Close
[0m12:50:57.166416 [error] [Thread-1 (]: 4 of 23 FAIL 1 source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending  [[31mFAIL 1[0m in 0.04s]
[0m12:50:57.168023 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700
[0m12:50:57.168678 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:50:57.169217 [info ] [Thread-1 (]: 5 of 23 START test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [RUN]
[0m12:50:57.169783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending.3948e38700, now test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e)
[0m12:50:57.169783 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:50:57.174511 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:50:57.175083 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:50:57.177371 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:50:57.178534 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:50:57.179543 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: BEGIN
[0m12:50:57.179543 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.198102 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m12:50:57.198628 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:50:57.198628 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        order_level_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."raw_data"
    group by order_level_payment_status

)

select *
from all_values
where value_field not in (
    'Paid','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m12:50:57.201023 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.202230 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: ROLLBACK
[0m12:50:57.203360 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: Close
[0m12:50:57.204478 [info ] [Thread-1 (]: 5 of 23 PASS source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [[32mPASS[0m in 0.03s]
[0m12:50:57.206926 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:50:57.207433 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:50:57.207433 [info ] [Thread-1 (]: 6 of 23 START test source_not_null_raw_customers_source_customer_id ............ [RUN]
[0m12:50:57.208591 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e, now test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556)
[0m12:50:57.209153 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:50:57.213463 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:50:57.215243 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:50:57.216463 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:50:57.217616 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:50:57.218193 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: BEGIN
[0m12:50:57.218850 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.233891 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:57.233891 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:50:57.234464 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."customers_source"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.235638 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.236800 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: ROLLBACK
[0m12:50:57.237862 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: Close
[0m12:50:57.239085 [info ] [Thread-1 (]: 6 of 23 PASS source_not_null_raw_customers_source_customer_id .................. [[32mPASS[0m in 0.03s]
[0m12:50:57.240291 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:50:57.240845 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:50:57.241438 [info ] [Thread-1 (]: 7 of 23 START test source_not_null_raw_customers_source_signup_date ............ [RUN]
[0m12:50:57.242017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556, now test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f)
[0m12:50:57.242573 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:50:57.249692 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:50:57.251491 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:50:57.253761 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:50:57.254320 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:50:57.255105 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: BEGIN
[0m12:50:57.255798 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.272463 [debug] [Thread-1 (]: SQL status: BEGIN in 0.017 seconds
[0m12:50:57.272976 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:50:57.272976 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from "datamart"."raw"."customers_source"
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.275212 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.277507 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: ROLLBACK
[0m12:50:57.278790 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: Close
[0m12:50:57.280079 [info ] [Thread-1 (]: 7 of 23 PASS source_not_null_raw_customers_source_signup_date .................. [[32mPASS[0m in 0.04s]
[0m12:50:57.281253 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:50:57.281989 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:50:57.281989 [info ] [Thread-1 (]: 8 of 23 START test source_not_null_raw_payments_order_id ....................... [RUN]
[0m12:50:57.282672 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f, now test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9)
[0m12:50:57.283249 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:50:57.287874 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:50:57.288989 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:50:57.290772 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:50:57.291378 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:50:57.291963 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: BEGIN
[0m12:50:57.291963 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.328764 [debug] [Thread-1 (]: SQL status: BEGIN in 0.037 seconds
[0m12:50:57.329388 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:50:57.330651 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."payments"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.333250 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.334952 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: ROLLBACK
[0m12:50:57.335499 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: Close
[0m12:50:57.337098 [info ] [Thread-1 (]: 8 of 23 PASS source_not_null_raw_payments_order_id ............................. [[32mPASS[0m in 0.05s]
[0m12:50:57.339216 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:50:57.339626 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:50:57.340841 [info ] [Thread-1 (]: 9 of 23 START test source_not_null_raw_payments_payment_id ..................... [RUN]
[0m12:50:57.341399 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9, now test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5)
[0m12:50:57.342575 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:50:57.348397 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:50:57.349647 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:50:57.425051 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:50:57.427875 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:50:57.428479 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: BEGIN
[0m12:50:57.429198 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.450057 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m12:50:57.451260 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:50:57.451831 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select payment_id
from "datamart"."raw"."payments"
where payment_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.454099 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.455871 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: ROLLBACK
[0m12:50:57.457614 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: Close
[0m12:50:57.458761 [info ] [Thread-1 (]: 9 of 23 PASS source_not_null_raw_payments_payment_id ........................... [[32mPASS[0m in 0.12s]
[0m12:50:57.459996 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:50:57.460505 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:50:57.461861 [info ] [Thread-1 (]: 10 of 23 START test source_not_null_raw_products_category ...................... [RUN]
[0m12:50:57.462438 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5, now test.data_pipeline.source_not_null_raw_products_category.9265557239)
[0m12:50:57.463007 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:50:57.468170 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:50:57.469863 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:50:57.472167 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:50:57.472746 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:50:57.473308 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: BEGIN
[0m12:50:57.473308 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.491284 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m12:50:57.491789 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:50:57.492371 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_category.9265557239"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select category
from "datamart"."raw"."products"
where category is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.494088 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.496344 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: ROLLBACK
[0m12:50:57.497713 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: Close
[0m12:50:57.498896 [info ] [Thread-1 (]: 10 of 23 PASS source_not_null_raw_products_category ............................ [[32mPASS[0m in 0.04s]
[0m12:50:57.499511 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:50:57.500112 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:50:57.500679 [info ] [Thread-1 (]: 11 of 23 START test source_not_null_raw_products_price ......................... [RUN]
[0m12:50:57.501236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_category.9265557239, now test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b)
[0m12:50:57.502325 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:50:57.508761 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:50:57.510032 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:50:57.513410 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:50:57.515306 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:50:57.515903 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: BEGIN
[0m12:50:57.516481 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.534966 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m12:50:57.535536 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:50:57.535536 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select price
from "datamart"."raw"."products"
where price is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.537204 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.538226 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: ROLLBACK
[0m12:50:57.539869 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: Close
[0m12:50:57.540456 [info ] [Thread-1 (]: 11 of 23 PASS source_not_null_raw_products_price ............................... [[32mPASS[0m in 0.04s]
[0m12:50:57.542991 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:50:57.542991 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:50:57.542991 [info ] [Thread-1 (]: 12 of 23 START test source_not_null_raw_products_product_id .................... [RUN]
[0m12:50:57.544546 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b, now test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae)
[0m12:50:57.545215 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:50:57.550276 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:50:57.552570 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:50:57.554277 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:50:57.555332 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:50:57.555332 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: BEGIN
[0m12:50:57.555839 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.583698 [debug] [Thread-1 (]: SQL status: BEGIN in 0.028 seconds
[0m12:50:57.584252 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:50:57.584911 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."products"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.586658 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.588108 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: ROLLBACK
[0m12:50:57.588994 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: Close
[0m12:50:57.589670 [info ] [Thread-1 (]: 12 of 23 PASS source_not_null_raw_products_product_id .......................... [[32mPASS[0m in 0.05s]
[0m12:50:57.590907 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:50:57.591593 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:50:57.592782 [info ] [Thread-1 (]: 13 of 23 START test source_not_null_raw_products_product_name .................. [RUN]
[0m12:50:57.593353 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae, now test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51)
[0m12:50:57.593353 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:50:57.596177 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:50:57.597835 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:50:57.602336 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:50:57.603428 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:50:57.604562 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: BEGIN
[0m12:50:57.605103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.638549 [debug] [Thread-1 (]: SQL status: BEGIN in 0.033 seconds
[0m12:50:57.639611 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:50:57.640680 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_name
from "datamart"."raw"."products"
where product_name is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.642886 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.645251 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: ROLLBACK
[0m12:50:57.647002 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: Close
[0m12:50:57.647965 [info ] [Thread-1 (]: 13 of 23 PASS source_not_null_raw_products_product_name ........................ [[32mPASS[0m in 0.05s]
[0m12:50:57.649872 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:50:57.650448 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:50:57.651559 [info ] [Thread-1 (]: 14 of 23 START test source_not_null_raw_raw_data_customer_id ................... [RUN]
[0m12:50:57.652138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51, now test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3)
[0m12:50:57.652682 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:50:57.657497 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:50:57.658681 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:50:57.660506 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:50:57.661069 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:50:57.661625 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: BEGIN
[0m12:50:57.661625 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.694451 [debug] [Thread-1 (]: SQL status: BEGIN in 0.032 seconds
[0m12:50:57.694955 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:50:57.694955 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."raw_data"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.696699 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.698521 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: ROLLBACK
[0m12:50:57.699660 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: Close
[0m12:50:57.700769 [info ] [Thread-1 (]: 14 of 23 PASS source_not_null_raw_raw_data_customer_id ......................... [[32mPASS[0m in 0.05s]
[0m12:50:57.702340 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:50:57.702340 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:50:57.702995 [info ] [Thread-1 (]: 15 of 23 START test source_not_null_raw_raw_data_order_date .................... [RUN]
[0m12:50:57.703555 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3, now test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc)
[0m12:50:57.703555 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:50:57.707739 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:50:57.708420 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:50:57.709998 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:50:57.710715 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:50:57.711388 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: BEGIN
[0m12:50:57.711388 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.724906 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m12:50:57.725458 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:50:57.725458 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_date
from "datamart"."raw"."raw_data"
where order_date is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.727226 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.728338 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: ROLLBACK
[0m12:50:57.728882 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: Close
[0m12:50:57.729674 [info ] [Thread-1 (]: 15 of 23 PASS source_not_null_raw_raw_data_order_date .......................... [[32mPASS[0m in 0.03s]
[0m12:50:57.731480 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:50:57.731898 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:50:57.732404 [info ] [Thread-1 (]: 16 of 23 START test source_not_null_raw_raw_data_order_id ...................... [RUN]
[0m12:50:57.733412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc, now test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6)
[0m12:50:57.733918 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:50:57.737776 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:50:57.739911 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:50:57.742201 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:50:57.742990 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:50:57.743649 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: BEGIN
[0m12:50:57.744160 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.760157 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:50:57.760699 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:50:57.760699 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."raw_data"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.762432 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.763688 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: ROLLBACK
[0m12:50:57.764457 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: Close
[0m12:50:57.765628 [info ] [Thread-1 (]: 16 of 23 PASS source_not_null_raw_raw_data_order_id ............................ [[32mPASS[0m in 0.03s]
[0m12:50:57.765628 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:50:57.766925 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:50:57.767485 [info ] [Thread-1 (]: 17 of 23 START test source_not_null_raw_raw_data_product_id .................... [RUN]
[0m12:50:57.768029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6, now test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e)
[0m12:50:57.768623 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:50:57.772108 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:50:57.773234 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:50:57.776533 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:50:57.777083 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:50:57.777621 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: BEGIN
[0m12:50:57.778188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.790948 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m12:50:57.791499 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:50:57.791499 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."raw_data"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.793636 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.794589 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: ROLLBACK
[0m12:50:57.795229 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: Close
[0m12:50:57.797322 [info ] [Thread-1 (]: 17 of 23 PASS source_not_null_raw_raw_data_product_id .......................... [[32mPASS[0m in 0.03s]
[0m12:50:57.798095 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:50:57.798610 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:50:57.798961 [info ] [Thread-1 (]: 18 of 23 START test source_not_null_raw_raw_data_total_amount .................. [RUN]
[0m12:50:57.799755 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e, now test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e)
[0m12:50:57.800072 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:50:57.803680 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:50:57.804733 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:50:57.807389 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:50:57.808456 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:50:57.808456 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: BEGIN
[0m12:50:57.809455 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.824448 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:57.824957 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:50:57.824957 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_amount
from "datamart"."raw"."raw_data"
where total_amount is null



  
  
      
    ) dbt_internal_test
[0m12:50:57.826565 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:57.827632 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: ROLLBACK
[0m12:50:57.828161 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: Close
[0m12:50:57.829559 [info ] [Thread-1 (]: 18 of 23 PASS source_not_null_raw_raw_data_total_amount ........................ [[32mPASS[0m in 0.03s]
[0m12:50:57.830407 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:50:57.831162 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:50:57.831162 [info ] [Thread-1 (]: 19 of 23 START test source_unique_raw_customers_source_customer_id ............. [RUN]
[0m12:50:57.831847 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e, now test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c)
[0m12:50:57.832431 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:50:57.837345 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:50:57.838685 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:50:57.840944 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:50:57.841336 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:50:57.841887 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: BEGIN
[0m12:50:57.842426 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.884874 [debug] [Thread-1 (]: SQL status: BEGIN in 0.043 seconds
[0m12:50:57.886117 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:50:57.886117 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:50:57.888902 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.891850 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: ROLLBACK
[0m12:50:57.893694 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: Close
[0m12:50:57.895423 [info ] [Thread-1 (]: 19 of 23 PASS source_unique_raw_customers_source_customer_id ................... [[32mPASS[0m in 0.06s]
[0m12:50:57.896757 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:50:57.897268 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:50:57.898267 [info ] [Thread-1 (]: 20 of 23 START test source_unique_raw_customers_source_email ................... [RUN]
[0m12:50:57.899824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c, now test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b)
[0m12:50:57.900583 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:50:57.907926 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:50:57.909589 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:50:57.914296 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:50:57.916188 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:50:57.917136 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: BEGIN
[0m12:50:57.917686 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.939252 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m12:50:57.940204 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:50:57.940774 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    email as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where email is not null
group by email
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:50:57.943683 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.945350 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: ROLLBACK
[0m12:50:57.946235 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: Close
[0m12:50:57.947505 [info ] [Thread-1 (]: 20 of 23 PASS source_unique_raw_customers_source_email ......................... [[32mPASS[0m in 0.05s]
[0m12:50:57.948212 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:50:57.948893 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:50:57.948893 [info ] [Thread-1 (]: 21 of 23 START test source_unique_raw_payments_payment_id ...................... [RUN]
[0m12:50:57.949981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b, now test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533)
[0m12:50:57.949981 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:50:57.953491 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:50:57.954078 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:50:57.956693 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:50:57.957919 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:50:57.957919 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: BEGIN
[0m12:50:57.957919 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:57.973242 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:57.973865 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:50:57.973865 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."payments"
where payment_id is not null
group by payment_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:50:57.976234 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:57.979013 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: ROLLBACK
[0m12:50:57.980485 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: Close
[0m12:50:57.981865 [info ] [Thread-1 (]: 21 of 23 PASS source_unique_raw_payments_payment_id ............................ [[32mPASS[0m in 0.03s]
[0m12:50:57.983080 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:50:57.983650 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:50:57.984818 [info ] [Thread-1 (]: 22 of 23 START test source_unique_raw_products_product_id ...................... [RUN]
[0m12:50:57.985400 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533, now test.data_pipeline.source_unique_raw_products_product_id.518dac90ba)
[0m12:50:57.986032 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:50:57.992562 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:50:57.993960 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:50:57.996546 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:50:57.997635 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:50:57.998148 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: BEGIN
[0m12:50:57.998487 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:58.010926 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m12:50:58.011499 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:50:58.012068 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."products"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:50:58.013626 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:50:58.015370 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: ROLLBACK
[0m12:50:58.015940 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: Close
[0m12:50:58.017118 [info ] [Thread-1 (]: 22 of 23 PASS source_unique_raw_products_product_id ............................ [[32mPASS[0m in 0.03s]
[0m12:50:58.018276 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:50:58.018834 [debug] [Thread-1 (]: Began running node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:50:58.019379 [info ] [Thread-1 (]: 23 of 23 START test unique_reviews_product_id .................................. [RUN]
[0m12:50:58.019964 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_products_product_id.518dac90ba, now test.data_pipeline.unique_reviews_product_id.d6f6d62f1d)
[0m12:50:58.020496 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:50:58.023685 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:50:58.024783 [debug] [Thread-1 (]: Began executing node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:50:58.025890 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:50:58.026985 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:50:58.026985 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: BEGIN
[0m12:50:58.027545 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:50:58.042132 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:50:58.042691 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:50:58.042691 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."public"."reviews"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:50:58.044510 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:50:58.045581 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: ROLLBACK
[0m12:50:58.046144 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: Close
[0m12:50:58.048044 [info ] [Thread-1 (]: 23 of 23 PASS unique_reviews_product_id ........................................ [[32mPASS[0m in 0.03s]
[0m12:50:58.050240 [debug] [Thread-1 (]: Finished running node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:50:58.052212 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:58.052758 [debug] [MainThread]: On master: BEGIN
[0m12:50:58.053319 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:50:58.071108 [debug] [MainThread]: SQL status: BEGIN in 0.017 seconds
[0m12:50:58.071108 [debug] [MainThread]: On master: COMMIT
[0m12:50:58.071713 [debug] [MainThread]: Using postgres connection "master"
[0m12:50:58.071713 [debug] [MainThread]: On master: COMMIT
[0m12:50:58.072912 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m12:50:58.073451 [debug] [MainThread]: On master: Close
[0m12:50:58.075274 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:50:58.075864 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:50:58.076425 [debug] [MainThread]: Connection 'test.data_pipeline.unique_reviews_product_id.d6f6d62f1d' was properly closed.
[0m12:50:58.077008 [info ] [MainThread]: 
[0m12:50:58.077559 [info ] [MainThread]: Finished running 23 data tests in 0 hours 0 minutes and 1.23 seconds (1.23s).
[0m12:50:58.080987 [debug] [MainThread]: Command end result
[0m12:50:58.111737 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:50:58.116905 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:50:58.128378 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:50:58.128962 [info ] [MainThread]: 
[0m12:50:58.129539 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:50:58.130768 [info ] [MainThread]: 
[0m12:50:58.131926 [error] [MainThread]: [31mFailure in test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending (../../models\sources.yml)[0m
[0m12:50:58.132473 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m12:50:58.133016 [info ] [MainThread]: 
[0m12:50:58.133558 [info ] [MainThread]:   compiled code at target\compiled\data_pipeline\../../models\sources.yml\source_accepted_values_raw_pay_b9c989267c20c7ce6e61eefb643b9c68.sql
[0m12:50:58.134100 [info ] [MainThread]: 
[0m12:50:58.134648 [info ] [MainThread]: Done. PASS=22 WARN=0 ERROR=1 SKIP=0 TOTAL=23
[0m12:50:58.136345 [debug] [MainThread]: Command `dbt test` failed at 12:50:58.135802 after 2.20 seconds
[0m12:50:58.136907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB3663380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB62EBF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ABB3E33DA0>]}
[0m12:50:58.137919 [debug] [MainThread]: Flushing usage events
[0m12:50:58.518945 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:54:22.983397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C864CA2360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C866D80530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C867757FB0>]}


============================== 12:54:22.987306 | 87b215eb-02fe-4b41-b9d1-18de5ee187f4 ==============================
[0m12:54:22.987306 [info ] [MainThread]: Running with dbt=1.9.6
[0m12:54:22.987878 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:54:23.126729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C864D6F320>]}
[0m12:54:23.177055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C867CA4B60>]}
[0m12:54:23.178157 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m12:54:23.373617 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m12:54:23.526671 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:54:23.528420 [debug] [MainThread]: Partial parsing: updated file: data_pipeline://../../models\sources.yml
[0m12:54:24.018046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C869BF5640>]}
[0m12:54:24.158286 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:54:24.185065 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:54:24.233751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C869CD2390>]}
[0m12:54:24.234316 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m12:54:24.234915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C869A26AE0>]}
[0m12:54:24.239062 [info ] [MainThread]: 
[0m12:54:24.239851 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:54:24.240779 [info ] [MainThread]: 
[0m12:54:24.241876 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:54:24.248798 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m12:54:24.323248 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:54:24.323248 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m12:54:24.323877 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:24.354516 [debug] [ThreadPool]: SQL status: BEGIN in 0.031 seconds
[0m12:54:24.355068 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m12:54:24.355613 [debug] [ThreadPool]: On list_datamart_public: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart_public"} */
select
      'datamart' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'public'
    union all
    select
      'datamart' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'public'
  
[0m12:54:24.359335 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.003 seconds
[0m12:54:24.360489 [debug] [ThreadPool]: On list_datamart_public: ROLLBACK
[0m12:54:24.361016 [debug] [ThreadPool]: On list_datamart_public: Close
[0m12:54:24.366955 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:24.367504 [debug] [MainThread]: On master: BEGIN
[0m12:54:24.367504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:54:24.378262 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m12:54:24.378811 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:24.379352 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v', 'm')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m12:54:24.383970 [debug] [MainThread]: SQL status: SELECT 16 in 0.004 seconds
[0m12:54:24.386758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87b215eb-02fe-4b41-b9d1-18de5ee187f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C869B86240>]}
[0m12:54:24.386758 [debug] [MainThread]: On master: ROLLBACK
[0m12:54:24.387884 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:24.387884 [debug] [MainThread]: On master: BEGIN
[0m12:54:24.389563 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:54:24.389563 [debug] [MainThread]: On master: COMMIT
[0m12:54:24.390108 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:24.390668 [debug] [MainThread]: On master: COMMIT
[0m12:54:24.391335 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:54:24.391912 [debug] [MainThread]: On master: Close
[0m12:54:24.398950 [debug] [Thread-1 (]: Began running node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:54:24.398950 [info ] [Thread-1 (]: 1 of 23 START test not_null_reviews_product_id ................................. [RUN]
[0m12:54:24.400094 [debug] [Thread-1 (]: Acquiring new postgres connection 'test.data_pipeline.not_null_reviews_product_id.8432b9024c'
[0m12:54:24.400636 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:54:24.413434 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:54:24.414992 [debug] [Thread-1 (]: Began executing node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:54:24.433606 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:54:24.435493 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:54:24.436077 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: BEGIN
[0m12:54:24.436656 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:54:24.452300 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:24.452847 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.not_null_reviews_product_id.8432b9024c"
[0m12:54:24.453428 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.not_null_reviews_product_id.8432b9024c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."public"."reviews"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.455272 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.459311 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: ROLLBACK
[0m12:54:24.460407 [debug] [Thread-1 (]: On test.data_pipeline.not_null_reviews_product_id.8432b9024c: Close
[0m12:54:24.461551 [info ] [Thread-1 (]: 1 of 23 PASS not_null_reviews_product_id ....................................... [[32mPASS[0m in 0.06s]
[0m12:54:24.462870 [debug] [Thread-1 (]: Finished running node test.data_pipeline.not_null_reviews_product_id.8432b9024c
[0m12:54:24.463770 [debug] [Thread-1 (]: Began running node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:54:24.464276 [info ] [Thread-1 (]: 2 of 23 START test relationships_reviews_product_id__customer_id__ref_customers_  [RUN]
[0m12:54:24.465413 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.not_null_reviews_product_id.8432b9024c, now test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449)
[0m12:54:24.465976 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:54:24.472621 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:54:24.473185 [debug] [Thread-1 (]: Began executing node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:54:24.475551 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:54:24.476682 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:54:24.477254 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: BEGIN
[0m12:54:24.477792 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.492406 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:54:24.493633 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"
[0m12:54:24.494217 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with child as (
    select product_id as from_field
    from "datamart"."public"."reviews"
    where product_id is not null
),

parent as (
    select customer_id as to_field
    from "datamart"."public"."customers"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.497857 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:24.498842 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: ROLLBACK
[0m12:54:24.499898 [debug] [Thread-1 (]: On test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449: Close
[0m12:54:24.501016 [info ] [Thread-1 (]: 2 of 23 PASS relationships_reviews_product_id__customer_id__ref_customers_ ..... [[32mPASS[0m in 0.04s]
[0m12:54:24.501741 [debug] [Thread-1 (]: Finished running node test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449
[0m12:54:24.502320 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:54:24.502320 [info ] [Thread-1 (]: 3 of 23 START test source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [RUN]
[0m12:54:24.503441 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.relationships_reviews_product_id__customer_id__ref_customers_.9f572b1449, now test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6)
[0m12:54:24.503996 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:54:24.508344 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:54:24.508906 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:54:24.510541 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:54:24.511648 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:54:24.511648 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: BEGIN
[0m12:54:24.512192 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.528645 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:24.529229 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"
[0m12:54:24.529229 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by payment_method

)

select *
from all_values
where value_field not in (
    'Credit Card','PayPal','Bank Transfer'
)



  
  
      
    ) dbt_internal_test
[0m12:54:24.531788 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:24.533545 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: ROLLBACK
[0m12:54:24.535251 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6: Close
[0m12:54:24.535822 [info ] [Thread-1 (]: 3 of 23 PASS source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer  [[32mPASS[0m in 0.03s]
[0m12:54:24.537393 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6
[0m12:54:24.537936 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca
[0m12:54:24.538571 [info ] [Thread-1 (]: 4 of 23 START test source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded  [RUN]
[0m12:54:24.540522 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_payment_method__Credit_Card__PayPal__Bank_Transfer.52bf7791a6, now test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca)
[0m12:54:24.540522 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca
[0m12:54:24.544166 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca"
[0m12:54:24.545303 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca
[0m12:54:24.546991 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca"
[0m12:54:24.547675 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca"
[0m12:54:24.547675 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca: BEGIN
[0m12:54:24.547675 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.562740 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:54:24.563324 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca"
[0m12:54:24.563324 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        transaction_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."payments"
    group by transaction_payment_status

)

select *
from all_values
where value_field not in (
    'Completed','Failed','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m12:54:24.565782 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:24.567044 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca: ROLLBACK
[0m12:54:24.568185 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca: Close
[0m12:54:24.568758 [info ] [Thread-1 (]: 4 of 23 PASS source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded  [[32mPASS[0m in 0.03s]
[0m12:54:24.570062 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca
[0m12:54:24.570062 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:54:24.570642 [info ] [Thread-1 (]: 5 of 23 START test source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [RUN]
[0m12:54:24.571337 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_payments_transaction_payment_status__Completed__Failed__Pending__Refunded.3b8b3e45ca, now test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e)
[0m12:54:24.571915 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:54:24.576832 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:54:24.577969 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:54:24.579676 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:54:24.580239 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:54:24.580795 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: BEGIN
[0m12:54:24.580795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.618029 [debug] [Thread-1 (]: SQL status: BEGIN in 0.037 seconds
[0m12:54:24.619116 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"
[0m12:54:24.619736 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        order_level_payment_status as value_field,
        count(*) as n_records

    from "datamart"."raw"."raw_data"
    group by order_level_payment_status

)

select *
from all_values
where value_field not in (
    'Paid','Pending','Refunded'
)



  
  
      
    ) dbt_internal_test
[0m12:54:24.621923 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:24.623436 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: ROLLBACK
[0m12:54:24.625055 [debug] [Thread-1 (]: On test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e: Close
[0m12:54:24.626118 [info ] [Thread-1 (]: 5 of 23 PASS source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded  [[32mPASS[0m in 0.05s]
[0m12:54:24.626705 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e
[0m12:54:24.627256 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:54:24.627846 [info ] [Thread-1 (]: 6 of 23 START test source_not_null_raw_customers_source_customer_id ............ [RUN]
[0m12:54:24.628418 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_accepted_values_raw_raw_data_order_level_payment_status__Paid__Pending__Refunded.125146fc6e, now test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556)
[0m12:54:24.628418 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:54:24.631732 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:54:24.632929 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:54:24.634611 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:54:24.635271 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:54:24.635271 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: BEGIN
[0m12:54:24.636246 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.657626 [debug] [Thread-1 (]: SQL status: BEGIN in 0.021 seconds
[0m12:54:24.658180 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"
[0m12:54:24.658742 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."customers_source"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.660408 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.661577 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: ROLLBACK
[0m12:54:24.662683 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556: Close
[0m12:54:24.663297 [info ] [Thread-1 (]: 6 of 23 PASS source_not_null_raw_customers_source_customer_id .................. [[32mPASS[0m in 0.03s]
[0m12:54:24.664165 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556
[0m12:54:24.664942 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:54:24.664942 [info ] [Thread-1 (]: 7 of 23 START test source_not_null_raw_customers_source_signup_date ............ [RUN]
[0m12:54:24.665564 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_customer_id.5943a6f556, now test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f)
[0m12:54:24.666112 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:54:24.668987 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:54:24.670090 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:54:24.671769 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:54:24.672288 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:54:24.672288 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: BEGIN
[0m12:54:24.672913 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.711927 [debug] [Thread-1 (]: SQL status: BEGIN in 0.039 seconds
[0m12:54:24.713137 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"
[0m12:54:24.714057 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select signup_date
from "datamart"."raw"."customers_source"
where signup_date is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.715305 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.717753 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: ROLLBACK
[0m12:54:24.718875 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f: Close
[0m12:54:24.719986 [info ] [Thread-1 (]: 7 of 23 PASS source_not_null_raw_customers_source_signup_date .................. [[32mPASS[0m in 0.05s]
[0m12:54:24.721191 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f
[0m12:54:24.721191 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:54:24.721740 [info ] [Thread-1 (]: 8 of 23 START test source_not_null_raw_payments_order_id ....................... [RUN]
[0m12:54:24.722314 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_customers_source_signup_date.f04306d89f, now test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9)
[0m12:54:24.723006 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:54:24.726905 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:54:24.727994 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:54:24.729677 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:54:24.730224 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:54:24.730731 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: BEGIN
[0m12:54:24.730731 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.742377 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m12:54:24.743383 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"
[0m12:54:24.743890 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."payments"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.745854 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:24.746991 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: ROLLBACK
[0m12:54:24.748129 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9: Close
[0m12:54:24.748681 [info ] [Thread-1 (]: 8 of 23 PASS source_not_null_raw_payments_order_id ............................. [[32mPASS[0m in 0.03s]
[0m12:54:24.749362 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9
[0m12:54:24.749926 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:54:24.749926 [info ] [Thread-1 (]: 9 of 23 START test source_not_null_raw_payments_payment_id ..................... [RUN]
[0m12:54:24.750960 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_order_id.def3cb6fc9, now test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5)
[0m12:54:24.750960 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:54:24.757295 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:54:24.758334 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:54:24.760652 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:54:24.761213 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:54:24.761759 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: BEGIN
[0m12:54:24.761759 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.796994 [debug] [Thread-1 (]: SQL status: BEGIN in 0.035 seconds
[0m12:54:24.797553 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"
[0m12:54:24.798220 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select payment_id
from "datamart"."raw"."payments"
where payment_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.800757 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.801892 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: ROLLBACK
[0m12:54:24.803019 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5: Close
[0m12:54:24.803588 [info ] [Thread-1 (]: 9 of 23 PASS source_not_null_raw_payments_payment_id ........................... [[32mPASS[0m in 0.05s]
[0m12:54:24.804738 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5
[0m12:54:24.805348 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:54:24.805905 [info ] [Thread-1 (]: 10 of 23 START test source_not_null_raw_products_category ...................... [RUN]
[0m12:54:24.807501 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_payments_payment_id.a7f5b41ef5, now test.data_pipeline.source_not_null_raw_products_category.9265557239)
[0m12:54:24.808057 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:54:24.816426 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:54:24.818268 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:54:24.821713 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:54:24.823375 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:54:24.823790 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: BEGIN
[0m12:54:24.823790 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.839713 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:24.840152 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_category.9265557239"
[0m12:54:24.840697 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_category.9265557239"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select category
from "datamart"."raw"."products"
where category is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.843147 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.844295 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: ROLLBACK
[0m12:54:24.845562 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_category.9265557239: Close
[0m12:54:24.846687 [info ] [Thread-1 (]: 10 of 23 PASS source_not_null_raw_products_category ............................ [[32mPASS[0m in 0.04s]
[0m12:54:24.848415 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_category.9265557239
[0m12:54:24.849080 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:54:24.849080 [info ] [Thread-1 (]: 11 of 23 START test source_not_null_raw_products_price ......................... [RUN]
[0m12:54:24.850266 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_category.9265557239, now test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b)
[0m12:54:24.850806 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:54:24.856996 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:54:24.857573 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:54:24.859264 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:54:24.860915 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:54:24.861562 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: BEGIN
[0m12:54:24.861562 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.877413 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:24.877972 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"
[0m12:54:24.878520 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select price
from "datamart"."raw"."products"
where price is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.879665 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.881335 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: ROLLBACK
[0m12:54:24.882476 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b: Close
[0m12:54:24.883029 [info ] [Thread-1 (]: 11 of 23 PASS source_not_null_raw_products_price ............................... [[32mPASS[0m in 0.03s]
[0m12:54:24.883586 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b
[0m12:54:24.884262 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:54:24.884860 [info ] [Thread-1 (]: 12 of 23 START test source_not_null_raw_products_product_id .................... [RUN]
[0m12:54:24.885465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_price.ce9a7bfd3b, now test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae)
[0m12:54:24.886036 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:54:24.891192 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:54:24.892260 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:54:24.894634 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:54:24.895801 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:54:24.896371 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: BEGIN
[0m12:54:24.896371 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.911057 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:54:24.912227 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"
[0m12:54:24.912800 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."products"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.914283 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.916864 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: ROLLBACK
[0m12:54:24.918815 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae: Close
[0m12:54:24.920070 [info ] [Thread-1 (]: 12 of 23 PASS source_not_null_raw_products_product_id .......................... [[32mPASS[0m in 0.03s]
[0m12:54:24.921385 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae
[0m12:54:24.922392 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:54:24.922770 [info ] [Thread-1 (]: 13 of 23 START test source_not_null_raw_products_product_name .................. [RUN]
[0m12:54:24.923766 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_id.b984a33cae, now test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51)
[0m12:54:24.924301 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:54:24.931429 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:54:24.932957 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:54:24.935217 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:54:24.935769 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:54:24.936339 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: BEGIN
[0m12:54:24.936339 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:24.952784 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:24.953915 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"
[0m12:54:24.954475 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_name
from "datamart"."raw"."products"
where product_name is null



  
  
      
    ) dbt_internal_test
[0m12:54:24.956281 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:24.959404 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: ROLLBACK
[0m12:54:24.961234 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51: Close
[0m12:54:24.962412 [info ] [Thread-1 (]: 13 of 23 PASS source_not_null_raw_products_product_name ........................ [[32mPASS[0m in 0.04s]
[0m12:54:24.963922 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51
[0m12:54:24.965320 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:54:24.965826 [info ] [Thread-1 (]: 14 of 23 START test source_not_null_raw_raw_data_customer_id ................... [RUN]
[0m12:54:24.966746 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_products_product_name.2fb02dfa51, now test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3)
[0m12:54:24.966746 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:54:24.970086 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:54:24.971181 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:54:24.972679 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:54:24.973238 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:54:24.973787 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: BEGIN
[0m12:54:24.973787 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.001072 [debug] [Thread-1 (]: SQL status: BEGIN in 0.027 seconds
[0m12:54:25.001655 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"
[0m12:54:25.001655 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select customer_id
from "datamart"."raw"."raw_data"
where customer_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:25.003348 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.004505 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: ROLLBACK
[0m12:54:25.005055 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3: Close
[0m12:54:25.006618 [info ] [Thread-1 (]: 14 of 23 PASS source_not_null_raw_raw_data_customer_id ......................... [[32mPASS[0m in 0.04s]
[0m12:54:25.007944 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3
[0m12:54:25.007944 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:54:25.007944 [info ] [Thread-1 (]: 15 of 23 START test source_not_null_raw_raw_data_order_date .................... [RUN]
[0m12:54:25.009151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_customer_id.e7d23030b3, now test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc)
[0m12:54:25.009655 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:54:25.012663 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:54:25.013798 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:54:25.015643 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:54:25.016227 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:54:25.016227 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: BEGIN
[0m12:54:25.016836 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.029825 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m12:54:25.030477 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"
[0m12:54:25.030837 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_date
from "datamart"."raw"."raw_data"
where order_date is null



  
  
      
    ) dbt_internal_test
[0m12:54:25.032138 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.033528 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: ROLLBACK
[0m12:54:25.034721 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc: Close
[0m12:54:25.035575 [info ] [Thread-1 (]: 15 of 23 PASS source_not_null_raw_raw_data_order_date .......................... [[32mPASS[0m in 0.03s]
[0m12:54:25.036079 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc
[0m12:54:25.036079 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:54:25.036744 [info ] [Thread-1 (]: 16 of 23 START test source_not_null_raw_raw_data_order_id ...................... [RUN]
[0m12:54:25.037290 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_date.8fd09fb7bc, now test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6)
[0m12:54:25.037882 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:54:25.041439 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:54:25.042129 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:54:25.044412 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:54:25.044992 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:54:25.044992 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: BEGIN
[0m12:54:25.045576 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.059972 [debug] [Thread-1 (]: SQL status: BEGIN in 0.015 seconds
[0m12:54:25.060577 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"
[0m12:54:25.061160 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select order_id
from "datamart"."raw"."raw_data"
where order_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:25.062282 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.063985 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: ROLLBACK
[0m12:54:25.064326 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6: Close
[0m12:54:25.065701 [info ] [Thread-1 (]: 16 of 23 PASS source_not_null_raw_raw_data_order_id ............................ [[32mPASS[0m in 0.03s]
[0m12:54:25.066697 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6
[0m12:54:25.067581 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:54:25.067581 [info ] [Thread-1 (]: 17 of 23 START test source_not_null_raw_raw_data_product_id .................... [RUN]
[0m12:54:25.068366 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_order_id.bd97ab0ff6, now test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e)
[0m12:54:25.068366 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:54:25.072923 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:54:25.073434 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:54:25.075873 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:54:25.076524 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:54:25.077101 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: BEGIN
[0m12:54:25.077101 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.105828 [debug] [Thread-1 (]: SQL status: BEGIN in 0.028 seconds
[0m12:54:25.106632 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"
[0m12:54:25.106632 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select product_id
from "datamart"."raw"."raw_data"
where product_id is null



  
  
      
    ) dbt_internal_test
[0m12:54:25.108336 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.110079 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: ROLLBACK
[0m12:54:25.110653 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e: Close
[0m12:54:25.112186 [info ] [Thread-1 (]: 17 of 23 PASS source_not_null_raw_raw_data_product_id .......................... [[32mPASS[0m in 0.04s]
[0m12:54:25.112692 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e
[0m12:54:25.113261 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:54:25.113809 [info ] [Thread-1 (]: 18 of 23 START test source_not_null_raw_raw_data_total_amount .................. [RUN]
[0m12:54:25.114300 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_product_id.cc144a5c3e, now test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e)
[0m12:54:25.114854 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:54:25.118325 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:54:25.119447 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:54:25.121780 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:54:25.122096 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:54:25.122678 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: BEGIN
[0m12:54:25.122678 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.137459 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:54:25.137459 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"
[0m12:54:25.138022 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select total_amount
from "datamart"."raw"."raw_data"
where total_amount is null



  
  
      
    ) dbt_internal_test
[0m12:54:25.140193 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.142048 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: ROLLBACK
[0m12:54:25.143166 [debug] [Thread-1 (]: On test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e: Close
[0m12:54:25.143711 [info ] [Thread-1 (]: 18 of 23 PASS source_not_null_raw_raw_data_total_amount ........................ [[32mPASS[0m in 0.03s]
[0m12:54:25.144476 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e
[0m12:54:25.145052 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:54:25.145607 [info ] [Thread-1 (]: 19 of 23 START test source_unique_raw_customers_source_customer_id ............. [RUN]
[0m12:54:25.146168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_not_null_raw_raw_data_total_amount.45226b5b6e, now test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c)
[0m12:54:25.146726 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:54:25.151453 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:54:25.152020 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:54:25.153686 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:54:25.154835 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:54:25.154835 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: BEGIN
[0m12:54:25.155376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.168832 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m12:54:25.169380 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"
[0m12:54:25.169380 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where customer_id is not null
group by customer_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:54:25.171669 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.001 seconds
[0m12:54:25.172708 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: ROLLBACK
[0m12:54:25.173928 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c: Close
[0m12:54:25.174431 [info ] [Thread-1 (]: 19 of 23 PASS source_unique_raw_customers_source_customer_id ................... [[32mPASS[0m in 0.03s]
[0m12:54:25.176523 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c
[0m12:54:25.177090 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:54:25.177090 [info ] [Thread-1 (]: 20 of 23 START test source_unique_raw_customers_source_email ................... [RUN]
[0m12:54:25.177636 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_customer_id.5589f45b7c, now test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b)
[0m12:54:25.178173 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:54:25.180921 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:54:25.182279 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:54:25.183900 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:54:25.185060 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:54:25.185060 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: BEGIN
[0m12:54:25.185690 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.199368 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m12:54:25.199940 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"
[0m12:54:25.199940 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    email as unique_field,
    count(*) as n_records

from "datamart"."raw"."customers_source"
where email is not null
group by email
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:54:25.202213 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:25.203329 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: ROLLBACK
[0m12:54:25.203899 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b: Close
[0m12:54:25.205440 [info ] [Thread-1 (]: 20 of 23 PASS source_unique_raw_customers_source_email ......................... [[32mPASS[0m in 0.03s]
[0m12:54:25.206929 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b
[0m12:54:25.207433 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:54:25.208023 [info ] [Thread-1 (]: 21 of 23 START test source_unique_raw_payments_payment_id ...................... [RUN]
[0m12:54:25.208023 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_customers_source_email.d45d0fa05b, now test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533)
[0m12:54:25.208803 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:54:25.215056 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:54:25.216173 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:54:25.219461 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:54:25.220614 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:54:25.221195 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: BEGIN
[0m12:54:25.221764 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.238617 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m12:54:25.239183 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"
[0m12:54:25.239823 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."payments"
where payment_id is not null
group by payment_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:54:25.242739 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:25.243860 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: ROLLBACK
[0m12:54:25.244993 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533: Close
[0m12:54:25.246147 [info ] [Thread-1 (]: 21 of 23 PASS source_unique_raw_payments_payment_id ............................ [[32mPASS[0m in 0.04s]
[0m12:54:25.247389 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533
[0m12:54:25.248454 [debug] [Thread-1 (]: Began running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:54:25.249403 [info ] [Thread-1 (]: 22 of 23 START test source_unique_raw_products_product_id ...................... [RUN]
[0m12:54:25.250435 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_payments_payment_id.ad2e113533, now test.data_pipeline.source_unique_raw_products_product_id.518dac90ba)
[0m12:54:25.251128 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:54:25.256693 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:54:25.258228 [debug] [Thread-1 (]: Began executing node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:54:25.260545 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:54:25.261648 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:54:25.261648 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: BEGIN
[0m12:54:25.262249 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.289864 [debug] [Thread-1 (]: SQL status: BEGIN in 0.028 seconds
[0m12:54:25.290189 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"
[0m12:54:25.290189 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.source_unique_raw_products_product_id.518dac90ba"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."raw"."products"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:54:25.292870 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:25.294674 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: ROLLBACK
[0m12:54:25.295910 [debug] [Thread-1 (]: On test.data_pipeline.source_unique_raw_products_product_id.518dac90ba: Close
[0m12:54:25.297047 [info ] [Thread-1 (]: 22 of 23 PASS source_unique_raw_products_product_id ............................ [[32mPASS[0m in 0.05s]
[0m12:54:25.298801 [debug] [Thread-1 (]: Finished running node test.data_pipeline.source_unique_raw_products_product_id.518dac90ba
[0m12:54:25.299403 [debug] [Thread-1 (]: Began running node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:54:25.299403 [info ] [Thread-1 (]: 23 of 23 START test unique_reviews_product_id .................................. [RUN]
[0m12:54:25.300509 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.data_pipeline.source_unique_raw_products_product_id.518dac90ba, now test.data_pipeline.unique_reviews_product_id.d6f6d62f1d)
[0m12:54:25.301062 [debug] [Thread-1 (]: Began compiling node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:54:25.306751 [debug] [Thread-1 (]: Writing injected SQL for node "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:54:25.307419 [debug] [Thread-1 (]: Began executing node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:54:25.311110 [debug] [Thread-1 (]: Writing runtime sql for node "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:54:25.312772 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:54:25.312772 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: BEGIN
[0m12:54:25.313361 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:54:25.331478 [debug] [Thread-1 (]: SQL status: BEGIN in 0.018 seconds
[0m12:54:25.332360 [debug] [Thread-1 (]: Using postgres connection "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"
[0m12:54:25.332360 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "node_id": "test.data_pipeline.unique_reviews_product_id.d6f6d62f1d"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    product_id as unique_field,
    count(*) as n_records

from "datamart"."public"."reviews"
where product_id is not null
group by product_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m12:54:25.335530 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.002 seconds
[0m12:54:25.336673 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: ROLLBACK
[0m12:54:25.337800 [debug] [Thread-1 (]: On test.data_pipeline.unique_reviews_product_id.d6f6d62f1d: Close
[0m12:54:25.338945 [info ] [Thread-1 (]: 23 of 23 PASS unique_reviews_product_id ........................................ [[32mPASS[0m in 0.04s]
[0m12:54:25.340898 [debug] [Thread-1 (]: Finished running node test.data_pipeline.unique_reviews_product_id.d6f6d62f1d
[0m12:54:25.342604 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:25.343724 [debug] [MainThread]: On master: BEGIN
[0m12:54:25.344290 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:54:25.361404 [debug] [MainThread]: SQL status: BEGIN in 0.017 seconds
[0m12:54:25.362003 [debug] [MainThread]: On master: COMMIT
[0m12:54:25.362003 [debug] [MainThread]: Using postgres connection "master"
[0m12:54:25.362555 [debug] [MainThread]: On master: COMMIT
[0m12:54:25.363121 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:54:25.363677 [debug] [MainThread]: On master: Close
[0m12:54:25.364491 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:54:25.364491 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m12:54:25.364491 [debug] [MainThread]: Connection 'test.data_pipeline.unique_reviews_product_id.d6f6d62f1d' was properly closed.
[0m12:54:25.364491 [info ] [MainThread]: 
[0m12:54:25.366497 [info ] [MainThread]: Finished running 23 data tests in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m12:54:25.369521 [debug] [MainThread]: Command end result
[0m12:54:25.405851 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m12:54:25.409159 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m12:54:25.418134 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Project\DataPipeline\dags\dbt_project\target\run_results.json
[0m12:54:25.418724 [info ] [MainThread]: 
[0m12:54:25.419286 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:54:25.419897 [info ] [MainThread]: 
[0m12:54:25.421188 [info ] [MainThread]: Done. PASS=23 WARN=0 ERROR=0 SKIP=0 TOTAL=23
[0m12:54:25.422332 [debug] [MainThread]: Command `dbt test` succeeded at 12:54:25.422332 after 2.60 seconds
[0m12:54:25.422778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C864CA2360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C8698CFDA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C8651F2480>]}
[0m12:54:25.423372 [debug] [MainThread]: Flushing usage events
[0m12:54:25.842153 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:21:36.016812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC88631AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC85B52BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC88614170>]}


============================== 15:21:36.027404 | 17efc3d8-1c54-4cf7-bb9b-7139aef80698 ==============================
[0m15:21:36.027404 [info ] [MainThread]: Running with dbt=1.9.6
[0m15:21:36.031417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:21:36.298205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '17efc3d8-1c54-4cf7-bb9b-7139aef80698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A0C6ED0>]}
[0m15:21:36.352156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '17efc3d8-1c54-4cf7-bb9b-7139aef80698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8880CFB0>]}
[0m15:21:36.359029 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:21:36.567540 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m15:21:36.791483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:21:36.791483 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:21:36.820344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '17efc3d8-1c54-4cf7-bb9b-7139aef80698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A1334D0>]}
[0m15:21:36.884184 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m15:21:36.962328 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m15:21:37.085792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '17efc3d8-1c54-4cf7-bb9b-7139aef80698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A0C4F20>]}
[0m15:21:37.085792 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m15:21:37.092835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '17efc3d8-1c54-4cf7-bb9b-7139aef80698', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A14C770>]}
[0m15:21:37.092835 [info ] [MainThread]: 
[0m15:21:37.092835 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:21:37.092835 [info ] [MainThread]: 
[0m15:21:37.092835 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:21:37.100153 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m15:21:37.251577 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m15:21:37.251577 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m15:21:37.251577 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:21:37.317141 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m15:21:37.360270 [debug] [ThreadPool]: Postgres adapter: Error running SQL: BEGIN
[0m15:21:37.360270 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:21:37.360270 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_relations_without_caching
[0m15:21:37.360270 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:21:37.360270 [debug] [ThreadPool]: On list_datamart_public: No close available on handle
[0m15:21:37.366948 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:21:37.368198 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m15:21:37.368198 [info ] [MainThread]: 
[0m15:21:37.374935 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m15:21:37.374935 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m15:21:37.378817 [debug] [MainThread]: Command `dbt test` failed at 15:21:37.378284 after 1.58 seconds
[0m15:21:37.378817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC887D4CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A7AD640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC8A2893D0>]}
[0m15:21:37.378817 [debug] [MainThread]: Flushing usage events
[0m15:21:37.777354 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:22:47.796804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E983D62B70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E986781C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98694C170>]}


============================== 15:22:47.802798 | 56b92b3a-cdde-49b8-991b-85ca1d6a499a ==============================
[0m15:22:47.802798 [info ] [MainThread]: Running with dbt=1.9.6
[0m15:22:47.803802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:22:47.947794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '56b92b3a-cdde-49b8-991b-85ca1d6a499a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E987EA9850>]}
[0m15:22:47.996442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '56b92b3a-cdde-49b8-991b-85ca1d6a499a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E986DA4A10>]}
[0m15:22:47.996442 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:22:48.191725 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m15:22:48.328483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:22:48.329264 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:22:48.369483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '56b92b3a-cdde-49b8-991b-85ca1d6a499a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E987EDDB20>]}
[0m15:22:48.404919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '56b92b3a-cdde-49b8-991b-85ca1d6a499a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98844AAE0>]}
[0m15:22:48.405429 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m15:22:48.405429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '56b92b3a-cdde-49b8-991b-85ca1d6a499a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9882E42F0>]}
[0m15:22:48.408077 [info ] [MainThread]: 
[0m15:22:48.409138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:22:48.410273 [info ] [MainThread]: 
[0m15:22:48.415253 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:22:48.419072 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m15:22:48.484363 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m15:22:48.484363 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m15:22:48.484363 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:22:48.525524 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m15:22:48.581636 [debug] [ThreadPool]: Postgres adapter: Error running SQL: BEGIN
[0m15:22:48.581636 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:22:48.581636 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_relations_without_caching
[0m15:22:48.581636 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:22:48.581636 [debug] [ThreadPool]: On list_datamart_public: No close available on handle
[0m15:22:48.581636 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:22:48.581636 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m15:22:48.581636 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m15:22:48.585271 [debug] [MainThread]: Command `dbt docs generate` failed at 15:22:48.585271 after 1.08 seconds
[0m15:22:48.585271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9866B2F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98895F920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E98892BDA0>]}
[0m15:22:48.586306 [debug] [MainThread]: Flushing usage events
[0m15:22:49.060967 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:25:08.796492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018564E72060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185648AC6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000185648AC620>]}


============================== 15:25:08.798687 | 47edbc37-8451-4234-a66e-e28dc9a1453b ==============================
[0m15:25:08.798687 [info ] [MainThread]: Running with dbt=1.9.6
[0m15:25:08.800696 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:25:08.950735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47edbc37-8451-4234-a66e-e28dc9a1453b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018565172E40>]}
[0m15:25:08.994625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47edbc37-8451-4234-a66e-e28dc9a1453b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018565301160>]}
[0m15:25:08.994625 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:25:09.190670 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m15:25:09.331887 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:25:09.333895 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:25:09.366834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47edbc37-8451-4234-a66e-e28dc9a1453b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018565025940>]}
[0m15:25:09.393583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47edbc37-8451-4234-a66e-e28dc9a1453b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018566AB4920>]}
[0m15:25:09.393583 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m15:25:09.393583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47edbc37-8451-4234-a66e-e28dc9a1453b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018566B7E780>]}
[0m15:25:09.398833 [info ] [MainThread]: 
[0m15:25:09.398833 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:25:09.398833 [info ] [MainThread]: 
[0m15:25:09.401350 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:25:09.404358 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart_public'
[0m15:25:09.492862 [debug] [ThreadPool]: Using postgres connection "list_datamart_public"
[0m15:25:09.493369 [debug] [ThreadPool]: On list_datamart_public: BEGIN
[0m15:25:09.493369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:09.544728 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m15:25:09.593337 [debug] [ThreadPool]: Postgres adapter: Error running SQL: BEGIN
[0m15:25:09.593337 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:25:09.594424 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_relations_without_caching
[0m15:25:09.594424 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:25:09.594424 [debug] [ThreadPool]: On list_datamart_public: No close available on handle
[0m15:25:09.596431 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:25:09.597484 [debug] [MainThread]: Connection 'list_datamart_public' was properly closed.
[0m15:25:09.598009 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m15:25:09.598009 [debug] [MainThread]: Command `dbt docs generate` failed at 15:25:09.598009 after 1.00 seconds
[0m15:25:09.598009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018564E9B5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018566F9F7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018566865E80>]}
[0m15:25:09.598009 [debug] [MainThread]: Flushing usage events
[0m15:25:10.036144 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:28:09.685684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E020F1640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E020F10D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E020F2030>]}


============================== 15:28:09.690679 | d2037caf-f333-4d94-9f5b-950d29b6b24e ==============================
[0m15:28:09.690679 [info ] [MainThread]: Running with dbt=1.9.6
[0m15:28:09.693049 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:28:09.898422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2037caf-f333-4d94-9f5b-950d29b6b24e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E0257CFB0>]}
[0m15:28:09.960860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2037caf-f333-4d94-9f5b-950d29b6b24e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E01505AF0>]}
[0m15:28:09.960860 [info ] [MainThread]: Registered adapter: postgres=1.9.0
[0m15:28:10.233982 [debug] [MainThread]: checksum: dd6dc1e5178459e3de3bf2eeb7c86bed4be2266c311fce8c15d86eb4ff94e7ad, vars: {}, profile: , target: , version: 1.9.6
[0m15:28:10.472244 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:28:10.472797 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:28:10.521940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd2037caf-f333-4d94-9f5b-950d29b6b24e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E03A8EF60>]}
[0m15:28:10.625722 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m15:28:10.679965 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m15:28:10.752007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd2037caf-f333-4d94-9f5b-950d29b6b24e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E03A8EEA0>]}
[0m15:28:10.753008 [info ] [MainThread]: Found 10 models, 23 data tests, 4 sources, 766 macros
[0m15:28:10.759006 [info ] [MainThread]: 
[0m15:28:10.760867 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:28:10.760867 [info ] [MainThread]: 
[0m15:28:10.761873 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:28:10.767871 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m15:28:10.869502 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m15:28:10.869502 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m15:28:10.869502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:28:10.938052 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m15:28:11.009525 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.9.6", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m15:28:11.010531 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:28:11.010531 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m15:28:11.011900 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m15:28:11.011900 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m15:28:11.012745 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:28:11.013507 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m15:28:11.014021 [info ] [MainThread]: 
[0m15:28:11.014021 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m15:28:11.015029 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m15:28:11.016260 [debug] [MainThread]: Command `dbt build` failed at 15:28:11.016260 after 1.58 seconds
[0m15:28:11.017258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E7FF010A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E04145CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E044DB050>]}
[0m15:28:11.017258 [debug] [MainThread]: Flushing usage events
[0m15:28:11.455376 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m03:46:22.548529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A7CB8650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AA4D3860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AA4D0140>]}


============================== 03:46:22.552420 | 983849da-c638-4f0c-adc9-8aa52db54105 ==============================
[0m03:46:22.552420 [info ] [MainThread]: Running with dbt=1.10.13
[0m03:46:22.553484 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Project\\DataPipeline\\dags\\dbt_project\\logs', 'profiles_dir': 'C:\\Users\\Gaurav Chugh\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt build --project-dir C:\\Project\\DataPipeline\\dags\\dbt_project', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:46:22.801370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '983849da-c638-4f0c-adc9-8aa52db54105', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AC26BDD0>]}
[0m03:46:22.876767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '983849da-c638-4f0c-adc9-8aa52db54105', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AAA0B0B0>]}
[0m03:46:22.880272 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m03:46:23.236044 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m03:46:23.690465 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m03:46:23.694250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '983849da-c638-4f0c-adc9-8aa52db54105', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AC6AFD10>]}
[0m03:46:25.166110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '983849da-c638-4f0c-adc9-8aa52db54105', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AC6AFF80>]}
[0m03:46:25.326785 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Project\DataPipeline\dags\dbt_project\target\manifest.json
[0m03:46:25.332710 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Project\DataPipeline\dags\dbt_project\target\semantic_manifest.json
[0m03:46:25.380575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '983849da-c638-4f0c-adc9-8aa52db54105', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6ACAB72F0>]}
[0m03:46:25.380575 [info ] [MainThread]: Found 6 models, 13 data tests, 4 sources, 782 macros
[0m03:46:25.380575 [info ] [MainThread]: 
[0m03:46:25.380575 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:46:25.380575 [info ] [MainThread]: 
[0m03:46:25.380575 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m03:46:25.396397 [debug] [ThreadPool]: Acquiring new postgres connection 'list_datamart'
[0m03:46:25.486418 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m03:46:25.487415 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m03:46:25.487415 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:46:25.540790 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m03:46:25.586563 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m03:46:25.586563 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m03:46:25.589555 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m03:46:25.589555 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m03:46:25.589555 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m03:46:25.591562 [debug] [ThreadPool]: Using postgres connection "list_datamart"
[0m03:46:25.591562 [debug] [ThreadPool]: On list_datamart: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m03:46:25.591562 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:46:25.683776 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 0 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"

[0m03:46:25.750254 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "data_pipeline", "target_name": "dev", "connection_name": "list_datamart"} */

    select distinct nspname from pg_namespace
  
[0m03:46:25.750254 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m03:46:25.750254 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m03:46:25.750254 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m03:46:25.751760 [debug] [ThreadPool]: On list_datamart: No close available on handle
[0m03:46:25.751760 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:46:25.752766 [debug] [MainThread]: Connection 'list_datamart' was properly closed.
[0m03:46:25.752766 [info ] [MainThread]: 
[0m03:46:25.753768 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.37 seconds (0.37s).
[0m03:46:25.754766 [error] [MainThread]: Encountered an error:
Database Error
  connection to server at "localhost" (::1), port 5432 failed: FATAL:  password authentication failed for user "dwh_user"
  
[0m03:46:25.756130 [debug] [MainThread]: Command `dbt build` failed at 03:46:25.756130 after 3.61 seconds
[0m03:46:25.756634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AA4D0140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AC9DD7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6AC9DF110>]}
[0m03:46:25.757167 [debug] [MainThread]: Flushing usage events
[0m03:46:26.166312 [debug] [MainThread]: An error was encountered while trying to flush usage events
