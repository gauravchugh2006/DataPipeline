version: '3.8'

networks:
  default:
    name: datahub_network

# Define the custom Airflow settings that will be reused (the anchor must be defined first)
x-airflow-common: &airflow-common
  image: apache/airflow:2.3.4
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__FERNET_KEY=ehE_UybKeIGETmag57Fy1MBeAIQRWDt7NS9zcl017Yg=
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_CREATION=True
    - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=True
  volumes:
    - ./dags:/opt/airflow/dags
    - ./airflow-data/logs:/opt/airflow/logs
    - ./airflow-data/plugins:/opt/airflow/plugins
    - ./airflow-data/airflow.cfg:/opt/airflow/airflow.cfg
    - ./dbt_profiles:/opt/airflow/.dbt

services:

  ### üöÄ PostgreSQL for Airflow & Metadata Storage ###
  postgres_airflow:
    image: postgres:15
    container_name: postgres_airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: always
    ports:
      - "5439:5432"
    volumes:
      - postgres_airflow_data:/var/lib/postgresql/data

  ### üöÄ Redis for Airflow Caching ###
  redis:
    image: redis:7.2-bookworm
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  ### üöÄ Apache Airflow ###
  airflow:
    image: apache/airflow:2.3.4
    container_name: airflow
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    depends_on:
      - postgres_airflow
    ports:
      - "8082:8082"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
      - ./airflow-data/airflow.cfg:/opt/airflow/airflow.cfg
      - ./dbt_profiles:/opt/airflow/.dbt
    command: webserver

  # ‚úÖ Airflow Init Service (runs once after everything is set up)
  airflow-init:
    image: apache/airflow:2.3.4       # Use the same version as your primary Airflow service
    container_name: airflow_init
    restart: "no"                   # Do not restart‚Äîrun once and exit
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres_airflow/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    command: >
      bash -cx '
      echo "Waiting for Postgres to be ready...";
      until pg_isready -h postgres_airflow -p 5432 -U airflow; do
        echo "Postgres not ready yet, sleeping 5 seconds...";
        sleep 5;
      done;
      echo "Postgres is ready. Initializing Airflow DB...";
      airflow db init;
      echo "Airflow DB initialized. Creating admin user...";
      airflow users create --username airflow --firstname Air --lastname Flow --role Admin --email airflow@example.com -p airflow || true;
      echo "Airflow initialization tasks complete."'
    depends_on:
      postgres_airflow:
        condition: service_healthy
 
  ### üê≥ Airflow Scheduler service ###
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    command: airflow scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres_airflow:
        condition: service_healthy
      redis:
        condition: service_healthy

  ### üõ†Ô∏è MinIO (S3-compatible Object Storage) ###
  minio:
    image: minio/minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  ### üóÉÔ∏è PostgreSQL for Metadata (DBT, DataHub) ###
  postgres_dw:
    image: postgres:15
    container_name: postgres_dw
    environment:
      POSTGRES_USER: dwh_user
      POSTGRES_PASSWORD: dwh_password
      POSTGRES_DB: datamart
    healthcheck:
     test: ["CMD", "pg_isready", "-U", "dwh_user"]
     interval: 10s
     retries: 5
     start_period: 15s
     timeout: 5s
    restart: always
    ports:
      - "5432:5432"
    volumes:
      - postgres_dw_data:/var/lib/postgresql/data

  ### üß† Neo4j Graph Database ###
  neo4j:
    image: neo4j:4.4.9-community
    container_name: neo4j
    environment:
      - NEO4J_AUTH=neo4j/datahub
      - NEO4J_dbms_default__database=graph.db
      - NEO4J_dbms_allow__upgrade=true
      - NEO4JLABS_PLUGINS=["apoc"]
    healthcheck:
      interval: 5s
      retries: 10
      start_period: 60s
      test: wget http://neo4j:7474
      timeout: 20s
    hostname: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data

  ### üîç Zookeeper (Kafka Dependency) ###
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    healthcheck:
      interval: 5s
      retries: 3
      start_period: 10s
      test: echo srvr | nc zookeeper 2181
      timeout: 5s
    hostname: zookeeper
    ports:
      - "2181:2181"
    volumes:
      - zkdata:/var/lib/zookeeper/data
      - zklogs:/var/lib/zookeeper/log

  ### üî• Kafka (Broker for DataHub) ###
  broker:
    image: confluentinc/cp-kafka:7.4.0
    container_name: broker
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_HEAP_OPTS=-Xms256m -Xmx256m
      - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
      - KAFKA_MESSAGE_MAX_BYTES=5242880
      - KAFKA_MAX_MESSAGE_BYTES=5242880
    healthcheck:
      interval: 1s
      retries: 5
      start_period: 60s
      test: nc -z broker $${DATAHUB_KAFKA_BROKER_PORT:-9092}
      timeout: 5s
    hostname: broker
    ports:
      - "9092:9092"
    volumes:
      - broker:/var/lib/kafka/data/

  ### üìú Schema Registry (Ensures Consistent Message Formats) ###
  schema-registry:
    image: confluentinc/cp-schema-registry:7.4.0
    container_name: schema-registry
    depends_on:
      - broker
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: broker:29092
    ports:
      - "8081:8081"

  ### üîç DataHub Core Components ###
  datahub-gms:
    image: acryldata/datahub-gms:head
    container_name: datahub-gms
    environment:
      DATAHUB_GMS_HOST: datahub-gms
      DATAHUB_GMS_PORT: 8080
      DATAHUB_SECRET: YouKnowNothing
      KAFKA_BOOTSTRAP_SERVER: broker:29092
    ports:
      - "8080:8080"
    depends_on:
      - postgres_dw
      - broker

  datahub-actions:
    image: acryldata/datahub-actions:head
    container_name: datahub-actions
    environment:
      DATAHUB_GMS_HOST: datahub-gms
      DATAHUB_GMS_PORT: 8080
      DATAHUB_SECRET: YouKnowNothing
      KAFKA_BOOTSTRAP_SERVER: broker:29092
    depends_on:
      - datahub-gms

  datahub-frontend-react:
    image: acryldata/datahub-frontend-react:head
    container_name: datahub-frontend-react
    depends_on:
      - datahub-gms
    environment:
      DATAHUB_GMS_HOST: datahub-gms
      DATAHUB_GMS_PORT: 8080
      DATAHUB_SECRET: YouKnowNothing
    ports:
      - "9002:9002"

volumes:
  broker:
  postgres_dw_data:
  postgres_airflow_data:
  minio_data:
  neo4j_data:
  zkdata:
  zklogs:
