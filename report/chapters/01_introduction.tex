\chapter{Introduction}
\section{Context}
Ecommerce has entered a phase where digital storefronts, mobile applications, physical stores, and third-party marketplaces are intertwined. Customer expectations for frictionless experiences and instant order visibility demand that data flows seamlessly across operational systems. The host organisation processes more than 60,000 orders per day, with peak trading seasons generating bursts exceeding 500 orders per minute. Legacy reporting processes relied on overnight \gls{etl} jobs and spreadsheet-driven analysis, resulting in inconsistent KPIs and limited ability to react to flash sales or supply chain disruptions.

\section{Project Motivation}
The strategic vision is to create a unified data backbone capable of ingesting multi-channel signals, automating data quality enforcement, and presenting actionable insights to business stakeholders in near real-time. The project aims to:
\begin{itemize}
    \item Reduce decision latency by providing sales, inventory, and customer experience metrics within a configurable two-minute window.
    \item Improve confidence in analytical outputs through governed data models, repeatable validation, and full lineage tracking.
    \item Enable omnichannel personalisation by exposing curated datasets and APIs to downstream digital products and partners.
    \item Lay the groundwork for predictive intelligence by capturing granular behavioural and operational data.
\end{itemize}

\section{Research Questions}
Three research questions were submitted for supervisory validation in line with the MSc programme requirements:
\begin{enumerate}
    \item How can a modular cloud-native data architecture sustain sub-three-minute KPI refreshes while maintaining data quality for ecommerce workloads?
    \item What automation patterns most effectively balance rapid feature delivery with compliance and security constraints in a multi-cloud scenario?
    \item Which governance and observability practices maximise stakeholder trust in near real-time analytics and dashboards?
\end{enumerate}

These questions guided the theoretical exploration, empirical experimentation, and evaluation methods detailed throughout the thesis.

\section{Data Engineering Vision}
The enhanced scope of the internship required a unifying vision that bridges data platform engineering with product thinking. The vision statement articulated to the steering committee emphasised four pillars:
\begin{enumerate}
    \item \textbf{Composable pipelines:} every ingestion, transformation, and serving component must be reusable across geographies and tenants, following SOLID-like modularity while remaining configuration-driven.
    \item \textbf{Trust by design:} observability, lineage, and privacy guardrails are embedded in the first sprint rather than added as compliance afterthoughts.
    \item \textbf{Data as a product:} curated datasets are versioned, documented, and operated with explicit SLAs, enabling merchandising, finance, and partner teams to self-serve.
    \item \textbf{Manifesto-guided decision making:} the newly introduced \emph{Data Engineering Manifesto} provides decision heuristics for trade-offs between latency, cost, ethics, and resilience.
\end{enumerate}
This vision anchors subsequent chapters and ensures that the manifesto principles cascade from strategy to implementation tactics.
