\chapter{Methodology}
\section{Analytical Framework}
The project followed a mixed-methods approach blending qualitative stakeholder research with quantitative system benchmarking. Figure~\ref{fig:research-methodology} summarises the iterative workflow combining discovery, design, implementation, and validation activities.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2.2cm,>=stealth',thick]
        \tikzstyle{phase}=[rectangle, rounded corners, minimum width=3.5cm, minimum height=1.2cm, draw=blue!60, fill=blue!10]
        \tikzstyle{arrow}=[->, line width=1pt, color=blue!70]
        \node[phase] (discovery) {Discovery\\Stakeholder Interviews\\Data Audit};
        \node[phase, right of=discovery, xshift=2.5cm] (design) {Design\\Target Architecture\\Experiment Plan};
        \node[phase, right of=design, xshift=2.5cm] (build) {Implementation\\IaC \& Pipelines\\Data Products};
        \node[phase, right of=build, xshift=2.5cm] (validate) {Validation\\Performance Tests\\User Adoption};
        \draw[arrow] (discovery) -- (design);
        \draw[arrow] (design) -- (build);
        \draw[arrow] (build) -- (validate);
        \draw[arrow] (validate) .. controls +(0,-2) and +(0,-2) .. (discovery);
    \end{tikzpicture}
    \caption{Iterative research and delivery methodology}
    \label{fig:research-methodology}
\end{figure}

\section{Data Collection}
Data sources encompassed:
\begin{itemize}
    \item \textbf{Operational systems:} Order management, product catalogue, fulfilment, marketing automation, and customer support platforms exposed via REST APIs, Kafka topics, and SFTP drops.
    \item \textbf{Web and mobile telemetry:} Clickstream events generated from tag managers and server-side instrumentation stored in Amazon Kinesis Data Streams.
    \item \textbf{Reference data:} Currency rates, supplier rosters, logistics carriers, and promotional calendars maintained in master data services.
    \item \textbf{Stakeholder insights:} Semi-structured interviews with 14 stakeholders complemented by survey data regarding dashboard usage patterns.
\end{itemize}

Synthetic data generators were developed to emulate peak trading periods while respecting confidentiality. Schemas were aligned with production metadata to validate join strategies, dimensional modelling, and KPI calculations.

\section{Data Processing and Tooling}
\begin{itemize}
    \item \textbf{Ingestion:} Apache Airflow orchestrated ingestion DAGs using Python operators, AWS Lambda for lightweight transformations, and AWS Glue for schema evolution.
    \item \textbf{Transformation:} dbt Core executed staging, intermediate, and mart models backed by Amazon Redshift or Azure Synapse dedicated pools.
    \item \textbf{Storage:} Amazon S3 served as the bronze and silver zones, with PostgreSQL and Delta Lake delivering curated gold datasets.
    \item \textbf{Serving:} FastAPI and GraphQL endpoints powered the ecommerce portal, while BI tools consumed semantic models through Azure Analysis Services or Power BI datasets.
\end{itemize}

\section{Validation Approach}
Validation combined automated testing with human-centred evaluation:
\begin{enumerate}
    \item \textbf{Technical validation} measured latency, throughput, and fault tolerance through controlled load tests using Locust and Kinesis replay scripts.
    \item \textbf{Data validation} applied Great Expectations suites, dbt tests, and anomaly detection thresholds to guarantee data fitness.
    \item \textbf{User validation} leveraged usability sessions, think-aloud testing, and adoption analytics to refine dashboards and alerts.
    \item \textbf{Governance validation} involved security reviews, privacy impact assessments, and architecture risk registers presented to the Data Protection Officer.
\end{enumerate}

\section{Limitations}
\begin{itemize}
    \item Subscription limits restricted the scale of long-running performance tests; extrapolations were supported by cloud provider sizing guides.
    \item Real-world customer identifiers were anonymised, limiting the ability to validate personalised recommendations beyond synthetic cohorts.
    \item The project timeline constrained exposure to full peak-season load patterns; mitigation involved scenario-based modelling and stress tests.
\end{itemize}
